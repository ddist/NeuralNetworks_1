{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from numpy.random import rand\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Problema 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Definimos las funciones para extraer los diccionarios de los archivos adjuntos de la siguiente forma. El resultado son 6 matrices, las dos primeras contienen los datos y etiquetas del set de entrenamiento, de dimensiones (50000,3072) y (50000,1). Las siguientes dos corresponden a los datos y etiquetas de pruebas, de dimensiones (10000, 3072) y (10000,1). Por último, se toman 5000 imágenes del set de entrenamiento para crear el set de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_batch(filename):\n",
    "    with open(filename, 'rb') as fo:\n",
    "        datadict = pickle.load(fo, encoding='latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(path):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for batch in range(1,6):\n",
    "        file = os.path.join(path, 'data_batch_%d' % (batch,))\n",
    "        X, Y = load_batch(file)\n",
    "        # Se transforma el arreglo de forma que coincida con la representación original de la imágen.\n",
    "        xs.append(X.astype('float'))\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X,Y\n",
    "    Xte, Yte = load_batch(os.path.join(path, 'test_batch'))\n",
    "    # El set de validación (5000 imágenes) extraído del set de entrenamiento.\n",
    "    Xva, Yva = Xtr[15000:20000], Ytr[15000:20000] \n",
    "    return Xtr, Ytr, Xte, Yte, Xva, Yva\n",
    "\n",
    "Xtr, Ytr, Xte, Yte, Xva, Yva = load_CIFAR10('./')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### b)\n",
    "En primera instancia, centraremos restando la media, para luego escalar dividiendo por la desviación estándar, proceso conocido como normalización. Utilizamos el StandardScaler que realiza precisamente esta operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo: -2.207429\n",
      "Máximo: 2.625075\n",
      "Media: 0.000000\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(data):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler().fit(data)\n",
    "    scaled_data = scaler.transform(data)\n",
    "    return scaled_data\n",
    "\n",
    "Xtr_normalized = normalize_data(Xtr)\n",
    "print('Mínimo: %f' % np.min(Xtr_normalized))\n",
    "print('Máximo: %f' % np.max(Xtr_normalized))\n",
    "print('Media: %f' % np.mean(Xtr_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que la media es 0. \n",
    "\n",
    "Ya que todos las características se encuentran en la misma escala, es posible realizar sólo el centrado, restando la media. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo: -140.268820\n",
      "Máximo: 155.004940\n",
      "Media: 0\n"
     ]
    }
   ],
   "source": [
    "def center_data(data):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler(with_std=False).fit(data)\n",
    "    scaled_data = scaler.transform(data)\n",
    "    return scaled_data\n",
    "\n",
    "Xtr_centered = center_data(Xtr)\n",
    "print('Mínimo: %f' % np.min(Xtr_centered))\n",
    "print('Máximo: %f' % np.max(Xtr_centered))\n",
    "print('Media: %f' % np.mean(Xtr_centered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Con el centrado se obtiene una media igual a 0, pero en una escala mayor. \n",
    "\n",
    "También es posible realizar un escalado MinMax, que entrega un set de datos entre 0 y 1. Como el mínimo en el espacio RGB es 0 y el máximo 255, es posible simplificar la función, y sólo dividir por 255. No es exactamente equivalente al escalado MinMax, pero para este caso es útil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo: 0.000000\n",
      "Maximo: 1.000000\n",
      "Media: 0.473363\n"
     ]
    }
   ],
   "source": [
    "def minmax_data(data):\n",
    "    return data/255\n",
    "\n",
    "Xtr_minmax = minmax_data(Xtr)\n",
    "print('Mínimo: %f' % np.min(Xtr_minmax))\n",
    "print('Maximo: %f' % np.max(Xtr_minmax))\n",
    "print('Media: %f' % np.mean(Xtr_minmax))\n",
    "\n",
    "Xva_minmax = minmax_data(Xva)\n",
    "Xte_minmax = minmax_data(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Como se indica en el enunciado, comenzaremos con una red pequeña, con una capa oculta de 50 neuronas y funciones de activación ReLu. Como se trata de un problema de clasificación, todas las redes utilizadas tendrán salida softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 13s - loss: 2.0638 - acc: 0.2125    \n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 21s - loss: 1.9843 - acc: 0.2466    \n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 20s - loss: 1.9467 - acc: 0.2659    \n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 22s - loss: 1.9338 - acc: 0.2710    \n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 22s - loss: 1.9199 - acc: 0.2760    \n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 21s - loss: 1.9038 - acc: 0.2898    \n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 22s - loss: 1.8849 - acc: 0.2930    \n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 24s - loss: 1.8620 - acc: 0.3080    \n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 23s - loss: 1.8447 - acc: 0.3162    \n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 24s - loss: 1.8255 - acc: 0.3266    \n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 23s - loss: 1.8041 - acc: 0.3390    \n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 25s - loss: 1.7862 - acc: 0.3452    \n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 28s - loss: 1.7759 - acc: 0.3493    \n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 28s - loss: 1.7523 - acc: 0.3559    \n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 29s - loss: 1.7405 - acc: 0.3645    \n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 28s - loss: 1.7327 - acc: 0.3659    \n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 30s - loss: 1.7359 - acc: 0.3683    \n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 29s - loss: 1.7227 - acc: 0.3715    \n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 29s - loss: 1.7204 - acc: 0.3735    \n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 31s - loss: 1.7108 - acc: 0.3786    \n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 31s - loss: 1.7037 - acc: 0.3818    \n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 36s - loss: 1.6925 - acc: 0.3873    \n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 30s - loss: 1.6950 - acc: 0.3849    \n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 30s - loss: 1.6910 - acc: 0.3864    \n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 32s - loss: 1.6854 - acc: 0.3894    \n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 33s - loss: 1.6778 - acc: 0.3903    \n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 29s - loss: 1.6739 - acc: 0.3921    \n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 33s - loss: 1.6726 - acc: 0.3943    \n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 29s - loss: 1.6678 - acc: 0.3951    \n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 36s - loss: 1.6632 - acc: 0.3979    \n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 31s - loss: 1.6636 - acc: 0.3968    \n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 23s - loss: 1.6655 - acc: 0.3986    \n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 24s - loss: 1.6617 - acc: 0.3983    \n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 26s - loss: 1.6585 - acc: 0.3991    \n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 27s - loss: 1.6506 - acc: 0.4028    \n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 21s - loss: 1.6460 - acc: 0.4022    \n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 22s - loss: 1.6475 - acc: 0.4007    \n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 26s - loss: 1.6431 - acc: 0.4051    \n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 25s - loss: 1.6468 - acc: 0.4037    \n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 24s - loss: 1.6476 - acc: 0.4029    \n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 22s - loss: 1.6522 - acc: 0.4007    \n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 22s - loss: 1.6352 - acc: 0.4066    \n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 24s - loss: 1.6402 - acc: 0.4049    \n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 28s - loss: 1.6378 - acc: 0.4041    \n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 30s - loss: 1.6367 - acc: 0.4070    \n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 26s - loss: 1.6289 - acc: 0.4095    \n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 27s - loss: 1.6301 - acc: 0.4087    \n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 29s - loss: 1.6258 - acc: 0.4086    \n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 25s - loss: 1.6298 - acc: 0.4073    \n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 25s - loss: 1.6295 - acc: 0.4089    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe304f804e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear el modelo\n",
    "small_model = Sequential()\n",
    "small_model.add(Dense(50, input_dim=3072, activation='relu'))\n",
    "small_model.add(Dense(10, activation='softmax'))\n",
    "# Compilar modelo\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "small_model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Entrenar modelo\n",
    "small_model.fit(Xtr_minmax, Ytr, nb_epoch=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4960/5000 [============================>.] - ETA: 0s\n",
      "Precisión de clasificación: 40.12%\n"
     ]
    }
   ],
   "source": [
    "sm_scores = small_model.evaluate(Xva_minmax, Yva)\n",
    "print(\"\\nPrecisión de clasificación: {0:.2f}%\".format(sm_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo simple, se logra un 40.12% de precisión luego de 50 epochs, utilizando backpropagation con tasa de aprendizaje decreciente (con momentum) y sin *weight decay*.\n",
    "\n",
    "En el siguiente experimento, modelaremos un *Multilayer Perceptron (MLP)*, que consiste de dos capas ocultas de 64 neuronas cada una, con activación ReLu. Ambas capas tendrán un dropout de 0.5. El entrenamiento se realizará en mini-batches y tasa de aprendizaje decreciente con momentum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/diego/.theano/compiledir_Linux-3.19--generic-x86_64-with-elementary_OS-0.3.2-freya-x86_64-3.4.3-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 19s - loss: 2.5142 - acc: 0.1061    \n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 25s - loss: 2.5402 - acc: 0.1013    \n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 29s - loss: 2.5407 - acc: 0.1019    \n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 24s - loss: 2.5432 - acc: 0.0998    \n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 22s - loss: 2.5433 - acc: 0.1014    \n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 27s - loss: 2.5378 - acc: 0.1004    \n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 32s - loss: 2.5435 - acc: 0.1004    \n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 36s - loss: 2.5349 - acc: 0.0981    \n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 34s - loss: 2.5412 - acc: 0.0995    \n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 37s - loss: 2.5359 - acc: 0.0982    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe308778550>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear el modelo\n",
    "medium_model = Sequential()\n",
    "medium_model.add(Dense(64, input_dim=3072, init='uniform'))\n",
    "medium_model.add(Activation('tanh'))\n",
    "medium_model.add(Dense(32, init='uniform'))\n",
    "medium_model.add(Activation('tanh'))\n",
    "medium_model.add(Dense(10, init='uniform'))\n",
    "medium_model.add(Activation('softmax'))\n",
    "# Compilar modelo\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "medium_model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Entrenar modelo\n",
    "medium_model.fit(Xtr_minmax, Ytr, nb_epoch=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4832/5000 [===========================>..] - ETA: 0s\n",
      "Precisión de clasificación: 10.02%\n"
     ]
    }
   ],
   "source": [
    "md_scores = medium_model.evaluate(Xva_minmax, Yva)\n",
    "print(\"\\nPrecisión de clasificación: {0:.2f}%\".format(md_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el nuevo modelo obtuvo peores resultados que el anterior. EL MLP modelado es útil para la clasificación multiclase, que es lo que se pretende en este ejercicio, pero queda claro que la primera capa no es capaz de extraer las características necesarias de forma correcta, por lo que la segunda capa trabaja con un input que no representativo de la imágen.\n",
    "\n",
    "El problema con las capas utilizadas, es que no consideran la información espacial de la imágen. De la forma en que se han modelado las redes, intentan predecir la categoría de una imágen en base a los colores que la componen. Pero gran parte de sus características se encuentran en forma de bordes y contornos, en diferentes escalas. \n",
    "\n",
    "El siguiente modelo utiliza capas convolucionales, que aplican filtros en 'ventanas' pequeñas de la imágen, y permiten extraer características relevantes a nivel de vecindarios (o celdas), en contraste con las características a nivel de pixel de los modelos anteriores. El modelo consta de cuatro capas convolucionales, las dos primeras realizarán 32 filtros de 3x3 y las dos últimas 64 filtros de 3x3. Entre las parejas de capas existe una capa de pooling, que realiza un downsampling de factor 2, con dropout de 0.25. Luego de las capas convolucionales, la imágen se 'aplana' (vector unidimensional) y se pasa a una capa totalmente conectada de 512 neuronas con dropout 0.5 para finalmente llegar a la capa de salida, con 10 neuronas y activación softmax. Todas las otras capas utilizan activación ReLu.\n",
    "\n",
    "Es necesario transformar la matriz de datos Xtr de forma que las imágenes queden con dimensiones (3,32,32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transformar los conjuntos de datos\n",
    "Xtr_orig = Xtr_minmax.reshape(50000,3,32,32).transpose(0,1,3,2)\n",
    "Xva_orig = Xva_minmax.reshape(5000,3,32,32).transpose(0,1,3,2)\n",
    "Xte_orig = Xte_minmax.reshape(10000,3,32,32).transpose(0,1,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 484s - loss: 1.7078 - acc: 0.3738   \n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 456s - loss: 1.2789 - acc: 0.5417   \n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 460s - loss: 1.1097 - acc: 0.6066   \n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 441s - loss: 0.9976 - acc: 0.6487   \n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 446s - loss: 0.9311 - acc: 0.6712   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ef1e4f908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear el modelo\n",
    "conv_model = Sequential();\n",
    "conv_model.add(Convolution2D(32,3,3, input_shape=(3,32,32), border_mode='same'))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(Convolution2D(32,3,3))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "conv_model.add(Dropout(0.25))\n",
    "\n",
    "conv_model.add(Convolution2D(64,3,3, border_mode='same'))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(Convolution2D(64,3,3))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "conv_model.add(Dropout(0.25))\n",
    "\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(512))\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(Dropout(0.5))\n",
    "conv_model.add(Dense(10))\n",
    "conv_model.add(Activation('softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "conv_model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "conv_model.fit(Xtr_orig, Ytr, nb_epoch=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados parecen indicar que este tipo de redes es capaz de extraer características relevantes de las imágenes, que permiten obtener una precisión alta de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 13s    \n",
      "\n",
      "Precisión de clasificación: 74.20%\n"
     ]
    }
   ],
   "source": [
    "conv_scores = conv_model.evaluate(Xva_orig, Yva)\n",
    "print(\"\\nPrecisión de clasificación: {0:.2f}%\".format(conv_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el modelo convolucional obtuvo el mejor resultado, evaluamos el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_scores = conv_model.evaluate(Xte_orig, Yte)\n",
    "print(\"\\nPrecisión de clasificación: {0:.2f}%\".format(test_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión del modelo sobre el conjunto de pruebas, es de 68.40%, superando el 50% indicado en el enunciado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "Importamos la librería adjunta a la tarea. Debido a que el modelo convolucional requiere de un input de dimensiones (50000,3,32,32) y el extractor de características entrega vectores unidimensionales, haremos la comparación con el modelo simple, que obtuvo 40.12% de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from top_level_features import hog_features\n",
    "from top_level_features import color_histogram_hsv\n",
    "from top_level_features import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/dev/ANN/Tarea1/tarea1-ann/top_level_features.py:123: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[cx/2::cx, cy/2::cy].T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_hog = extract_features(Xtr,[hog_features])\n",
    "features_hog.shape\n",
    "features_hsv = extract_features(Xtr,[color_histogram_hsv])\n",
    "features_hsv.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para los descriptores HOG, ajustamos la capa de input a la dimensión de los vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.6726 - acc: 0.4114     \n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.5328 - acc: 0.4678     \n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.4900 - acc: 0.4804     \n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.4704 - acc: 0.4907     \n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.4500 - acc: 0.4964     \n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.4367 - acc: 0.5052     \n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.4191 - acc: 0.5116     \n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.4055 - acc: 0.5148     \n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3993 - acc: 0.5168     \n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3864 - acc: 0.5246     \n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3834 - acc: 0.5244     \n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3740 - acc: 0.5259     \n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3697 - acc: 0.5289     \n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3630 - acc: 0.5307     \n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3574 - acc: 0.5304     \n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3513 - acc: 0.5357     \n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3460 - acc: 0.5360     \n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3445 - acc: 0.5361     \n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3416 - acc: 0.5373     \n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3348 - acc: 0.5393     \n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3325 - acc: 0.5411     \n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3256 - acc: 0.5420     \n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3230 - acc: 0.5431     \n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3194 - acc: 0.5462     \n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3152 - acc: 0.5473     \n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3172 - acc: 0.5467     \n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3143 - acc: 0.5468     \n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3141 - acc: 0.5458     \n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3099 - acc: 0.5472     \n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3045 - acc: 0.5505     \n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3033 - acc: 0.5521     \n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3010 - acc: 0.5508     \n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.3016 - acc: 0.5517     \n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2970 - acc: 0.5543     \n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2978 - acc: 0.5525     \n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2936 - acc: 0.5529     \n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2928 - acc: 0.5542     \n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2902 - acc: 0.5527     \n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2917 - acc: 0.5552     \n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2864 - acc: 0.5572     \n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2867 - acc: 0.5551     \n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2862 - acc: 0.5564     \n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2845 - acc: 0.5557     \n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2851 - acc: 0.5560     \n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2837 - acc: 0.5578     \n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2825 - acc: 0.5562     \n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2778 - acc: 0.5581     \n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2838 - acc: 0.5566     \n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2797 - acc: 0.5566     \n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2806 - acc: 0.5596     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e8227e710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear el modelo\n",
    "small_model_hog = Sequential()\n",
    "small_model_hog.add(Dense(50, input_dim=features_hog.shape[1], activation='relu'))\n",
    "small_model_hog.add(Dense(10, activation='softmax'))\n",
    "# Compilar modelo\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "small_model_hog.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Entrenar modelo\n",
    "small_model_hog.fit(features_hog, Ytr, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/dev/ANN/Tarea1/tarea1-ann/top_level_features.py:123: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[cx/2::cx, cy/2::cy].T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9536/10000 [===========================>..] - ETA: 0s\n",
      "Precisión de clasificación: 49.70%\n"
     ]
    }
   ],
   "source": [
    "features_test_hog = extract_features(Xte,[hog_features])\n",
    "sm_hog_scores = small_model_hog.evaluate(features_test_hog, Yte)\n",
    "print(\"\\nPrecisión de clasificación: {0:.2f}%\".format(sm_hog_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado muestra un incremento de 9.58% utilizando los descriptores HOG, evaluando contra el conjunto de pruebas. Realizamos el mismo procedimiento para los histogramas HSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 2s - loss: 2.1152 - acc: 0.2087     \n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 2s - loss: 2.0490 - acc: 0.2348     \n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 1s - loss: 2.0328 - acc: 0.2400     \n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 2s - loss: 2.0222 - acc: 0.2479     \n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s - loss: 2.0140 - acc: 0.2517     \n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 2s - loss: 2.0085 - acc: 0.2538     \n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s - loss: 2.0035 - acc: 0.2573     \n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s - loss: 2.0002 - acc: 0.2580     \n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9973 - acc: 0.2592     \n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9947 - acc: 0.2618     \n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9927 - acc: 0.2618     \n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9909 - acc: 0.2625     \n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9889 - acc: 0.2645     \n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9875 - acc: 0.2641     \n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9865 - acc: 0.2656     \n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9846 - acc: 0.2651     \n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.9841 - acc: 0.2673     \n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9827 - acc: 0.2671     \n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.9815 - acc: 0.2664     \n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9802 - acc: 0.2671     \n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9793 - acc: 0.2669     \n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9790 - acc: 0.2680     \n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9774 - acc: 0.2690     \n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.9777 - acc: 0.2678     \n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9765 - acc: 0.2681     \n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.9753 - acc: 0.2688     \n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.9756 - acc: 0.2685     \n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9745 - acc: 0.2699     \n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9739 - acc: 0.2690     \n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9733 - acc: 0.2719     \n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.9725 - acc: 0.2708     \n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9727 - acc: 0.2702     \n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9712 - acc: 0.2726     \n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9716 - acc: 0.2694     \n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.9705 - acc: 0.2716     \n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9699 - acc: 0.2726     \n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9693 - acc: 0.2723     \n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.9691 - acc: 0.2736     \n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9680 - acc: 0.2731     \n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9680 - acc: 0.2726     \n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9671 - acc: 0.2729     \n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.9673 - acc: 0.2729     \n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9674 - acc: 0.2730     \n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9669 - acc: 0.2731     \n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.9666 - acc: 0.2727     \n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.9651 - acc: 0.2734     \n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9647 - acc: 0.2728     \n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.9651 - acc: 0.2734     \n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9645 - acc: 0.2750     \n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.9644 - acc: 0.2737     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e7edbf940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear el modelo\n",
    "small_model_hsv = Sequential()\n",
    "small_model_hsv.add(Dense(50, input_dim=features_hsv.shape[1], activation='relu'))\n",
    "small_model_hsv.add(Dense(10, activation='softmax'))\n",
    "# Compilar modelo\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "small_model_hsv.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Entrenar modelo\n",
    "small_model_hsv.fit(features_hsv, Ytr, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n",
      " 9312/10000 [==========================>...] - ETA: 0s\n",
      "Precisión de clasificación: 27.21%\n"
     ]
    }
   ],
   "source": [
    "features_test_hsv = extract_features(Xte,[color_histogram_hsv])\n",
    "sm_hsv_scores = small_model_hsv.evaluate(features_test_hsv, Yte)\n",
    "print(\"\\nPrecisión de clasificación: {0:.2f}%\".format(sm_hsv_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que la función de pérdida no varía mucho en cada iteración, por lo que es posible que se encuentre en un óptimo local. El resultado empeoró al usar los histogramas, disminuyendo la precisión en un 12.90%.\n",
    "\n",
    "Por último, repetimos el experimento para ambos descriptores en conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/dev/ANN/Tarea1/tarea1-ann/top_level_features.py:123: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[cx/2::cx, cy/2::cy].T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.6483 - acc: 0.4195     \n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.4807 - acc: 0.4840     \n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.4368 - acc: 0.5009     \n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.4056 - acc: 0.5116     \n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3842 - acc: 0.5201     \n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.3667 - acc: 0.5240     \n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3490 - acc: 0.5303     \n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3396 - acc: 0.5321     \n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3307 - acc: 0.5345     \n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3186 - acc: 0.5375     \n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3136 - acc: 0.5437     \n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.3012 - acc: 0.5474     \n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2956 - acc: 0.5498     \n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2943 - acc: 0.5490     \n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2879 - acc: 0.5521     \n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2842 - acc: 0.5542     \n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.2745 - acc: 0.5554     \n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2748 - acc: 0.5573     \n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2640 - acc: 0.5600     \n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2661 - acc: 0.5607     \n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2613 - acc: 0.5598     \n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2547 - acc: 0.5652     \n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2533 - acc: 0.5646     \n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2507 - acc: 0.5641     \n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2462 - acc: 0.5662     \n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2443 - acc: 0.5678     \n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2398 - acc: 0.5681     \n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2400 - acc: 0.5701     \n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2360 - acc: 0.5715     \n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2331 - acc: 0.5700     \n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2306 - acc: 0.5718     \n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2270 - acc: 0.5743     \n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2220 - acc: 0.5753     \n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2219 - acc: 0.5763     \n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2188 - acc: 0.5769     \n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2215 - acc: 0.5757     \n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2181 - acc: 0.5780     \n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2130 - acc: 0.5805     \n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2141 - acc: 0.5778     \n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2078 - acc: 0.5803     \n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2117 - acc: 0.5767     \n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.2089 - acc: 0.5815     \n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 1s - loss: 1.2087 - acc: 0.5799     \n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2055 - acc: 0.5803     \n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2039 - acc: 0.5809     \n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2009 - acc: 0.5829     \n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.2005 - acc: 0.5844     \n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.1968 - acc: 0.5856     \n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 2s - loss: 1.1954 - acc: 0.5850     \n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 3s - loss: 1.1969 - acc: 0.5843     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e7b087b38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_hog_hsv = extract_features(Xtr,[hog_features, color_histogram_hsv])\n",
    "features_test_hog_hsv = extract_features(Xte,[hog_features, color_histogram_hsv])\n",
    "# Crear el modelo\n",
    "small_model_hog_hsv = Sequential()\n",
    "small_model_hog_hsv.add(Dense(50, input_dim=features_hog_hsv.shape[1], activation='relu'))\n",
    "small_model_hog_hsv.add(Dense(10, activation='softmax'))\n",
    "# Compilar modelo\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "small_model_hog_hsv.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Entrenar modelo\n",
    "small_model_hog_hsv.fit(features_hog_hsv, Ytr, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9440/10000 [===========================>..] - ETA: 0s\n",
      "Precisión de clasificación: 51.61%\n"
     ]
    }
   ],
   "source": [
    "sm_hog_hsv_scores = small_model_hog_hsv.evaluate(features_test_hog_hsv, Yte)\n",
    "print(\"\\nPrecisión de clasificación: {0:.2f}%\".format(sm_hog_hsv_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vemos que al utilizar en conjunto los descriptores, se obtiene un aumento de 11.49% en precisión. Se puede concluir que los histogramas de colores contienen información relevante sobre las características, pero que por sí solas no son suficientes para lograr una precisión alta de clasificación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

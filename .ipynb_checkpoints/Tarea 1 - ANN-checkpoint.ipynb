{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Tarea 1 - Introducción a las Redes Neuronales Artificiales</center>\n",
    "## <center> Ariel Sanhueza - 201173005-4 </center>\n",
    "## <center> Diego Pérez - 201173xxx-y </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previo\n",
    "Primero importaremos las bibliotecas necesarias para desarrollar la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import rand\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.misc import imread\n",
    "\n",
    "import cPickle as pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones entregadas con la tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The next functions were slightly modified from the work of Alex Krizhevsky\n",
    "#http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "def extract_features(array_imgs, feature_fns, verbose=False):\n",
    "  \"\"\"\n",
    "  Given pixel data for images and several feature functions that can operate on\n",
    "  single images, apply all feature functions to all images, concatenating the\n",
    "  feature vectors for each image and storing the features for all images in\n",
    "  a single matrix.\n",
    "\n",
    "  Inputs:\n",
    "  - array_imgs: N array of pixel data for N images.\n",
    "  - feature_fns: List of k feature functions. The ith feature function should\n",
    "    take as input an H x W x D array and return a (one-dimensional) array of\n",
    "    length F_i. For CIFAR10, H=32, W=32, D=3\n",
    "  - verbose: Boolean; if true, print progress.\n",
    "\n",
    "  Returns:\n",
    "  An array of shape (N, F_1 + ... + F_k) where each column is the concatenation\n",
    "  of all features for a single image.\n",
    "  \"\"\"\n",
    "  imgs = array_imgs.reshape(array_imgs.shape[0], 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "  print imgs.shape\n",
    "  num_images = imgs.shape[0]\n",
    "  if num_images == 0:\n",
    "    return np.array([])\n",
    "\n",
    "  # Use the first image to determine feature dimensions\n",
    "  feature_dims = []\n",
    "  first_image_features = []\n",
    "  for feature_fn in feature_fns:\n",
    "    feats = feature_fn(imgs[0].squeeze())\n",
    "    assert len(feats.shape) == 1, 'Feature functions must be one-dimensional'\n",
    "    feature_dims.append(feats.size)\n",
    "    first_image_features.append(feats)\n",
    "\n",
    "  # Now that we know the dimensions of the features, we can allocate a single\n",
    "  # big array to store all features as columns.\n",
    "  total_feature_dim = sum(feature_dims)\n",
    "  imgs_features = np.zeros((num_images, total_feature_dim))\n",
    "  imgs_features[0] = np.hstack(first_image_features).T\n",
    "\n",
    "  # Extract features for the rest of the images.\n",
    "  for i in xrange(1, num_images):\n",
    "    idx = 0\n",
    "    for feature_fn, feature_dim in zip(feature_fns, feature_dims):\n",
    "      next_idx = idx + feature_dim\n",
    "      imgs_features[i, idx:next_idx] = feature_fn(imgs[i].squeeze())\n",
    "      idx = next_idx\n",
    "    if verbose and i % 1000 == 0:\n",
    "      print 'Done extracting features for %d / %d images' % (i, num_images)\n",
    "\n",
    "  return imgs_features\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "  \"\"\"Convert RGB image to grayscale\n",
    "\n",
    "    Parameters:\n",
    "      rgb : RGB image\n",
    "\n",
    "    Returns:\n",
    "      gray : grayscale image\n",
    "  \n",
    "  \"\"\"\n",
    "  return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "\n",
    "\n",
    "def hog_features(im):\n",
    "  \"\"\"Compute Histogram of Gradient (HOG) feature for an image\n",
    "  \n",
    "       Modified from skimage.feature.hog\n",
    "       http://pydoc.net/Python/scikits-image/0.4.2/skimage.feature.hog\n",
    "     \n",
    "     Reference:\n",
    "       Histograms of Oriented Gradients for Human Detection\n",
    "       Navneet Dalal and Bill Triggs, CVPR 2005\n",
    "     \n",
    "    Parameters:\n",
    "      im : an input grayscale or rgb image\n",
    "      \n",
    "    Returns:\n",
    "      feat: Histogram of Gradient (HOG) feature\n",
    "    \n",
    "  \"\"\"\n",
    "  # convert rgb to grayscale if needed\n",
    "  if im.ndim == 3:\n",
    "    image = rgb2gray(im)\n",
    "  else:\n",
    "    image = np.at_least_2d(im)\n",
    "\n",
    "  sx, sy = image.shape # image size\n",
    "  orientations = 9 # number of gradient bins\n",
    "  cx, cy = (8, 8) # pixels per cell\n",
    "\n",
    "  gx = np.zeros(image.shape)\n",
    "  gy = np.zeros(image.shape)\n",
    "  gx[:, :-1] = np.diff(image, n=1, axis=1) # compute gradient on x-direction\n",
    "  gy[:-1, :] = np.diff(image, n=1, axis=0) # compute gradient on y-direction\n",
    "  grad_mag = np.sqrt(gx ** 2 + gy ** 2) # gradient magnitude\n",
    "  grad_ori = np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90 # gradient orientation\n",
    "\n",
    "  n_cellsx = int(np.floor(sx / cx))  # number of cells in x\n",
    "  n_cellsy = int(np.floor(sy / cy))  # number of cells in y\n",
    "  # compute orientations integral images\n",
    "  orientation_histogram = np.zeros((n_cellsx, n_cellsy, orientations))\n",
    "  for i in range(orientations):\n",
    "    # create new integral image for this orientation\n",
    "    # isolate orientations in this range\n",
    "    temp_ori = np.where(grad_ori < 180 / orientations * (i + 1),\n",
    "                        grad_ori, 0)\n",
    "    temp_ori = np.where(grad_ori >= 180 / orientations * i,\n",
    "                        temp_ori, 0)\n",
    "    # select magnitudes for those orientations\n",
    "    cond2 = temp_ori > 0\n",
    "    temp_mag = np.where(cond2, grad_mag, 0)\n",
    "    orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[cx/2::cx, cy/2::cy].T\n",
    "  \n",
    "  return orientation_histogram.ravel()\n",
    "\n",
    "\n",
    "def color_histogram_hsv(im, nbin=10, xmin=0, xmax=255, normalized=True):\n",
    "  \"\"\"\n",
    "  Compute color histogram for an image using hue.\n",
    "\n",
    "  Inputs:\n",
    "  - im: H x W x C array of pixel data for an RGB image.\n",
    "  - nbin: Number of histogram bins. (default: 10)\n",
    "  - xmin: Minimum pixel value (default: 0)\n",
    "  - xmax: Maximum pixel value (default: 255)\n",
    "  - normalized: Whether to normalize the histogram (default: True)\n",
    "\n",
    "  Returns:\n",
    "    1D vector of length nbin giving the color histogram over the hue of the\n",
    "    input image.\n",
    "  \"\"\"\n",
    "  ndim = im.ndim\n",
    "  bins = np.linspace(xmin, xmax, nbin+1)\n",
    "  hsv = matplotlib.colors.rgb_to_hsv(im/xmax) * xmax\n",
    "  imhist, bin_edges = np.histogram(hsv[:,:,0], bins=bins, density=normalized)\n",
    "  imhist = imhist * np.diff(bin_edges)\n",
    "\n",
    "  # return histogram\n",
    "  return imhist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Primero generamos una función que genere $n$ pares (x,y), donde $x \\in [-1,1]^2, y \\in \\{0,1\\}$, como se sugirió en el enunciado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data(n):\n",
    "    # Generamos la matriz con los datos, de tamaño nx3\n",
    "    ndata = 2*rand(n,3) - 1\n",
    "    # Calculamos el valor que corresponde a y\n",
    "    for i in range(n):\n",
    "        if (ndata[i,0] > 0 and ndata[i,1] > 0) or (ndata[i,0] <= 0 and ndata[i,1] <= 0):\n",
    "            ndata[i,2] = 0\n",
    "        else:\n",
    "            ndata[i,2] = 1\n",
    "    return ndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema se llama **XOR** pues la función de clasificación es la de la operación \"OR-Exclusive\". La separación sigue la misma lógica del XOR binario. Generamos los datos de entrenamiento y prueba para luego separarlos en la *data* de entrada y la data con los outputs deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_dt = generate_data(1000)\n",
    "test_dt = generate_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separamos la data en entrada y salida\n",
    "X_training = training_dt[:,:2]\n",
    "Y_training = training_dt[:,2]\n",
    "X_test = test_dt[:,:2]\n",
    "Y_test = test_dt[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "Procedemos a generar neurona utilizando *keras* (como red neuronal de una capa oculta). Se mostrarán dos ejemplos donde se utilizarán dos funciones de activación: **lineal** y **sigmoidal**. Esto se hará para demostrar que independiente de los parámetros de la neurona, el problema no puede ser resuelto por ella. Los parámetros de la compilación son seteados de acuerdo a la página oficial de *keras* sobre un ejemplo de clasificador binario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Primero modelo\n",
    "model_linear = Sequential()\n",
    "model_linear.add(Dense(1, input_dim=2, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compilamos\n",
    "model_linear.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.9386 - acc: 0.4000     \n",
      "Epoch 2/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.8518 - acc: 0.4100     \n",
      "Epoch 3/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.7246 - acc: 0.4130     \n",
      "Epoch 4/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.6671 - acc: 0.4190     \n",
      "Epoch 5/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.6021 - acc: 0.4220     \n",
      "Epoch 6/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.5465 - acc: 0.4270     \n",
      "Epoch 7/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.4831 - acc: 0.4360     \n",
      "Epoch 8/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.4120 - acc: 0.4450     \n",
      "Epoch 9/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3789 - acc: 0.4510     \n",
      "Epoch 10/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3601 - acc: 0.4580     \n",
      "Epoch 11/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3247 - acc: 0.4630     \n",
      "Epoch 12/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3010 - acc: 0.4720     \n",
      "Epoch 13/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2882 - acc: 0.4790     \n",
      "Epoch 14/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2767 - acc: 0.4920     \n",
      "Epoch 15/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2659 - acc: 0.4990     \n",
      "Epoch 16/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2460 - acc: 0.5170     \n",
      "Epoch 17/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2370 - acc: 0.5300     \n",
      "Epoch 18/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2304 - acc: 0.5340     \n",
      "Epoch 19/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2234 - acc: 0.5340     \n",
      "Epoch 20/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2060 - acc: 0.5390     \n",
      "Epoch 21/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1996 - acc: 0.5420     \n",
      "Epoch 22/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1943 - acc: 0.5470     \n",
      "Epoch 23/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1897 - acc: 0.5460     \n",
      "Epoch 24/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1859 - acc: 0.5500     \n",
      "Epoch 25/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1714 - acc: 0.5590     \n",
      "Epoch 26/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1686 - acc: 0.5580     \n",
      "Epoch 27/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1663 - acc: 0.5550     \n",
      "Epoch 28/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1648 - acc: 0.5530     \n",
      "Epoch 29/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1635 - acc: 0.5540     \n",
      "Epoch 30/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1627 - acc: 0.5550     \n",
      "Epoch 31/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1621 - acc: 0.5500     \n",
      "Epoch 32/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1618 - acc: 0.5500     \n",
      "Epoch 33/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5500     \n",
      "Epoch 34/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5490     \n",
      "Epoch 35/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5510     \n",
      "Epoch 36/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5500     \n",
      "Epoch 37/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5510     \n",
      "Epoch 38/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5500     \n",
      "Epoch 39/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5520     \n",
      "Epoch 40/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5490     \n",
      "Epoch 41/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5460     \n",
      "Epoch 42/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1610 - acc: 0.5490     \n",
      "Epoch 43/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5470     \n",
      "Epoch 44/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1610 - acc: 0.5480     \n",
      "Epoch 45/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5470     \n",
      "Epoch 46/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1610 - acc: 0.5480     \n",
      "Epoch 47/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1610 - acc: 0.5470     \n",
      "Epoch 48/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5480     \n",
      "Epoch 49/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5430     \n",
      "Epoch 50/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1610 - acc: 0.5460     \n",
      "Epoch 51/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5480     \n",
      "Epoch 52/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5450     \n",
      "Epoch 53/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5460     \n",
      "Epoch 54/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5470     \n",
      "Epoch 55/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5480     \n",
      "Epoch 56/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5470     \n",
      "Epoch 57/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5450     \n",
      "Epoch 58/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5460     \n",
      "Epoch 59/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5450     \n",
      "Epoch 60/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5460     \n",
      "Epoch 61/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5450     \n",
      "Epoch 62/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1610 - acc: 0.5460     \n",
      "Epoch 63/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5450     \n",
      "Epoch 64/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5440     \n",
      "Epoch 65/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5460     \n",
      "Epoch 66/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5470     \n",
      "Epoch 67/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5420     \n",
      "Epoch 68/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5420     \n",
      "Epoch 69/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5430     \n",
      "Epoch 70/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5420     \n",
      "Epoch 71/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5440     \n",
      "Epoch 72/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5430     \n",
      "Epoch 73/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5420     \n",
      "Epoch 74/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5410     \n",
      "Epoch 75/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5420     \n",
      "Epoch 76/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 77/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5430     \n",
      "Epoch 78/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1611 - acc: 0.5420     \n",
      "Epoch 79/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5440     \n",
      "Epoch 80/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5460     \n",
      "Epoch 81/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5430     \n",
      "Epoch 82/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5450     \n",
      "Epoch 83/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5440     \n",
      "Epoch 84/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5440     \n",
      "Epoch 85/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5420     \n",
      "Epoch 86/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5430     \n",
      "Epoch 87/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5420     \n",
      "Epoch 88/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5430     \n",
      "Epoch 89/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5420     \n",
      "Epoch 90/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5420     \n",
      "Epoch 91/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5390     \n",
      "Epoch 92/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 93/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 94/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 95/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5420     \n",
      "Epoch 96/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 97/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 98/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 99/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 100/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 101/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 102/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 103/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5370     \n",
      "Epoch 104/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 105/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 106/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 107/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 108/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 109/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 110/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5420     \n",
      "Epoch 111/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 112/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 113/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 114/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 115/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 116/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 117/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 118/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 119/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5400     \n",
      "Epoch 120/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5380     \n",
      "Epoch 121/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 122/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 123/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 124/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 125/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 126/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 127/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 128/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 129/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 130/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 131/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 132/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 133/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5400     \n",
      "Epoch 134/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 135/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5390     \n",
      "Epoch 136/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 137/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 138/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 139/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 140/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5370     \n",
      "Epoch 141/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 142/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 143/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 144/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5390     \n",
      "Epoch 145/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 146/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 147/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 148/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 149/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5430     \n",
      "Epoch 150/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5420     \n",
      "Epoch 151/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 152/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 153/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 154/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5420     \n",
      "Epoch 155/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5430     \n",
      "Epoch 156/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5420     \n",
      "Epoch 157/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 158/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 159/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 160/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 161/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5430     \n",
      "Epoch 162/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5400     \n",
      "Epoch 163/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 164/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 165/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 166/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 167/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 168/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5390     \n",
      "Epoch 169/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 170/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 171/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 172/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 173/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 174/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 175/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5380     \n",
      "Epoch 176/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 177/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 178/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 179/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 180/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 181/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 182/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 183/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 184/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 185/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 186/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5400     \n",
      "Epoch 187/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 188/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 189/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5370     \n",
      "Epoch 190/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 191/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 192/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 193/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 194/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 195/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 196/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 197/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5370     \n",
      "Epoch 198/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 199/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 200/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5380     \n",
      "Epoch 201/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 202/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 203/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 204/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5370     \n",
      "Epoch 205/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 206/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 207/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5380     \n",
      "Epoch 208/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 209/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 210/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5400     \n",
      "Epoch 211/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 212/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 213/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 214/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 215/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 216/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 217/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 218/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5400     \n",
      "Epoch 219/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 220/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 221/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5390     \n",
      "Epoch 222/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 223/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 224/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1612 - acc: 0.5380     \n",
      "Epoch 225/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 226/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 227/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 228/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 229/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 230/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 231/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 232/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 233/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 234/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 235/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 236/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 237/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 238/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5420     \n",
      "Epoch 239/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 240/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 241/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 242/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 243/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 244/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 245/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 246/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 247/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 248/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 249/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 250/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1616 - acc: 0.5410     \n",
      "Epoch 251/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 252/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 253/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 254/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5400     \n",
      "Epoch 255/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 256/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 257/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 258/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 259/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 260/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5400     \n",
      "Epoch 261/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 262/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 263/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 264/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 265/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 266/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 267/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 268/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 269/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 270/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5390     \n",
      "Epoch 271/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 272/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 273/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5380     \n",
      "Epoch 274/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 275/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5400     \n",
      "Epoch 276/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5370     \n",
      "Epoch 277/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 278/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5370     \n",
      "Epoch 279/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 280/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 281/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 282/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5370     \n",
      "Epoch 283/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 284/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5420     \n",
      "Epoch 285/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5410     \n",
      "Epoch 286/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1616 - acc: 0.5370     \n",
      "Epoch 287/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5390     \n",
      "Epoch 288/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5400     \n",
      "Epoch 289/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 290/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1613 - acc: 0.5410     \n",
      "Epoch 291/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5390     \n",
      "Epoch 292/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5390     \n",
      "Epoch 293/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5380     \n",
      "Epoch 294/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5410     \n",
      "Epoch 295/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5400     \n",
      "Epoch 296/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5400     \n",
      "Epoch 297/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5390     \n",
      "Epoch 298/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1615 - acc: 0.5390     \n",
      "Epoch 299/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1614 - acc: 0.5370     \n",
      "Epoch 300/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.1616 - acc: 0.5380     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114c36d50>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos\n",
    "model_linear.fit(X_training, Y_training, nb_epoch=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32/1000 [..............................] - ETA: 0s\n",
      "Precisión de clasificación: 53.40%\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo\n",
    "scores_linear = model_linear.evaluate(X_test, Y_test)\n",
    "print \"\\nPrecisión de clasificación: {0:.2f}%\".format(scores_linear[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Segundo modelo\n",
    "model_sig = Sequential()\n",
    "model_sig.add(Dense(1, input_dim=2, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compilamos\n",
    "model_sig.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1000/1000 [==============================] - 0s - loss: 5.1193 - acc: 0.3960     \n",
      "Epoch 2/300\n",
      "1000/1000 [==============================] - 0s - loss: 5.0944 - acc: 0.3990     \n",
      "Epoch 3/300\n",
      "1000/1000 [==============================] - 0s - loss: 5.0354 - acc: 0.4040     \n",
      "Epoch 4/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.9580 - acc: 0.4090     \n",
      "Epoch 5/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.8772 - acc: 0.4150     \n",
      "Epoch 6/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.8240 - acc: 0.4240     \n",
      "Epoch 7/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.7706 - acc: 0.4330     \n",
      "Epoch 8/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.7670 - acc: 0.4380     \n",
      "Epoch 9/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.6901 - acc: 0.4450     \n",
      "Epoch 10/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.6533 - acc: 0.4450     \n",
      "Epoch 11/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.5827 - acc: 0.4500     \n",
      "Epoch 12/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.5290 - acc: 0.4580     \n",
      "Epoch 13/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.5003 - acc: 0.4670     \n",
      "Epoch 14/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.4575 - acc: 0.4770     \n",
      "Epoch 15/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3937 - acc: 0.4870     \n",
      "Epoch 16/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3767 - acc: 0.4960     \n",
      "Epoch 17/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3616 - acc: 0.5130     \n",
      "Epoch 18/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3486 - acc: 0.5220     \n",
      "Epoch 19/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3237 - acc: 0.5350     \n",
      "Epoch 20/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.3117 - acc: 0.5480     \n",
      "Epoch 21/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2910 - acc: 0.5630     \n",
      "Epoch 22/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2707 - acc: 0.5680     \n",
      "Epoch 23/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2618 - acc: 0.5780     \n",
      "Epoch 24/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2540 - acc: 0.5830     \n",
      "Epoch 25/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2472 - acc: 0.5840     \n",
      "Epoch 26/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2414 - acc: 0.5900     \n",
      "Epoch 27/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2361 - acc: 0.6000     \n",
      "Epoch 28/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2321 - acc: 0.6020     \n",
      "Epoch 29/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2289 - acc: 0.6000     \n",
      "Epoch 30/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2260 - acc: 0.6000     \n",
      "Epoch 31/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2240 - acc: 0.6050     \n",
      "Epoch 32/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2226 - acc: 0.6070     \n",
      "Epoch 33/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2216 - acc: 0.6040     \n",
      "Epoch 34/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2207 - acc: 0.6010     \n",
      "Epoch 35/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2201 - acc: 0.6000     \n",
      "Epoch 36/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2197 - acc: 0.5970     \n",
      "Epoch 37/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2195 - acc: 0.5970     \n",
      "Epoch 38/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2194 - acc: 0.5970     \n",
      "Epoch 39/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2191 - acc: 0.5940     \n",
      "Epoch 40/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2191 - acc: 0.5930     \n",
      "Epoch 41/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2189 - acc: 0.5890     \n",
      "Epoch 42/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5900     \n",
      "Epoch 43/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5900     \n",
      "Epoch 44/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5890     \n",
      "Epoch 45/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5870     \n",
      "Epoch 46/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5870     \n",
      "Epoch 47/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5860     \n",
      "Epoch 48/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5880     \n",
      "Epoch 49/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5850     \n",
      "Epoch 50/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5820     \n",
      "Epoch 51/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5800     \n",
      "Epoch 52/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5800     \n",
      "Epoch 53/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5810     \n",
      "Epoch 54/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5790     \n",
      "Epoch 55/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5770     \n",
      "Epoch 56/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5770     \n",
      "Epoch 57/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2182 - acc: 0.5750     \n",
      "Epoch 58/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5740     \n",
      "Epoch 59/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5760     \n",
      "Epoch 60/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5720     \n",
      "Epoch 61/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5750     \n",
      "Epoch 62/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2182 - acc: 0.5700     \n",
      "Epoch 63/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5710     \n",
      "Epoch 64/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5720     \n",
      "Epoch 65/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5700     \n",
      "Epoch 66/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2182 - acc: 0.5700     \n",
      "Epoch 67/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5690     \n",
      "Epoch 68/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5680     \n",
      "Epoch 69/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5690     \n",
      "Epoch 70/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5680     \n",
      "Epoch 71/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5670     \n",
      "Epoch 72/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5650     \n",
      "Epoch 73/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5670     \n",
      "Epoch 74/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5660     \n",
      "Epoch 75/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5670     \n",
      "Epoch 76/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5640     \n",
      "Epoch 77/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2183 - acc: 0.5640     \n",
      "Epoch 78/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5650     \n",
      "Epoch 79/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5660     \n",
      "Epoch 80/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5650     \n",
      "Epoch 81/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5650     \n",
      "Epoch 82/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5630     \n",
      "Epoch 83/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5640     \n",
      "Epoch 84/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5650     \n",
      "Epoch 85/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5650     \n",
      "Epoch 86/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5640     \n",
      "Epoch 87/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5620     \n",
      "Epoch 88/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 89/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5650     \n",
      "Epoch 90/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5620     \n",
      "Epoch 91/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5630     \n",
      "Epoch 92/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5620     \n",
      "Epoch 93/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 94/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5630     \n",
      "Epoch 95/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 96/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 97/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 98/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 99/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 100/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5640     \n",
      "Epoch 101/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 102/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 103/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 104/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5600     \n",
      "Epoch 105/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 106/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5600     \n",
      "Epoch 107/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 108/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 109/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 110/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 111/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2189 - acc: 0.5600     \n",
      "Epoch 112/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 113/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 114/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 115/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2189 - acc: 0.5600     \n",
      "Epoch 116/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5590     \n",
      "Epoch 117/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5590     \n",
      "Epoch 118/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 119/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 120/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 121/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 122/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5610     \n",
      "Epoch 123/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 124/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 125/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 126/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 127/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 128/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 129/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 130/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5610     \n",
      "Epoch 131/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 132/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 133/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 134/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5610     \n",
      "Epoch 135/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 136/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5590     \n",
      "Epoch 137/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 138/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5590     \n",
      "Epoch 139/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 140/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 141/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5580     \n",
      "Epoch 142/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 143/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 144/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 145/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 146/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 147/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 148/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 149/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 150/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 151/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 152/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 153/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 154/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 155/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 156/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 157/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 158/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 159/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 160/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 161/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 162/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 163/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 164/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 165/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2191 - acc: 0.5590     \n",
      "Epoch 166/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 167/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5620     \n",
      "Epoch 168/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 169/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 170/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 171/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 172/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 173/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5590     \n",
      "Epoch 174/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 175/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 176/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 177/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2189 - acc: 0.5590     \n",
      "Epoch 178/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 179/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 180/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 181/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 182/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 183/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5590     \n",
      "Epoch 184/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 185/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 186/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 187/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 188/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5610     \n",
      "Epoch 189/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 190/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 191/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5620     \n",
      "Epoch 192/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 193/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 194/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 195/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 196/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5630     \n",
      "Epoch 197/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5590     \n",
      "Epoch 198/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 199/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 200/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 201/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 202/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 203/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 204/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 205/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 206/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2189 - acc: 0.5600     \n",
      "Epoch 207/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 208/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 209/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 210/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 211/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 212/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 213/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 214/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 215/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 216/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 217/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 218/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 219/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5590     \n",
      "Epoch 220/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 221/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 222/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 223/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 224/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 225/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5620     \n",
      "Epoch 226/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5620     \n",
      "Epoch 227/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 228/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 229/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 230/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 231/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 232/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 233/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 234/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 235/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 236/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2184 - acc: 0.5600     \n",
      "Epoch 237/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 238/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 239/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 240/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2189 - acc: 0.5600     \n",
      "Epoch 241/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5590     \n",
      "Epoch 242/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 243/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 244/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 245/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 246/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 247/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 248/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 249/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 250/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2191 - acc: 0.5590     \n",
      "Epoch 251/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5580     \n",
      "Epoch 252/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 253/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 254/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2189 - acc: 0.5600     \n",
      "Epoch 255/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5580     \n",
      "Epoch 256/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 257/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 258/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5580     \n",
      "Epoch 259/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5580     \n",
      "Epoch 260/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5580     \n",
      "Epoch 261/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 262/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5580     \n",
      "Epoch 263/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 264/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 265/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 266/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 267/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 268/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 269/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 270/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5610     \n",
      "Epoch 271/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5610     \n",
      "Epoch 272/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5580     \n",
      "Epoch 273/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 274/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 275/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 276/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 277/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 278/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 279/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 280/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 281/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5590     \n",
      "Epoch 282/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 283/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 284/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 285/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 286/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5610     \n",
      "Epoch 287/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 288/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 289/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 290/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 291/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2188 - acc: 0.5600     \n",
      "Epoch 292/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5590     \n",
      "Epoch 293/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2185 - acc: 0.5600     \n",
      "Epoch 294/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2187 - acc: 0.5600     \n",
      "Epoch 295/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 296/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 297/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 298/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n",
      "Epoch 299/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5600     \n",
      "Epoch 300/300\n",
      "1000/1000 [==============================] - 0s - loss: 4.2186 - acc: 0.5590     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114966ed0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos\n",
    "model_sig.fit(X_training, Y_training, nb_epoch=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32/1000 [..............................] - ETA: 0s\n",
      "Precisión de clasificación: 50.60%\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo\n",
    "scores_sig = model_sig.evaluate(X_test, Y_test)\n",
    "print \"\\nPrecisión de clasificación: {0:.2f}%\".format(scores_sig[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en ambos modelos, la precisión de clasificación en el conjunto de prueba es de aproximadamente un 50%. Esto se debe a que una sola neurona solo puede aproximar funciones lineales y por lo tanto solo puede resolver problemas linealmente separables. Dado que el problema **XOR** no es linealmente separable, utilizando una sola recta se tiene en aproximadamente un 50% de error teórico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Ahora veremos para un modelo con 2 capas ocultas. La activación de las capas ocultas será una función sigmoidal. La capa de salida tendrá activación lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creamos la red con sus parámetros\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(2, input_dim=2, activation='sigmoid'))\n",
    "model_nn.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compilamos\n",
    "model_nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1000/1000 [==============================] - 0s - loss: 2.5264 - acc: 0.4930     \n",
      "Epoch 2/300\n",
      "1000/1000 [==============================] - 0s - loss: 1.5279 - acc: 0.4820     \n",
      "Epoch 3/300\n",
      "1000/1000 [==============================] - 0s - loss: 1.1806 - acc: 0.4720     \n",
      "Epoch 4/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.9698 - acc: 0.4480     \n",
      "Epoch 5/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.8675 - acc: 0.4170     \n",
      "Epoch 6/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.8003 - acc: 0.3580     \n",
      "Epoch 7/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7561 - acc: 0.3320     \n",
      "Epoch 8/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7341 - acc: 0.3600     \n",
      "Epoch 9/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7240 - acc: 0.4020     \n",
      "Epoch 10/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7190 - acc: 0.4280     \n",
      "Epoch 11/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7160 - acc: 0.4370     \n",
      "Epoch 12/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7134 - acc: 0.4490     \n",
      "Epoch 13/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7111 - acc: 0.4710     \n",
      "Epoch 14/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7089 - acc: 0.4520     \n",
      "Epoch 15/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7068 - acc: 0.4630     \n",
      "Epoch 16/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7052 - acc: 0.4870     \n",
      "Epoch 17/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7038 - acc: 0.4740     \n",
      "Epoch 18/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7027 - acc: 0.4530     \n",
      "Epoch 19/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7015 - acc: 0.4560     \n",
      "Epoch 20/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.7003 - acc: 0.5040     \n",
      "Epoch 21/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6993 - acc: 0.5120     \n",
      "Epoch 22/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6984 - acc: 0.4890     \n",
      "Epoch 23/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6977 - acc: 0.4620     \n",
      "Epoch 24/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6971 - acc: 0.4970     \n",
      "Epoch 25/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6962 - acc: 0.5120     \n",
      "Epoch 26/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6960 - acc: 0.5310     \n",
      "Epoch 27/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6953 - acc: 0.5030     \n",
      "Epoch 28/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6951 - acc: 0.5370     \n",
      "Epoch 29/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6947 - acc: 0.5120     \n",
      "Epoch 30/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6945 - acc: 0.5370     \n",
      "Epoch 31/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6942 - acc: 0.5520     \n",
      "Epoch 32/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6937 - acc: 0.5730     \n",
      "Epoch 33/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6936 - acc: 0.5140     \n",
      "Epoch 34/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6933 - acc: 0.5340     \n",
      "Epoch 35/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6930 - acc: 0.5300     \n",
      "Epoch 36/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6929 - acc: 0.5690     \n",
      "Epoch 37/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6928 - acc: 0.4970     \n",
      "Epoch 38/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6927 - acc: 0.5140     \n",
      "Epoch 39/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6925 - acc: 0.5370     \n",
      "Epoch 40/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6923 - acc: 0.5020     \n",
      "Epoch 41/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6924 - acc: 0.5420     \n",
      "Epoch 42/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6923 - acc: 0.5650     \n",
      "Epoch 43/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6921 - acc: 0.5470     \n",
      "Epoch 44/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6920 - acc: 0.5410     \n",
      "Epoch 45/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6920 - acc: 0.4990     \n",
      "Epoch 46/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6920 - acc: 0.5270     \n",
      "Epoch 47/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6918 - acc: 0.5120     \n",
      "Epoch 48/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6919 - acc: 0.5400     \n",
      "Epoch 49/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6917 - acc: 0.4760     \n",
      "Epoch 50/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6918 - acc: 0.5300     \n",
      "Epoch 51/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6916 - acc: 0.5140     \n",
      "Epoch 52/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6917 - acc: 0.5190     \n",
      "Epoch 53/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6917 - acc: 0.5500     \n",
      "Epoch 54/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6915 - acc: 0.5340     \n",
      "Epoch 55/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6915 - acc: 0.5530     \n",
      "Epoch 56/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6914 - acc: 0.4810     \n",
      "Epoch 57/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6918 - acc: 0.5450     \n",
      "Epoch 58/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6915 - acc: 0.5210     \n",
      "Epoch 59/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6915 - acc: 0.5340     \n",
      "Epoch 60/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6915 - acc: 0.5100     \n",
      "Epoch 61/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6913 - acc: 0.5410     \n",
      "Epoch 62/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6913 - acc: 0.5160     \n",
      "Epoch 63/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6913 - acc: 0.4970     \n",
      "Epoch 64/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6913 - acc: 0.5370     \n",
      "Epoch 65/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6912 - acc: 0.5400     \n",
      "Epoch 66/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6913 - acc: 0.5090     \n",
      "Epoch 67/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6911 - acc: 0.5110     \n",
      "Epoch 68/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6912 - acc: 0.5270     \n",
      "Epoch 69/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6910 - acc: 0.5080     \n",
      "Epoch 70/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6911 - acc: 0.5190     \n",
      "Epoch 71/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6911 - acc: 0.5190     \n",
      "Epoch 72/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6910 - acc: 0.5180     \n",
      "Epoch 73/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6909 - acc: 0.5230     \n",
      "Epoch 74/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6908 - acc: 0.5160     \n",
      "Epoch 75/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6908 - acc: 0.5140     \n",
      "Epoch 76/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6909 - acc: 0.5200     \n",
      "Epoch 77/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6909 - acc: 0.5150     \n",
      "Epoch 78/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6908 - acc: 0.5160     \n",
      "Epoch 79/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6907 - acc: 0.5330     \n",
      "Epoch 80/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6907 - acc: 0.5240     \n",
      "Epoch 81/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6905 - acc: 0.5160     \n",
      "Epoch 82/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6907 - acc: 0.5220     \n",
      "Epoch 83/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6906 - acc: 0.5160     \n",
      "Epoch 84/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6905 - acc: 0.5190     \n",
      "Epoch 85/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6905 - acc: 0.5220     \n",
      "Epoch 86/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6903 - acc: 0.5280     \n",
      "Epoch 87/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6904 - acc: 0.5180     \n",
      "Epoch 88/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6903 - acc: 0.5270     \n",
      "Epoch 89/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6902 - acc: 0.5190     \n",
      "Epoch 90/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6901 - acc: 0.5270     \n",
      "Epoch 91/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6900 - acc: 0.5110     \n",
      "Epoch 92/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6900 - acc: 0.5250     \n",
      "Epoch 93/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6902 - acc: 0.5210     \n",
      "Epoch 94/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6900 - acc: 0.5200     \n",
      "Epoch 95/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6897 - acc: 0.5190     \n",
      "Epoch 96/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6898 - acc: 0.5250     \n",
      "Epoch 97/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6898 - acc: 0.5190     \n",
      "Epoch 98/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6897 - acc: 0.5230     \n",
      "Epoch 99/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6896 - acc: 0.5260     \n",
      "Epoch 100/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6896 - acc: 0.5250     \n",
      "Epoch 101/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6894 - acc: 0.5260     \n",
      "Epoch 102/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6893 - acc: 0.5400     \n",
      "Epoch 103/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6891 - acc: 0.5210     \n",
      "Epoch 104/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6893 - acc: 0.5160     \n",
      "Epoch 105/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6889 - acc: 0.5260     \n",
      "Epoch 106/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6889 - acc: 0.5280     \n",
      "Epoch 107/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6890 - acc: 0.5080     \n",
      "Epoch 108/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6888 - acc: 0.5270     \n",
      "Epoch 109/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6886 - acc: 0.5250     \n",
      "Epoch 110/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6886 - acc: 0.5090     \n",
      "Epoch 111/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6883 - acc: 0.5290     \n",
      "Epoch 112/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6882 - acc: 0.5350     \n",
      "Epoch 113/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6880 - acc: 0.5120     \n",
      "Epoch 114/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6880 - acc: 0.5230     \n",
      "Epoch 115/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6878 - acc: 0.5320     \n",
      "Epoch 116/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6878 - acc: 0.5400     \n",
      "Epoch 117/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6876 - acc: 0.5360     \n",
      "Epoch 118/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6875 - acc: 0.5200     \n",
      "Epoch 119/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6874 - acc: 0.5210     \n",
      "Epoch 120/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6871 - acc: 0.5270     \n",
      "Epoch 121/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6871 - acc: 0.5200     \n",
      "Epoch 122/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6867 - acc: 0.5320     \n",
      "Epoch 123/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6868 - acc: 0.5120     \n",
      "Epoch 124/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6865 - acc: 0.5320     \n",
      "Epoch 125/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6864 - acc: 0.5280     \n",
      "Epoch 126/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6861 - acc: 0.5320     \n",
      "Epoch 127/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6859 - acc: 0.5280     \n",
      "Epoch 128/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6855 - acc: 0.5390     \n",
      "Epoch 129/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6855 - acc: 0.5230     \n",
      "Epoch 130/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6851 - acc: 0.5320     \n",
      "Epoch 131/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6851 - acc: 0.5300     \n",
      "Epoch 132/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6849 - acc: 0.5420     \n",
      "Epoch 133/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6844 - acc: 0.5250     \n",
      "Epoch 134/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6842 - acc: 0.5360     \n",
      "Epoch 135/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6839 - acc: 0.5250     \n",
      "Epoch 136/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6836 - acc: 0.5410     \n",
      "Epoch 137/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6833 - acc: 0.5360     \n",
      "Epoch 138/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6829 - acc: 0.5310     \n",
      "Epoch 139/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6825 - acc: 0.5530     \n",
      "Epoch 140/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6823 - acc: 0.5370     \n",
      "Epoch 141/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6819 - acc: 0.5340     \n",
      "Epoch 142/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6815 - acc: 0.5410     \n",
      "Epoch 143/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6811 - acc: 0.5520     \n",
      "Epoch 144/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6806 - acc: 0.5550     \n",
      "Epoch 145/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6802 - acc: 0.5350     \n",
      "Epoch 146/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6797 - acc: 0.5540     \n",
      "Epoch 147/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6792 - acc: 0.5320     \n",
      "Epoch 148/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6787 - acc: 0.5440     \n",
      "Epoch 149/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6781 - acc: 0.5580     \n",
      "Epoch 150/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6778 - acc: 0.5470     \n",
      "Epoch 151/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6770 - acc: 0.5490     \n",
      "Epoch 152/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6764 - acc: 0.5620     \n",
      "Epoch 153/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6759 - acc: 0.5600     \n",
      "Epoch 154/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6750 - acc: 0.5720     \n",
      "Epoch 155/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6744 - acc: 0.5700     \n",
      "Epoch 156/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6738 - acc: 0.5710     \n",
      "Epoch 157/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6730 - acc: 0.5810     \n",
      "Epoch 158/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6723 - acc: 0.5840     \n",
      "Epoch 159/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6716 - acc: 0.5880     \n",
      "Epoch 160/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6705 - acc: 0.6070     \n",
      "Epoch 161/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6697 - acc: 0.5970     \n",
      "Epoch 162/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6687 - acc: 0.6010     \n",
      "Epoch 163/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6677 - acc: 0.6120     \n",
      "Epoch 164/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6668 - acc: 0.6260     \n",
      "Epoch 165/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6655 - acc: 0.6190     \n",
      "Epoch 166/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6643 - acc: 0.6560     \n",
      "Epoch 167/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6634 - acc: 0.6550     \n",
      "Epoch 168/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6621 - acc: 0.6670     \n",
      "Epoch 169/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6608 - acc: 0.6820     \n",
      "Epoch 170/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6595 - acc: 0.6780     \n",
      "Epoch 171/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6582 - acc: 0.6800     \n",
      "Epoch 172/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6569 - acc: 0.6980     \n",
      "Epoch 173/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6552 - acc: 0.7080     \n",
      "Epoch 174/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6538 - acc: 0.7130     \n",
      "Epoch 175/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6520 - acc: 0.7210     \n",
      "Epoch 176/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6503 - acc: 0.7400     \n",
      "Epoch 177/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6484 - acc: 0.7330     \n",
      "Epoch 178/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6465 - acc: 0.7390     \n",
      "Epoch 179/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6447 - acc: 0.7510     \n",
      "Epoch 180/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6427 - acc: 0.7690     \n",
      "Epoch 181/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6408 - acc: 0.7670     \n",
      "Epoch 182/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6389 - acc: 0.7660     \n",
      "Epoch 183/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6368 - acc: 0.7760     \n",
      "Epoch 184/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6346 - acc: 0.7720     \n",
      "Epoch 185/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6321 - acc: 0.7920     \n",
      "Epoch 186/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6297 - acc: 0.7820     \n",
      "Epoch 187/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6274 - acc: 0.7900     \n",
      "Epoch 188/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6249 - acc: 0.7790     \n",
      "Epoch 189/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6225 - acc: 0.8010     \n",
      "Epoch 190/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6198 - acc: 0.8110     \n",
      "Epoch 191/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6170 - acc: 0.8180     \n",
      "Epoch 192/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6139 - acc: 0.8140     \n",
      "Epoch 193/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6108 - acc: 0.8170     \n",
      "Epoch 194/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6079 - acc: 0.8200     \n",
      "Epoch 195/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6048 - acc: 0.8220     \n",
      "Epoch 196/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.6018 - acc: 0.8310     \n",
      "Epoch 197/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5983 - acc: 0.8340     \n",
      "Epoch 198/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5950 - acc: 0.8330     \n",
      "Epoch 199/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5915 - acc: 0.8400     \n",
      "Epoch 200/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5881 - acc: 0.8320     \n",
      "Epoch 201/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5845 - acc: 0.8280     \n",
      "Epoch 202/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5807 - acc: 0.8360     \n",
      "Epoch 203/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5768 - acc: 0.8370     \n",
      "Epoch 204/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5730 - acc: 0.8340     \n",
      "Epoch 205/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5689 - acc: 0.8340     \n",
      "Epoch 206/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5649 - acc: 0.8370     \n",
      "Epoch 207/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5611 - acc: 0.8370     \n",
      "Epoch 208/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5574 - acc: 0.8380     \n",
      "Epoch 209/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5533 - acc: 0.8420     \n",
      "Epoch 210/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5496 - acc: 0.8410     \n",
      "Epoch 211/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5458 - acc: 0.8460     \n",
      "Epoch 212/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5419 - acc: 0.8450     \n",
      "Epoch 213/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5378 - acc: 0.8450     \n",
      "Epoch 214/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5334 - acc: 0.8450     \n",
      "Epoch 215/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5292 - acc: 0.8470     \n",
      "Epoch 216/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5246 - acc: 0.8470     \n",
      "Epoch 217/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5203 - acc: 0.8480     \n",
      "Epoch 218/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5162 - acc: 0.8460     \n",
      "Epoch 219/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5118 - acc: 0.8460     \n",
      "Epoch 220/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5073 - acc: 0.8480     \n",
      "Epoch 221/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.5029 - acc: 0.8480     \n",
      "Epoch 222/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4983 - acc: 0.8430     \n",
      "Epoch 223/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4937 - acc: 0.8470     \n",
      "Epoch 224/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4892 - acc: 0.8460     \n",
      "Epoch 225/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4846 - acc: 0.8470     \n",
      "Epoch 226/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4801 - acc: 0.8490     \n",
      "Epoch 227/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4756 - acc: 0.8460     \n",
      "Epoch 228/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4714 - acc: 0.8470     \n",
      "Epoch 229/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4673 - acc: 0.8460     \n",
      "Epoch 230/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4627 - acc: 0.8470     \n",
      "Epoch 231/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4586 - acc: 0.8490     \n",
      "Epoch 232/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4548 - acc: 0.8440     \n",
      "Epoch 233/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4507 - acc: 0.8460     \n",
      "Epoch 234/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4467 - acc: 0.8480     \n",
      "Epoch 235/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4429 - acc: 0.8440     \n",
      "Epoch 236/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4393 - acc: 0.8470     \n",
      "Epoch 237/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4354 - acc: 0.8450     \n",
      "Epoch 238/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4318 - acc: 0.8440     \n",
      "Epoch 239/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4280 - acc: 0.8450     \n",
      "Epoch 240/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4242 - acc: 0.8460     \n",
      "Epoch 241/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4208 - acc: 0.8420     \n",
      "Epoch 242/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4176 - acc: 0.8440     \n",
      "Epoch 243/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4146 - acc: 0.8450     \n",
      "Epoch 244/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4114 - acc: 0.8460     \n",
      "Epoch 245/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4084 - acc: 0.8420     \n",
      "Epoch 246/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4054 - acc: 0.8420     \n",
      "Epoch 247/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.4024 - acc: 0.8440     \n",
      "Epoch 248/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3998 - acc: 0.8430     \n",
      "Epoch 249/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3971 - acc: 0.8410     \n",
      "Epoch 250/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3945 - acc: 0.8420     \n",
      "Epoch 251/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3919 - acc: 0.8410     \n",
      "Epoch 252/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3897 - acc: 0.8420     \n",
      "Epoch 253/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3874 - acc: 0.8430     \n",
      "Epoch 254/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3850 - acc: 0.8430     \n",
      "Epoch 255/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3829 - acc: 0.8420     \n",
      "Epoch 256/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3805 - acc: 0.8420     \n",
      "Epoch 257/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3783 - acc: 0.8440     \n",
      "Epoch 258/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3760 - acc: 0.8420     \n",
      "Epoch 259/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3739 - acc: 0.8430     \n",
      "Epoch 260/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3722 - acc: 0.8430     \n",
      "Epoch 261/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3702 - acc: 0.8430     \n",
      "Epoch 262/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3686 - acc: 0.8410     \n",
      "Epoch 263/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3668 - acc: 0.8430     \n",
      "Epoch 264/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3652 - acc: 0.8420     \n",
      "Epoch 265/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3635 - acc: 0.8420     \n",
      "Epoch 266/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3620 - acc: 0.8410     \n",
      "Epoch 267/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3605 - acc: 0.8420     \n",
      "Epoch 268/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3591 - acc: 0.8430     \n",
      "Epoch 269/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3578 - acc: 0.8410     \n",
      "Epoch 270/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3565 - acc: 0.8430     \n",
      "Epoch 271/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3552 - acc: 0.8430     \n",
      "Epoch 272/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3542 - acc: 0.8460     \n",
      "Epoch 273/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3531 - acc: 0.8420     \n",
      "Epoch 274/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3520 - acc: 0.8440     \n",
      "Epoch 275/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3509 - acc: 0.8430     \n",
      "Epoch 276/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3500 - acc: 0.8440     \n",
      "Epoch 277/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3489 - acc: 0.8430     \n",
      "Epoch 278/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3481 - acc: 0.8440     \n",
      "Epoch 279/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3472 - acc: 0.8450     \n",
      "Epoch 280/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3465 - acc: 0.8440     \n",
      "Epoch 281/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3455 - acc: 0.8460     \n",
      "Epoch 282/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3450 - acc: 0.8420     \n",
      "Epoch 283/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3444 - acc: 0.8440     \n",
      "Epoch 284/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3436 - acc: 0.8440     \n",
      "Epoch 285/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3431 - acc: 0.8460     \n",
      "Epoch 286/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3425 - acc: 0.8460     \n",
      "Epoch 287/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3419 - acc: 0.8450     \n",
      "Epoch 288/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3415 - acc: 0.8460     \n",
      "Epoch 289/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3411 - acc: 0.8470     \n",
      "Epoch 290/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3405 - acc: 0.8470     \n",
      "Epoch 291/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3401 - acc: 0.8480     \n",
      "Epoch 292/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3397 - acc: 0.8470     \n",
      "Epoch 293/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3392 - acc: 0.8470     \n",
      "Epoch 294/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3388 - acc: 0.8470     \n",
      "Epoch 295/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3385 - acc: 0.8450     \n",
      "Epoch 296/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3383 - acc: 0.8460     \n",
      "Epoch 297/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3377 - acc: 0.8470     \n",
      "Epoch 298/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3377 - acc: 0.8470     \n",
      "Epoch 299/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3374 - acc: 0.8470     \n",
      "Epoch 300/300\n",
      "1000/1000 [==============================] - 0s - loss: 0.3370 - acc: 0.8480     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11555b190>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos\n",
    "model_nn.fit(X_training, Y_training, nb_epoch=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32/1000 [..............................] - ETA: 0s\n",
      "Precisión de clasificación: 83.20%\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo\n",
    "scores_nn = model_nn.evaluate(X_test, Y_test)\n",
    "print \"\\nPrecisión de clasificación: {0:.2f}%\".format(scores_nn[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en este experimento utilizando el modelo descrito la precisió que se obtiene es de un 83.20% por lo que, dependiendo de los parámetros escogidos, el problema sí es resolvible mediante un perceptrón multicapas. Considerar que simplemente se usó un modelo de dos capas para hacer énfasis en que la mínima diferencia respecto al modelo de una neurona y de un percentrón multicapas genera una diferencia sustancial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problema 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Obtenemos los datos del link facilitado. Las líneas realizan:\n",
    "* Línea 3: Se lee el CSV desde la URL entregada. Se marca el caracter ',' como separador de los datos y el parámetro names indica los nombres de cada columna.\n",
    "* Línea 4: importa desde sklearn la función train_test_split que separa el dataset entre datos de entrenamiento y testing de forma aleatoria.\n",
    "* Línea 5: Aplica train_test_split para separar en subconjuntos de entrenamiento y testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 500: Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a4a667496a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://mldata.org/repository/data/download/csv/regression-datasets-housing/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CRIM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ZN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INDUS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CHAS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NOX'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AGE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DIS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RAD'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TAX'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PTRATIO'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'LSTAT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MEDV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    299\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[1;32m    300\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         compression=kwds.get('compression', None))\n\u001b[0m\u001b[1;32m    302\u001b[0m     kwds['compression'] = (inferred_compression if compression == 'infer'\n\u001b[1;32m    303\u001b[0m                            else compression)\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/pandas/io/common.pyc\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 548\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 500: Internal Server Error"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'http://mldata.org/repository/data/download/csv/regression-datasets-housing/'\n",
    "\n",
    "df = pd.read_csv(url, sep=',',header=None, names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV'])\n",
    "from sklearn.cross_validation import train_test_split\n",
    "df_train,df_test= train_test_split(df,test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null int64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null int64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null int64\n",
      "TAX        506 non-null int64\n",
      "PTRATIO    506 non-null int64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(9), int64(5)\n",
      "memory usage: 55.4 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.347826</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.083004</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.310593</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.280574</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677082</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.347826   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.310593    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677082   12.000000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.083004  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.280574   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.000000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.000000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.000000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.000000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Cuando los datos vienen en escalas muy diferentes puede llevar a que los datos con escalas más grandes logren mayor peso frente a los de menor escala (por ejemplo a la hora de calcular distancias). Normalizar los datos permite que todos los datos tengan un aporte \"igualitario\" a la hora de entrenar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "# Datos de entrenamiento\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = df_train.pop('MEDV')\n",
    "# Datos de testing\n",
    "scaler = StandardScaler().fit(df_test)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(df_test), columns=df_test.columns)\n",
    "y_test_scaled = df_test.pop('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/300\n",
      "379/379 [==============================] - 0s - loss: 100.1319 - val_loss: 43.6565\n",
      "Epoch 2/300\n",
      "379/379 [==============================] - 0s - loss: 31.1978 - val_loss: 33.2084\n",
      "Epoch 3/300\n",
      "379/379 [==============================] - 0s - loss: 21.6650 - val_loss: 27.3776\n",
      "Epoch 4/300\n",
      "379/379 [==============================] - 0s - loss: 17.2133 - val_loss: 22.9570\n",
      "Epoch 5/300\n",
      "379/379 [==============================] - 0s - loss: 14.5297 - val_loss: 23.7012\n",
      "Epoch 6/300\n",
      "379/379 [==============================] - 0s - loss: 12.4190 - val_loss: 20.6273\n",
      "Epoch 7/300\n",
      "379/379 [==============================] - 0s - loss: 10.7662 - val_loss: 16.2033\n",
      "Epoch 8/300\n",
      "379/379 [==============================] - 0s - loss: 9.2955 - val_loss: 17.6258\n",
      "Epoch 9/300\n",
      "379/379 [==============================] - 0s - loss: 9.4140 - val_loss: 12.4275\n",
      "Epoch 10/300\n",
      "379/379 [==============================] - 0s - loss: 7.4631 - val_loss: 11.2238\n",
      "Epoch 11/300\n",
      "379/379 [==============================] - 0s - loss: 7.8038 - val_loss: 12.0480\n",
      "Epoch 12/300\n",
      "379/379 [==============================] - 0s - loss: 6.7672 - val_loss: 9.5976\n",
      "Epoch 13/300\n",
      "379/379 [==============================] - 0s - loss: 6.1963 - val_loss: 8.6788\n",
      "Epoch 14/300\n",
      "379/379 [==============================] - 0s - loss: 5.9637 - val_loss: 7.6517\n",
      "Epoch 15/300\n",
      "379/379 [==============================] - 0s - loss: 5.3529 - val_loss: 7.5253\n",
      "Epoch 16/300\n",
      "379/379 [==============================] - 0s - loss: 5.1685 - val_loss: 6.8029\n",
      "Epoch 17/300\n",
      "379/379 [==============================] - 0s - loss: 4.6780 - val_loss: 7.6383\n",
      "Epoch 18/300\n",
      "379/379 [==============================] - 0s - loss: 4.3905 - val_loss: 6.5062\n",
      "Epoch 19/300\n",
      "379/379 [==============================] - 0s - loss: 4.0737 - val_loss: 5.0327\n",
      "Epoch 20/300\n",
      "379/379 [==============================] - 0s - loss: 4.0069 - val_loss: 4.7257\n",
      "Epoch 21/300\n",
      "379/379 [==============================] - 0s - loss: 3.6010 - val_loss: 4.2064\n",
      "Epoch 22/300\n",
      "379/379 [==============================] - 0s - loss: 3.0147 - val_loss: 3.9135\n",
      "Epoch 23/300\n",
      "379/379 [==============================] - 0s - loss: 2.7891 - val_loss: 3.4657\n",
      "Epoch 24/300\n",
      "379/379 [==============================] - 0s - loss: 2.6767 - val_loss: 3.1447\n",
      "Epoch 25/300\n",
      "379/379 [==============================] - 0s - loss: 2.3423 - val_loss: 2.8577\n",
      "Epoch 26/300\n",
      "379/379 [==============================] - 0s - loss: 2.3053 - val_loss: 3.2492\n",
      "Epoch 27/300\n",
      "379/379 [==============================] - 0s - loss: 1.9623 - val_loss: 2.7201\n",
      "Epoch 28/300\n",
      "379/379 [==============================] - 0s - loss: 1.8350 - val_loss: 2.1397\n",
      "Epoch 29/300\n",
      "379/379 [==============================] - 0s - loss: 1.6655 - val_loss: 1.9291\n",
      "Epoch 30/300\n",
      "379/379 [==============================] - 0s - loss: 1.5550 - val_loss: 1.8182\n",
      "Epoch 31/300\n",
      "379/379 [==============================] - 0s - loss: 1.3620 - val_loss: 1.5921\n",
      "Epoch 32/300\n",
      "379/379 [==============================] - 0s - loss: 1.2078 - val_loss: 1.4931\n",
      "Epoch 33/300\n",
      "379/379 [==============================] - 0s - loss: 1.1235 - val_loss: 1.2888\n",
      "Epoch 34/300\n",
      "379/379 [==============================] - 0s - loss: 1.0148 - val_loss: 1.2724\n",
      "Epoch 35/300\n",
      "379/379 [==============================] - 0s - loss: 0.9237 - val_loss: 1.0559\n",
      "Epoch 36/300\n",
      "379/379 [==============================] - 0s - loss: 0.8433 - val_loss: 0.9538\n",
      "Epoch 37/300\n",
      "379/379 [==============================] - 0s - loss: 0.8241 - val_loss: 0.8682\n",
      "Epoch 38/300\n",
      "379/379 [==============================] - 0s - loss: 0.7268 - val_loss: 0.8296\n",
      "Epoch 39/300\n",
      "379/379 [==============================] - 0s - loss: 0.6733 - val_loss: 0.9897\n",
      "Epoch 40/300\n",
      "379/379 [==============================] - 0s - loss: 0.6499 - val_loss: 0.6967\n",
      "Epoch 41/300\n",
      "379/379 [==============================] - 0s - loss: 0.5693 - val_loss: 0.6291\n",
      "Epoch 42/300\n",
      "379/379 [==============================] - 0s - loss: 0.5628 - val_loss: 0.5898\n",
      "Epoch 43/300\n",
      "379/379 [==============================] - 0s - loss: 0.5567 - val_loss: 0.5591\n",
      "Epoch 44/300\n",
      "379/379 [==============================] - 0s - loss: 0.4843 - val_loss: 0.5196\n",
      "Epoch 45/300\n",
      "379/379 [==============================] - 0s - loss: 0.4773 - val_loss: 0.6591\n",
      "Epoch 46/300\n",
      "379/379 [==============================] - 0s - loss: 0.4278 - val_loss: 0.5108\n",
      "Epoch 47/300\n",
      "379/379 [==============================] - 0s - loss: 0.4285 - val_loss: 0.4561\n",
      "Epoch 48/300\n",
      "379/379 [==============================] - 0s - loss: 0.3921 - val_loss: 0.4751\n",
      "Epoch 49/300\n",
      "379/379 [==============================] - 0s - loss: 0.3795 - val_loss: 0.4107\n",
      "Epoch 50/300\n",
      "379/379 [==============================] - 0s - loss: 0.3670 - val_loss: 0.4523\n",
      "Epoch 51/300\n",
      "379/379 [==============================] - 0s - loss: 0.3541 - val_loss: 0.3646\n",
      "Epoch 52/300\n",
      "379/379 [==============================] - 0s - loss: 0.3379 - val_loss: 0.6331\n",
      "Epoch 53/300\n",
      "379/379 [==============================] - 0s - loss: 0.3452 - val_loss: 0.3669\n",
      "Epoch 54/300\n",
      "379/379 [==============================] - 0s - loss: 0.3246 - val_loss: 0.3403\n",
      "Epoch 55/300\n",
      "379/379 [==============================] - 0s - loss: 0.3081 - val_loss: 0.3237\n",
      "Epoch 56/300\n",
      "379/379 [==============================] - 0s - loss: 0.3028 - val_loss: 0.3587\n",
      "Epoch 57/300\n",
      "379/379 [==============================] - 0s - loss: 0.2907 - val_loss: 0.3173\n",
      "Epoch 58/300\n",
      "379/379 [==============================] - 0s - loss: 0.3078 - val_loss: 0.3999\n",
      "Epoch 59/300\n",
      "379/379 [==============================] - 0s - loss: 0.3042 - val_loss: 0.4392\n",
      "Epoch 60/300\n",
      "379/379 [==============================] - 0s - loss: 0.2660 - val_loss: 0.3232\n",
      "Epoch 61/300\n",
      "379/379 [==============================] - 0s - loss: 0.2744 - val_loss: 0.2964\n",
      "Epoch 62/300\n",
      "379/379 [==============================] - 0s - loss: 0.2676 - val_loss: 0.2760\n",
      "Epoch 63/300\n",
      "379/379 [==============================] - 0s - loss: 0.2933 - val_loss: 0.2868\n",
      "Epoch 64/300\n",
      "379/379 [==============================] - 0s - loss: 0.2521 - val_loss: 0.3269\n",
      "Epoch 65/300\n",
      "379/379 [==============================] - 0s - loss: 0.2745 - val_loss: 0.4714\n",
      "Epoch 66/300\n",
      "379/379 [==============================] - 0s - loss: 0.2470 - val_loss: 0.2954\n",
      "Epoch 67/300\n",
      "379/379 [==============================] - 0s - loss: 0.2363 - val_loss: 0.2867\n",
      "Epoch 68/300\n",
      "379/379 [==============================] - 0s - loss: 0.2377 - val_loss: 0.2771\n",
      "Epoch 69/300\n",
      "379/379 [==============================] - 0s - loss: 0.2362 - val_loss: 0.4463\n",
      "Epoch 70/300\n",
      "379/379 [==============================] - 0s - loss: 0.2271 - val_loss: 0.3788\n",
      "Epoch 71/300\n",
      "379/379 [==============================] - 0s - loss: 0.2280 - val_loss: 0.2540\n",
      "Epoch 72/300\n",
      "379/379 [==============================] - 0s - loss: 0.2331 - val_loss: 0.2449\n",
      "Epoch 73/300\n",
      "379/379 [==============================] - 0s - loss: 0.2198 - val_loss: 0.3737\n",
      "Epoch 74/300\n",
      "379/379 [==============================] - 0s - loss: 0.2290 - val_loss: 0.2467\n",
      "Epoch 75/300\n",
      "379/379 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2504\n",
      "Epoch 76/300\n",
      "379/379 [==============================] - 0s - loss: 0.2085 - val_loss: 0.2453\n",
      "Epoch 77/300\n",
      "379/379 [==============================] - 0s - loss: 0.2065 - val_loss: 0.2436\n",
      "Epoch 78/300\n",
      "379/379 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2951\n",
      "Epoch 79/300\n",
      "379/379 [==============================] - 0s - loss: 0.2054 - val_loss: 0.2944\n",
      "Epoch 80/300\n",
      "379/379 [==============================] - 0s - loss: 0.2036 - val_loss: 0.2241\n",
      "Epoch 81/300\n",
      "379/379 [==============================] - 0s - loss: 0.1976 - val_loss: 0.2902\n",
      "Epoch 82/300\n",
      "379/379 [==============================] - 0s - loss: 0.2060 - val_loss: 0.4343\n",
      "Epoch 83/300\n",
      "379/379 [==============================] - 0s - loss: 0.2170 - val_loss: 0.2550\n",
      "Epoch 84/300\n",
      "379/379 [==============================] - 0s - loss: 0.2058 - val_loss: 0.2782\n",
      "Epoch 85/300\n",
      "379/379 [==============================] - 0s - loss: 0.1910 - val_loss: 0.3087\n",
      "Epoch 86/300\n",
      "379/379 [==============================] - 0s - loss: 0.1899 - val_loss: 0.2164\n",
      "Epoch 87/300\n",
      "379/379 [==============================] - 0s - loss: 0.1908 - val_loss: 0.2053\n",
      "Epoch 88/300\n",
      "379/379 [==============================] - 0s - loss: 0.1906 - val_loss: 0.2585\n",
      "Epoch 89/300\n",
      "379/379 [==============================] - 0s - loss: 0.1885 - val_loss: 0.2890\n",
      "Epoch 90/300\n",
      "379/379 [==============================] - 0s - loss: 0.1917 - val_loss: 0.4617\n",
      "Epoch 91/300\n",
      "379/379 [==============================] - 0s - loss: 0.1982 - val_loss: 0.2002\n",
      "Epoch 92/300\n",
      "379/379 [==============================] - 0s - loss: 0.1890 - val_loss: 0.2190\n",
      "Epoch 93/300\n",
      "379/379 [==============================] - 0s - loss: 0.1982 - val_loss: 0.2499\n",
      "Epoch 94/300\n",
      "379/379 [==============================] - 0s - loss: 0.1828 - val_loss: 0.2880\n",
      "Epoch 95/300\n",
      "379/379 [==============================] - 0s - loss: 0.1837 - val_loss: 0.2487\n",
      "Epoch 96/300\n",
      "379/379 [==============================] - 0s - loss: 0.1748 - val_loss: 0.2516\n",
      "Epoch 97/300\n",
      "379/379 [==============================] - 0s - loss: 0.1731 - val_loss: 0.2367\n",
      "Epoch 98/300\n",
      "379/379 [==============================] - 0s - loss: 0.1724 - val_loss: 0.3141\n",
      "Epoch 99/300\n",
      "379/379 [==============================] - 0s - loss: 0.1901 - val_loss: 0.1896\n",
      "Epoch 100/300\n",
      "379/379 [==============================] - 0s - loss: 0.1782 - val_loss: 0.2215\n",
      "Epoch 101/300\n",
      "379/379 [==============================] - 0s - loss: 0.1821 - val_loss: 0.2160\n",
      "Epoch 102/300\n",
      "379/379 [==============================] - 0s - loss: 0.1717 - val_loss: 0.3208\n",
      "Epoch 103/300\n",
      "379/379 [==============================] - 0s - loss: 0.1724 - val_loss: 0.2074\n",
      "Epoch 104/300\n",
      "379/379 [==============================] - 0s - loss: 0.1660 - val_loss: 0.2100\n",
      "Epoch 105/300\n",
      "379/379 [==============================] - 0s - loss: 0.1637 - val_loss: 0.2987\n",
      "Epoch 106/300\n",
      "379/379 [==============================] - 0s - loss: 0.1772 - val_loss: 0.4451\n",
      "Epoch 107/300\n",
      "379/379 [==============================] - 0s - loss: 0.1656 - val_loss: 0.2088\n",
      "Epoch 108/300\n",
      "379/379 [==============================] - 0s - loss: 0.1779 - val_loss: 0.2407\n",
      "Epoch 109/300\n",
      "379/379 [==============================] - 0s - loss: 0.1602 - val_loss: 0.2326\n",
      "Epoch 110/300\n",
      "379/379 [==============================] - 0s - loss: 0.1623 - val_loss: 0.1963\n",
      "Epoch 111/300\n",
      "379/379 [==============================] - 0s - loss: 0.1646 - val_loss: 0.2201\n",
      "Epoch 112/300\n",
      "379/379 [==============================] - 0s - loss: 0.1663 - val_loss: 0.2695\n",
      "Epoch 113/300\n",
      "379/379 [==============================] - 0s - loss: 0.1639 - val_loss: 0.2849\n",
      "Epoch 114/300\n",
      "379/379 [==============================] - 0s - loss: 0.1614 - val_loss: 0.1792\n",
      "Epoch 115/300\n",
      "379/379 [==============================] - 0s - loss: 0.1586 - val_loss: 0.2310\n",
      "Epoch 116/300\n",
      "379/379 [==============================] - 0s - loss: 0.1664 - val_loss: 0.2137\n",
      "Epoch 117/300\n",
      "379/379 [==============================] - 0s - loss: 0.1545 - val_loss: 0.2006\n",
      "Epoch 118/300\n",
      "379/379 [==============================] - 0s - loss: 0.1541 - val_loss: 0.4870\n",
      "Epoch 119/300\n",
      "379/379 [==============================] - 0s - loss: 0.1623 - val_loss: 0.3415\n",
      "Epoch 120/300\n",
      "379/379 [==============================] - 0s - loss: 0.1628 - val_loss: 0.1778\n",
      "Epoch 121/300\n",
      "379/379 [==============================] - 0s - loss: 0.1498 - val_loss: 0.1685\n",
      "Epoch 122/300\n",
      "379/379 [==============================] - 0s - loss: 0.1578 - val_loss: 0.2260\n",
      "Epoch 123/300\n",
      "379/379 [==============================] - 0s - loss: 0.1518 - val_loss: 0.1725\n",
      "Epoch 124/300\n",
      "379/379 [==============================] - 0s - loss: 0.1578 - val_loss: 0.2179\n",
      "Epoch 125/300\n",
      "379/379 [==============================] - 0s - loss: 0.1447 - val_loss: 0.1801\n",
      "Epoch 126/300\n",
      "379/379 [==============================] - 0s - loss: 0.1546 - val_loss: 0.1999\n",
      "Epoch 127/300\n",
      "379/379 [==============================] - 0s - loss: 0.1522 - val_loss: 0.2751\n",
      "Epoch 128/300\n",
      "379/379 [==============================] - 0s - loss: 0.1473 - val_loss: 0.1936\n",
      "Epoch 129/300\n",
      "379/379 [==============================] - 0s - loss: 0.1450 - val_loss: 0.2141\n",
      "Epoch 130/300\n",
      "379/379 [==============================] - 0s - loss: 0.1475 - val_loss: 0.1627\n",
      "Epoch 131/300\n",
      "379/379 [==============================] - 0s - loss: 0.1456 - val_loss: 0.2094\n",
      "Epoch 132/300\n",
      "379/379 [==============================] - 0s - loss: 0.1425 - val_loss: 0.2178\n",
      "Epoch 133/300\n",
      "379/379 [==============================] - 0s - loss: 0.1390 - val_loss: 0.1595\n",
      "Epoch 134/300\n",
      "379/379 [==============================] - 0s - loss: 0.1447 - val_loss: 0.1845\n",
      "Epoch 135/300\n",
      "379/379 [==============================] - 0s - loss: 0.1415 - val_loss: 0.4366\n",
      "Epoch 136/300\n",
      "379/379 [==============================] - 0s - loss: 0.1362 - val_loss: 0.2475\n",
      "Epoch 137/300\n",
      "379/379 [==============================] - 0s - loss: 0.1454 - val_loss: 0.1756\n",
      "Epoch 138/300\n",
      "379/379 [==============================] - 0s - loss: 0.1394 - val_loss: 0.2121\n",
      "Epoch 139/300\n",
      "379/379 [==============================] - 0s - loss: 0.1391 - val_loss: 0.1621\n",
      "Epoch 140/300\n",
      "379/379 [==============================] - 0s - loss: 0.1349 - val_loss: 0.2603\n",
      "Epoch 141/300\n",
      "379/379 [==============================] - 0s - loss: 0.1399 - val_loss: 0.2986\n",
      "Epoch 142/300\n",
      "379/379 [==============================] - 0s - loss: 0.1599 - val_loss: 0.1849\n",
      "Epoch 143/300\n",
      "379/379 [==============================] - 0s - loss: 0.1368 - val_loss: 0.1790\n",
      "Epoch 144/300\n",
      "379/379 [==============================] - 0s - loss: 0.1352 - val_loss: 0.2299\n",
      "Epoch 145/300\n",
      "379/379 [==============================] - 0s - loss: 0.1416 - val_loss: 0.1768\n",
      "Epoch 146/300\n",
      "379/379 [==============================] - 0s - loss: 0.1355 - val_loss: 0.2081\n",
      "Epoch 147/300\n",
      "379/379 [==============================] - 0s - loss: 0.1329 - val_loss: 0.1673\n",
      "Epoch 148/300\n",
      "379/379 [==============================] - 0s - loss: 0.1363 - val_loss: 0.1914\n",
      "Epoch 149/300\n",
      "379/379 [==============================] - 0s - loss: 0.1310 - val_loss: 0.2088\n",
      "Epoch 150/300\n",
      "379/379 [==============================] - 0s - loss: 0.1301 - val_loss: 0.1499\n",
      "Epoch 151/300\n",
      "379/379 [==============================] - 0s - loss: 0.1315 - val_loss: 0.1921\n",
      "Epoch 152/300\n",
      "379/379 [==============================] - 0s - loss: 0.1434 - val_loss: 0.1816\n",
      "Epoch 153/300\n",
      "379/379 [==============================] - 0s - loss: 0.1293 - val_loss: 0.1918\n",
      "Epoch 154/300\n",
      "379/379 [==============================] - 0s - loss: 0.1263 - val_loss: 0.2116\n",
      "Epoch 155/300\n",
      "379/379 [==============================] - 0s - loss: 0.1325 - val_loss: 0.2093\n",
      "Epoch 156/300\n",
      "379/379 [==============================] - 0s - loss: 0.1280 - val_loss: 0.2371\n",
      "Epoch 157/300\n",
      "379/379 [==============================] - 0s - loss: 0.1288 - val_loss: 0.1797\n",
      "Epoch 158/300\n",
      "379/379 [==============================] - 0s - loss: 0.1256 - val_loss: 0.2231\n",
      "Epoch 159/300\n",
      "379/379 [==============================] - 0s - loss: 0.1238 - val_loss: 0.2302\n",
      "Epoch 160/300\n",
      "379/379 [==============================] - 0s - loss: 0.1290 - val_loss: 0.1704\n",
      "Epoch 161/300\n",
      "379/379 [==============================] - 0s - loss: 0.1233 - val_loss: 0.2673\n",
      "Epoch 162/300\n",
      "379/379 [==============================] - 0s - loss: 0.1199 - val_loss: 0.1893\n",
      "Epoch 163/300\n",
      "379/379 [==============================] - 0s - loss: 0.1244 - val_loss: 0.2259\n",
      "Epoch 164/300\n",
      "379/379 [==============================] - 0s - loss: 0.1245 - val_loss: 0.1609\n",
      "Epoch 165/300\n",
      "379/379 [==============================] - 0s - loss: 0.1320 - val_loss: 0.1876\n",
      "Epoch 166/300\n",
      "379/379 [==============================] - 0s - loss: 0.1191 - val_loss: 0.1854\n",
      "Epoch 167/300\n",
      "379/379 [==============================] - 0s - loss: 0.1238 - val_loss: 0.2300\n",
      "Epoch 168/300\n",
      "379/379 [==============================] - 0s - loss: 0.1167 - val_loss: 0.1592\n",
      "Epoch 169/300\n",
      "379/379 [==============================] - 0s - loss: 0.1202 - val_loss: 0.2209\n",
      "Epoch 170/300\n",
      "379/379 [==============================] - 0s - loss: 0.1177 - val_loss: 0.1695\n",
      "Epoch 171/300\n",
      "379/379 [==============================] - 0s - loss: 0.1155 - val_loss: 0.1533\n",
      "Epoch 172/300\n",
      "379/379 [==============================] - 0s - loss: 0.1170 - val_loss: 0.1713\n",
      "Epoch 173/300\n",
      "379/379 [==============================] - 0s - loss: 0.1186 - val_loss: 0.1472\n",
      "Epoch 174/300\n",
      "379/379 [==============================] - 0s - loss: 0.1209 - val_loss: 0.3188\n",
      "Epoch 175/300\n",
      "379/379 [==============================] - 0s - loss: 0.1220 - val_loss: 0.1719\n",
      "Epoch 176/300\n",
      "379/379 [==============================] - 0s - loss: 0.1174 - val_loss: 0.2441\n",
      "Epoch 177/300\n",
      "379/379 [==============================] - 0s - loss: 0.1212 - val_loss: 0.1532\n",
      "Epoch 178/300\n",
      "379/379 [==============================] - 0s - loss: 0.1176 - val_loss: 0.1686\n",
      "Epoch 179/300\n",
      "379/379 [==============================] - 0s - loss: 0.1139 - val_loss: 0.2554\n",
      "Epoch 180/300\n",
      "379/379 [==============================] - 0s - loss: 0.1169 - val_loss: 0.1978\n",
      "Epoch 181/300\n",
      "379/379 [==============================] - 0s - loss: 0.1125 - val_loss: 0.1682\n",
      "Epoch 182/300\n",
      "379/379 [==============================] - 0s - loss: 0.1130 - val_loss: 0.2218\n",
      "Epoch 183/300\n",
      "379/379 [==============================] - 0s - loss: 0.1188 - val_loss: 0.2355\n",
      "Epoch 184/300\n",
      "379/379 [==============================] - 0s - loss: 0.1168 - val_loss: 0.1462\n",
      "Epoch 185/300\n",
      "379/379 [==============================] - 0s - loss: 0.1108 - val_loss: 0.1987\n",
      "Epoch 186/300\n",
      "379/379 [==============================] - 0s - loss: 0.1179 - val_loss: 0.1864\n",
      "Epoch 187/300\n",
      "379/379 [==============================] - 0s - loss: 0.1229 - val_loss: 0.1576\n",
      "Epoch 188/300\n",
      "379/379 [==============================] - 0s - loss: 0.1157 - val_loss: 0.1719\n",
      "Epoch 189/300\n",
      "379/379 [==============================] - 0s - loss: 0.1133 - val_loss: 0.1885\n",
      "Epoch 190/300\n",
      "379/379 [==============================] - 0s - loss: 0.1090 - val_loss: 0.1789\n",
      "Epoch 191/300\n",
      "379/379 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1581\n",
      "Epoch 192/300\n",
      "379/379 [==============================] - 0s - loss: 0.1184 - val_loss: 0.2224\n",
      "Epoch 193/300\n",
      "379/379 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1806\n",
      "Epoch 194/300\n",
      "379/379 [==============================] - 0s - loss: 0.1046 - val_loss: 0.2022\n",
      "Epoch 195/300\n",
      "379/379 [==============================] - 0s - loss: 0.1082 - val_loss: 0.1954\n",
      "Epoch 196/300\n",
      "379/379 [==============================] - 0s - loss: 0.1069 - val_loss: 0.1580\n",
      "Epoch 197/300\n",
      "379/379 [==============================] - 0s - loss: 0.1097 - val_loss: 0.2203\n",
      "Epoch 198/300\n",
      "379/379 [==============================] - 0s - loss: 0.1038 - val_loss: 0.2108\n",
      "Epoch 199/300\n",
      "379/379 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1705\n",
      "Epoch 200/300\n",
      "379/379 [==============================] - 0s - loss: 0.1090 - val_loss: 0.2118\n",
      "Epoch 201/300\n",
      "379/379 [==============================] - 0s - loss: 0.1059 - val_loss: 0.1955\n",
      "Epoch 202/300\n",
      "379/379 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1357\n",
      "Epoch 203/300\n",
      "379/379 [==============================] - 0s - loss: 0.1099 - val_loss: 0.1881\n",
      "Epoch 204/300\n",
      "379/379 [==============================] - 0s - loss: 0.1002 - val_loss: 0.2565\n",
      "Epoch 205/300\n",
      "379/379 [==============================] - 0s - loss: 0.1048 - val_loss: 0.1896\n",
      "Epoch 206/300\n",
      "379/379 [==============================] - 0s - loss: 0.1088 - val_loss: 0.1613\n",
      "Epoch 207/300\n",
      "379/379 [==============================] - 0s - loss: 0.1040 - val_loss: 0.1587\n",
      "Epoch 208/300\n",
      "379/379 [==============================] - 0s - loss: 0.0985 - val_loss: 0.1477\n",
      "Epoch 209/300\n",
      "379/379 [==============================] - 0s - loss: 0.1028 - val_loss: 0.1831\n",
      "Epoch 210/300\n",
      "379/379 [==============================] - 0s - loss: 0.1023 - val_loss: 0.1752\n",
      "Epoch 211/300\n",
      "379/379 [==============================] - 0s - loss: 0.1008 - val_loss: 0.1857\n",
      "Epoch 212/300\n",
      "379/379 [==============================] - 0s - loss: 0.1029 - val_loss: 0.2137\n",
      "Epoch 213/300\n",
      "379/379 [==============================] - 0s - loss: 0.1044 - val_loss: 0.1579\n",
      "Epoch 214/300\n",
      "379/379 [==============================] - 0s - loss: 0.1063 - val_loss: 0.1692\n",
      "Epoch 215/300\n",
      "379/379 [==============================] - 0s - loss: 0.1057 - val_loss: 0.1508\n",
      "Epoch 216/300\n",
      "379/379 [==============================] - 0s - loss: 0.0997 - val_loss: 0.1907\n",
      "Epoch 217/300\n",
      "379/379 [==============================] - 0s - loss: 0.1032 - val_loss: 0.1598\n",
      "Epoch 218/300\n",
      "379/379 [==============================] - 0s - loss: 0.1001 - val_loss: 0.1939\n",
      "Epoch 219/300\n",
      "379/379 [==============================] - 0s - loss: 0.1034 - val_loss: 0.1474\n",
      "Epoch 220/300\n",
      "379/379 [==============================] - 0s - loss: 0.1002 - val_loss: 0.2117\n",
      "Epoch 221/300\n",
      "379/379 [==============================] - 0s - loss: 0.0982 - val_loss: 0.1711\n",
      "Epoch 222/300\n",
      "379/379 [==============================] - 0s - loss: 0.1103 - val_loss: 0.1842\n",
      "Epoch 223/300\n",
      "379/379 [==============================] - 0s - loss: 0.0959 - val_loss: 0.1784\n",
      "Epoch 224/300\n",
      "379/379 [==============================] - 0s - loss: 0.0960 - val_loss: 0.1578\n",
      "Epoch 225/300\n",
      "379/379 [==============================] - 0s - loss: 0.0973 - val_loss: 0.2091\n",
      "Epoch 226/300\n",
      "379/379 [==============================] - 0s - loss: 0.0959 - val_loss: 0.1363\n",
      "Epoch 227/300\n",
      "379/379 [==============================] - 0s - loss: 0.0972 - val_loss: 0.1620\n",
      "Epoch 228/300\n",
      "379/379 [==============================] - 0s - loss: 0.0975 - val_loss: 0.1860\n",
      "Epoch 229/300\n",
      "379/379 [==============================] - 0s - loss: 0.0941 - val_loss: 0.1656\n",
      "Epoch 230/300\n",
      "379/379 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1656\n",
      "Epoch 231/300\n",
      "379/379 [==============================] - 0s - loss: 0.0929 - val_loss: 0.2032\n",
      "Epoch 232/300\n",
      "379/379 [==============================] - 0s - loss: 0.0914 - val_loss: 0.2580\n",
      "Epoch 233/300\n",
      "379/379 [==============================] - 0s - loss: 0.0950 - val_loss: 0.1501\n",
      "Epoch 234/300\n",
      "379/379 [==============================] - 0s - loss: 0.0912 - val_loss: 0.1960\n",
      "Epoch 235/300\n",
      "379/379 [==============================] - 0s - loss: 0.0976 - val_loss: 0.1793\n",
      "Epoch 236/300\n",
      "379/379 [==============================] - 0s - loss: 0.0941 - val_loss: 0.1717\n",
      "Epoch 237/300\n",
      "379/379 [==============================] - 0s - loss: 0.0952 - val_loss: 0.1730\n",
      "Epoch 238/300\n",
      "379/379 [==============================] - 0s - loss: 0.0915 - val_loss: 0.1743\n",
      "Epoch 239/300\n",
      "379/379 [==============================] - 0s - loss: 0.0928 - val_loss: 0.2416\n",
      "Epoch 240/300\n",
      "379/379 [==============================] - 0s - loss: 0.0938 - val_loss: 0.1850\n",
      "Epoch 241/300\n",
      "379/379 [==============================] - 0s - loss: 0.0926 - val_loss: 0.1425\n",
      "Epoch 242/300\n",
      "379/379 [==============================] - 0s - loss: 0.0927 - val_loss: 0.1845\n",
      "Epoch 243/300\n",
      "379/379 [==============================] - 0s - loss: 0.0951 - val_loss: 0.1755\n",
      "Epoch 244/300\n",
      "379/379 [==============================] - 0s - loss: 0.0925 - val_loss: 0.1441\n",
      "Epoch 245/300\n",
      "379/379 [==============================] - 0s - loss: 0.0914 - val_loss: 0.1600\n",
      "Epoch 246/300\n",
      "379/379 [==============================] - 0s - loss: 0.0935 - val_loss: 0.1550\n",
      "Epoch 247/300\n",
      "379/379 [==============================] - 0s - loss: 0.0896 - val_loss: 0.1359\n",
      "Epoch 248/300\n",
      "379/379 [==============================] - 0s - loss: 0.0886 - val_loss: 0.1299\n",
      "Epoch 249/300\n",
      "379/379 [==============================] - 0s - loss: 0.0893 - val_loss: 0.1519\n",
      "Epoch 250/300\n",
      "379/379 [==============================] - 0s - loss: 0.0887 - val_loss: 0.1687\n",
      "Epoch 251/300\n",
      "379/379 [==============================] - 0s - loss: 0.0931 - val_loss: 0.1858\n",
      "Epoch 252/300\n",
      "379/379 [==============================] - 0s - loss: 0.0883 - val_loss: 0.2957\n",
      "Epoch 253/300\n",
      "379/379 [==============================] - 0s - loss: 0.0887 - val_loss: 0.2386\n",
      "Epoch 254/300\n",
      "379/379 [==============================] - 0s - loss: 0.0889 - val_loss: 0.1647\n",
      "Epoch 255/300\n",
      "379/379 [==============================] - 0s - loss: 0.0865 - val_loss: 0.2074\n",
      "Epoch 256/300\n",
      "379/379 [==============================] - 0s - loss: 0.0854 - val_loss: 0.1522\n",
      "Epoch 257/300\n",
      "379/379 [==============================] - 0s - loss: 0.0892 - val_loss: 0.2015\n",
      "Epoch 258/300\n",
      "379/379 [==============================] - 0s - loss: 0.0893 - val_loss: 0.1499\n",
      "Epoch 259/300\n",
      "379/379 [==============================] - 0s - loss: 0.0922 - val_loss: 0.1752\n",
      "Epoch 260/300\n",
      "379/379 [==============================] - 0s - loss: 0.0866 - val_loss: 0.1354\n",
      "Epoch 261/300\n",
      "379/379 [==============================] - 0s - loss: 0.0882 - val_loss: 0.2128\n",
      "Epoch 262/300\n",
      "379/379 [==============================] - 0s - loss: 0.0885 - val_loss: 0.1637\n",
      "Epoch 263/300\n",
      "379/379 [==============================] - 0s - loss: 0.0856 - val_loss: 0.1744\n",
      "Epoch 264/300\n",
      "379/379 [==============================] - 0s - loss: 0.0870 - val_loss: 0.1372\n",
      "Epoch 265/300\n",
      "379/379 [==============================] - 0s - loss: 0.0862 - val_loss: 0.1375\n",
      "Epoch 266/300\n",
      "379/379 [==============================] - 0s - loss: 0.0863 - val_loss: 0.2113\n",
      "Epoch 267/300\n",
      "379/379 [==============================] - 0s - loss: 0.0837 - val_loss: 0.2518\n",
      "Epoch 268/300\n",
      "379/379 [==============================] - 0s - loss: 0.0888 - val_loss: 0.1627\n",
      "Epoch 269/300\n",
      "379/379 [==============================] - 0s - loss: 0.0839 - val_loss: 0.1075\n",
      "Epoch 270/300\n",
      "379/379 [==============================] - 0s - loss: 0.0872 - val_loss: 0.1220\n",
      "Epoch 271/300\n",
      "379/379 [==============================] - 0s - loss: 0.0849 - val_loss: 0.1807\n",
      "Epoch 272/300\n",
      "379/379 [==============================] - 0s - loss: 0.0905 - val_loss: 0.1823\n",
      "Epoch 273/300\n",
      "379/379 [==============================] - 0s - loss: 0.0861 - val_loss: 0.1366\n",
      "Epoch 274/300\n",
      "379/379 [==============================] - 0s - loss: 0.0845 - val_loss: 0.1560\n",
      "Epoch 275/300\n",
      "379/379 [==============================] - 0s - loss: 0.0845 - val_loss: 0.1244\n",
      "Epoch 276/300\n",
      "379/379 [==============================] - 0s - loss: 0.0855 - val_loss: 0.1401\n",
      "Epoch 277/300\n",
      "379/379 [==============================] - 0s - loss: 0.0848 - val_loss: 0.1589\n",
      "Epoch 278/300\n",
      "379/379 [==============================] - 0s - loss: 0.0839 - val_loss: 0.2094\n",
      "Epoch 279/300\n",
      "379/379 [==============================] - 0s - loss: 0.0843 - val_loss: 0.1475\n",
      "Epoch 280/300\n",
      "379/379 [==============================] - 0s - loss: 0.0831 - val_loss: 0.1938\n",
      "Epoch 281/300\n",
      "379/379 [==============================] - 0s - loss: 0.0819 - val_loss: 0.1679\n",
      "Epoch 282/300\n",
      "379/379 [==============================] - 0s - loss: 0.0807 - val_loss: 0.1224\n",
      "Epoch 283/300\n",
      "379/379 [==============================] - 0s - loss: 0.0841 - val_loss: 0.1346\n",
      "Epoch 284/300\n",
      "379/379 [==============================] - 0s - loss: 0.0835 - val_loss: 0.1859\n",
      "Epoch 285/300\n",
      "379/379 [==============================] - 0s - loss: 0.0813 - val_loss: 0.1735\n",
      "Epoch 286/300\n",
      "379/379 [==============================] - 0s - loss: 0.0804 - val_loss: 0.1607\n",
      "Epoch 287/300\n",
      "379/379 [==============================] - 0s - loss: 0.0821 - val_loss: 0.2227\n",
      "Epoch 288/300\n",
      "379/379 [==============================] - 0s - loss: 0.0831 - val_loss: 0.2019\n",
      "Epoch 289/300\n",
      "379/379 [==============================] - 0s - loss: 0.0811 - val_loss: 0.1785\n",
      "Epoch 290/300\n",
      "379/379 [==============================] - 0s - loss: 0.0788 - val_loss: 0.1360\n",
      "Epoch 291/300\n",
      "379/379 [==============================] - 0s - loss: 0.0831 - val_loss: 0.1945\n",
      "Epoch 292/300\n",
      "379/379 [==============================] - 0s - loss: 0.0795 - val_loss: 0.1982\n",
      "Epoch 293/300\n",
      "379/379 [==============================] - 0s - loss: 0.0823 - val_loss: 0.1870\n",
      "Epoch 294/300\n",
      "379/379 [==============================] - 0s - loss: 0.0809 - val_loss: 0.1435\n",
      "Epoch 295/300\n",
      "379/379 [==============================] - 0s - loss: 0.0800 - val_loss: 0.1615\n",
      "Epoch 296/300\n",
      "379/379 [==============================] - 0s - loss: 0.0782 - val_loss: 0.2070\n",
      "Epoch 297/300\n",
      "379/379 [==============================] - 0s - loss: 0.0762 - val_loss: 0.1361\n",
      "Epoch 298/300\n",
      "379/379 [==============================] - 0s - loss: 0.0801 - val_loss: 0.1638\n",
      "Epoch 299/300\n",
      "379/379 [==============================] - 0s - loss: 0.0781 - val_loss: 0.1446\n",
      "Epoch 300/300\n",
      "379/379 [==============================] - 0s - loss: 0.0771 - val_loss: 0.1487\n"
     ]
    }
   ],
   "source": [
    "model2_sigmoid = Sequential()\n",
    "model2_sigmoid.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "model2_sigmoid.add(Activation('sigmoid'))\n",
    "model2_sigmoid.add(Dense(1, init='uniform'))\n",
    "model2_sigmoid.add(Activation('linear'))\n",
    "\n",
    "sgd = SGD(lr=0.2)\n",
    "model2_sigmoid.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "\n",
    "hist_sigmoid = model2_sigmoid.fit(X_train_scaled.as_matrix(), y_train_scaled.as_matrix(), nb_epoch=300, verbose=1, validation_data=(X_test_scaled.as_matrix(), y_test_scaled.as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114eaf9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_sigmoid.history['loss'])\n",
    "plt.title('Square Error - Sigmoid')\n",
    "plt.xlabel('Ephocs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar en el gráfico que el error disminuye en función del número de *epochs* que se utilicen pero de forma asintótica. Con un valor de aproximadamente 50 *epoch* se obtiene ya un error aproximadamente mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/300\n",
      "379/379 [==============================] - 0s - loss: 262.7564 - val_loss: 30.1954\n",
      "Epoch 2/300\n",
      "379/379 [==============================] - 0s - loss: 14.5123 - val_loss: 11.7285\n",
      "Epoch 3/300\n",
      "379/379 [==============================] - 0s - loss: 6.2734 - val_loss: 5.7712\n",
      "Epoch 4/300\n",
      "379/379 [==============================] - 0s - loss: 3.6331 - val_loss: 3.9208\n",
      "Epoch 5/300\n",
      "379/379 [==============================] - 0s - loss: 2.4617 - val_loss: 3.3576\n",
      "Epoch 6/300\n",
      "379/379 [==============================] - 0s - loss: 1.7917 - val_loss: 2.9982\n",
      "Epoch 7/300\n",
      "379/379 [==============================] - 0s - loss: 1.4611 - val_loss: 1.6607\n",
      "Epoch 8/300\n",
      "379/379 [==============================] - 0s - loss: 1.1252 - val_loss: 1.4218\n",
      "Epoch 9/300\n",
      "379/379 [==============================] - 0s - loss: 1.0055 - val_loss: 1.2230\n",
      "Epoch 10/300\n",
      "379/379 [==============================] - 0s - loss: 1.0434 - val_loss: 1.1703\n",
      "Epoch 11/300\n",
      "379/379 [==============================] - 0s - loss: 0.7622 - val_loss: 1.6772\n",
      "Epoch 12/300\n",
      "379/379 [==============================] - 0s - loss: 0.6786 - val_loss: 1.5099\n",
      "Epoch 13/300\n",
      "379/379 [==============================] - 0s - loss: 0.6284 - val_loss: 1.0377\n",
      "Epoch 14/300\n",
      "379/379 [==============================] - 0s - loss: 0.5793 - val_loss: 1.8484\n",
      "Epoch 15/300\n",
      "379/379 [==============================] - 0s - loss: 0.6058 - val_loss: 1.2636\n",
      "Epoch 16/300\n",
      "379/379 [==============================] - 0s - loss: 0.5638 - val_loss: 0.8561\n",
      "Epoch 17/300\n",
      "379/379 [==============================] - 0s - loss: 0.4769 - val_loss: 0.7720\n",
      "Epoch 18/300\n",
      "379/379 [==============================] - 0s - loss: 0.3601 - val_loss: 0.8756\n",
      "Epoch 19/300\n",
      "379/379 [==============================] - 0s - loss: 0.3549 - val_loss: 1.0409\n",
      "Epoch 20/300\n",
      "379/379 [==============================] - 0s - loss: 0.5888 - val_loss: 0.8061\n",
      "Epoch 21/300\n",
      "379/379 [==============================] - 0s - loss: 0.6145 - val_loss: 1.1841\n",
      "Epoch 22/300\n",
      "379/379 [==============================] - 0s - loss: 0.4257 - val_loss: 0.7231\n",
      "Epoch 23/300\n",
      "379/379 [==============================] - 0s - loss: 0.3097 - val_loss: 0.6056\n",
      "Epoch 24/300\n",
      "379/379 [==============================] - 0s - loss: 0.2631 - val_loss: 0.5477\n",
      "Epoch 25/300\n",
      "379/379 [==============================] - 0s - loss: 0.3099 - val_loss: 0.5488\n",
      "Epoch 26/300\n",
      "379/379 [==============================] - 0s - loss: 1.0238 - val_loss: 1.1185\n",
      "Epoch 27/300\n",
      "379/379 [==============================] - 0s - loss: 0.6001 - val_loss: 0.5397\n",
      "Epoch 28/300\n",
      "379/379 [==============================] - 0s - loss: 0.2718 - val_loss: 0.4001\n",
      "Epoch 29/300\n",
      "379/379 [==============================] - 0s - loss: 0.2509 - val_loss: 0.7105\n",
      "Epoch 30/300\n",
      "379/379 [==============================] - 0s - loss: 0.6031 - val_loss: 2.2095\n",
      "Epoch 31/300\n",
      "379/379 [==============================] - 0s - loss: 0.4560 - val_loss: 0.4759\n",
      "Epoch 32/300\n",
      "379/379 [==============================] - 0s - loss: 0.2552 - val_loss: 0.6746\n",
      "Epoch 33/300\n",
      "379/379 [==============================] - 0s - loss: 0.1698 - val_loss: 0.4097\n",
      "Epoch 34/300\n",
      "379/379 [==============================] - 0s - loss: 0.1735 - val_loss: 0.5390\n",
      "Epoch 35/300\n",
      "379/379 [==============================] - 0s - loss: 0.1502 - val_loss: 0.4870\n",
      "Epoch 36/300\n",
      "379/379 [==============================] - 0s - loss: 0.1548 - val_loss: 0.4476\n",
      "Epoch 37/300\n",
      "379/379 [==============================] - 0s - loss: 0.1836 - val_loss: 0.4893\n",
      "Epoch 38/300\n",
      "379/379 [==============================] - 0s - loss: 0.1344 - val_loss: 0.4602\n",
      "Epoch 39/300\n",
      "379/379 [==============================] - 0s - loss: 0.1353 - val_loss: 0.4428\n",
      "Epoch 40/300\n",
      "379/379 [==============================] - 0s - loss: 0.1468 - val_loss: 0.6919\n",
      "Epoch 41/300\n",
      "379/379 [==============================] - 0s - loss: 0.1704 - val_loss: 0.3475\n",
      "Epoch 42/300\n",
      "379/379 [==============================] - 0s - loss: 0.2160 - val_loss: 0.2861\n",
      "Epoch 43/300\n",
      "379/379 [==============================] - 0s - loss: 0.2812 - val_loss: 0.5570\n",
      "Epoch 44/300\n",
      "379/379 [==============================] - 0s - loss: 0.5303 - val_loss: 0.7733\n",
      "Epoch 45/300\n",
      "379/379 [==============================] - 0s - loss: 0.3868 - val_loss: 1.4416\n",
      "Epoch 46/300\n",
      "379/379 [==============================] - 0s - loss: 0.6014 - val_loss: 1.0605\n",
      "Epoch 47/300\n",
      "379/379 [==============================] - 0s - loss: 0.2627 - val_loss: 0.3830\n",
      "Epoch 48/300\n",
      "379/379 [==============================] - 0s - loss: 0.0970 - val_loss: 0.3334\n",
      "Epoch 49/300\n",
      "379/379 [==============================] - 0s - loss: 0.1808 - val_loss: 0.4019\n",
      "Epoch 50/300\n",
      "379/379 [==============================] - 0s - loss: 0.1818 - val_loss: 0.5677\n",
      "Epoch 51/300\n",
      "379/379 [==============================] - 0s - loss: 0.0944 - val_loss: 0.2694\n",
      "Epoch 52/300\n",
      "379/379 [==============================] - 0s - loss: 0.1127 - val_loss: 0.2858\n",
      "Epoch 53/300\n",
      "379/379 [==============================] - 0s - loss: 0.2533 - val_loss: 0.2960\n",
      "Epoch 54/300\n",
      "379/379 [==============================] - 0s - loss: 0.2665 - val_loss: 0.2990\n",
      "Epoch 55/300\n",
      "379/379 [==============================] - 0s - loss: 0.1029 - val_loss: 0.2528\n",
      "Epoch 56/300\n",
      "379/379 [==============================] - 0s - loss: 0.0852 - val_loss: 0.4232\n",
      "Epoch 57/300\n",
      "379/379 [==============================] - 0s - loss: 0.0681 - val_loss: 0.3626\n",
      "Epoch 58/300\n",
      "379/379 [==============================] - 0s - loss: 0.0677 - val_loss: 0.2466\n",
      "Epoch 59/300\n",
      "379/379 [==============================] - 0s - loss: 0.0814 - val_loss: 0.3775\n",
      "Epoch 60/300\n",
      "379/379 [==============================] - 0s - loss: 0.1636 - val_loss: 0.6362\n",
      "Epoch 61/300\n",
      "379/379 [==============================] - 0s - loss: 0.3211 - val_loss: 0.3180\n",
      "Epoch 62/300\n",
      "379/379 [==============================] - 0s - loss: 0.1373 - val_loss: 0.2115\n",
      "Epoch 63/300\n",
      "379/379 [==============================] - 0s - loss: 0.0998 - val_loss: 0.2181\n",
      "Epoch 64/300\n",
      "379/379 [==============================] - 0s - loss: 0.0569 - val_loss: 0.4881\n",
      "Epoch 65/300\n",
      "379/379 [==============================] - 0s - loss: 0.0820 - val_loss: 0.3819\n",
      "Epoch 66/300\n",
      "379/379 [==============================] - 0s - loss: 0.0902 - val_loss: 0.6751\n",
      "Epoch 67/300\n",
      "379/379 [==============================] - 0s - loss: 0.2111 - val_loss: 0.6857\n",
      "Epoch 68/300\n",
      "379/379 [==============================] - 0s - loss: 0.2904 - val_loss: 0.2671\n",
      "Epoch 69/300\n",
      "379/379 [==============================] - 0s - loss: 0.1619 - val_loss: 0.2636\n",
      "Epoch 70/300\n",
      "379/379 [==============================] - 0s - loss: 0.3067 - val_loss: 0.3382\n",
      "Epoch 71/300\n",
      "379/379 [==============================] - 0s - loss: 0.0550 - val_loss: 0.2219\n",
      "Epoch 72/300\n",
      "379/379 [==============================] - 0s - loss: 0.2893 - val_loss: 0.2915\n",
      "Epoch 73/300\n",
      "379/379 [==============================] - 0s - loss: 0.1883 - val_loss: 0.2452\n",
      "Epoch 74/300\n",
      "379/379 [==============================] - 0s - loss: 0.7585 - val_loss: 0.2145\n",
      "Epoch 75/300\n",
      "379/379 [==============================] - 0s - loss: 0.1130 - val_loss: 0.3849\n",
      "Epoch 76/300\n",
      "379/379 [==============================] - 0s - loss: 0.0482 - val_loss: 0.3020\n",
      "Epoch 77/300\n",
      "379/379 [==============================] - 0s - loss: 0.0546 - val_loss: 0.2917\n",
      "Epoch 78/300\n",
      "379/379 [==============================] - 0s - loss: 0.0390 - val_loss: 0.2681\n",
      "Epoch 79/300\n",
      "379/379 [==============================] - 0s - loss: 0.0431 - val_loss: 0.2348\n",
      "Epoch 80/300\n",
      "379/379 [==============================] - 0s - loss: 0.0519 - val_loss: 0.4007\n",
      "Epoch 81/300\n",
      "379/379 [==============================] - 0s - loss: 0.1302 - val_loss: 0.2589\n",
      "Epoch 82/300\n",
      "379/379 [==============================] - 0s - loss: 0.0522 - val_loss: 0.2898\n",
      "Epoch 83/300\n",
      "379/379 [==============================] - 0s - loss: 0.0648 - val_loss: 0.1770\n",
      "Epoch 84/300\n",
      "379/379 [==============================] - 0s - loss: 0.1361 - val_loss: 0.1614\n",
      "Epoch 85/300\n",
      "379/379 [==============================] - 0s - loss: 0.4110 - val_loss: 0.4797\n",
      "Epoch 86/300\n",
      "379/379 [==============================] - 0s - loss: 0.0608 - val_loss: 0.4905\n",
      "Epoch 87/300\n",
      "379/379 [==============================] - 0s - loss: 0.1148 - val_loss: 0.2857\n",
      "Epoch 88/300\n",
      "379/379 [==============================] - 0s - loss: 0.0484 - val_loss: 0.2684\n",
      "Epoch 89/300\n",
      "379/379 [==============================] - 0s - loss: 0.0491 - val_loss: 0.3182\n",
      "Epoch 90/300\n",
      "379/379 [==============================] - 0s - loss: 0.3588 - val_loss: 0.5012\n",
      "Epoch 91/300\n",
      "379/379 [==============================] - 0s - loss: 0.2802 - val_loss: 0.7532\n",
      "Epoch 92/300\n",
      "379/379 [==============================] - 0s - loss: 0.0798 - val_loss: 0.2521\n",
      "Epoch 93/300\n",
      "379/379 [==============================] - 0s - loss: 0.0387 - val_loss: 0.3643\n",
      "Epoch 94/300\n",
      "379/379 [==============================] - 0s - loss: 0.1146 - val_loss: 0.4078\n",
      "Epoch 95/300\n",
      "379/379 [==============================] - 0s - loss: 1.0523 - val_loss: 0.3204\n",
      "Epoch 96/300\n",
      "379/379 [==============================] - 0s - loss: 0.0549 - val_loss: 0.2377\n",
      "Epoch 97/300\n",
      "379/379 [==============================] - 0s - loss: 0.0901 - val_loss: 0.2950\n",
      "Epoch 98/300\n",
      "379/379 [==============================] - 0s - loss: 0.0692 - val_loss: 0.1642\n",
      "Epoch 99/300\n",
      "379/379 [==============================] - 0s - loss: 0.1209 - val_loss: 0.2348\n",
      "Epoch 100/300\n",
      "379/379 [==============================] - 0s - loss: 0.0359 - val_loss: 0.1843\n",
      "Epoch 101/300\n",
      "379/379 [==============================] - 0s - loss: 0.1373 - val_loss: 0.3735\n",
      "Epoch 102/300\n",
      "379/379 [==============================] - 0s - loss: 0.0641 - val_loss: 0.1883\n",
      "Epoch 103/300\n",
      "379/379 [==============================] - 0s - loss: 0.0285 - val_loss: 0.3054\n",
      "Epoch 104/300\n",
      "379/379 [==============================] - 0s - loss: 0.0538 - val_loss: 0.3341\n",
      "Epoch 105/300\n",
      "379/379 [==============================] - 0s - loss: 0.0308 - val_loss: 0.3253\n",
      "Epoch 106/300\n",
      "379/379 [==============================] - 0s - loss: 0.0381 - val_loss: 0.4156\n",
      "Epoch 107/300\n",
      "379/379 [==============================] - 0s - loss: 0.0425 - val_loss: 0.3314\n",
      "Epoch 108/300\n",
      "379/379 [==============================] - 0s - loss: 0.0464 - val_loss: 0.3542\n",
      "Epoch 109/300\n",
      "379/379 [==============================] - 0s - loss: 0.0402 - val_loss: 0.3473\n",
      "Epoch 110/300\n",
      "379/379 [==============================] - 0s - loss: 0.0293 - val_loss: 0.2329\n",
      "Epoch 111/300\n",
      "379/379 [==============================] - 0s - loss: 0.0395 - val_loss: 0.2965\n",
      "Epoch 112/300\n",
      "379/379 [==============================] - 0s - loss: 0.0258 - val_loss: 0.2670\n",
      "Epoch 113/300\n",
      "379/379 [==============================] - 0s - loss: 0.0343 - val_loss: 0.3256\n",
      "Epoch 114/300\n",
      "379/379 [==============================] - 0s - loss: 0.0433 - val_loss: 0.4610\n",
      "Epoch 115/300\n",
      "379/379 [==============================] - 0s - loss: 0.0452 - val_loss: 0.5061\n",
      "Epoch 116/300\n",
      "379/379 [==============================] - 0s - loss: 0.1167 - val_loss: 0.5647\n",
      "Epoch 117/300\n",
      "379/379 [==============================] - 0s - loss: 0.1172 - val_loss: 0.6744\n",
      "Epoch 118/300\n",
      "379/379 [==============================] - 0s - loss: 0.4022 - val_loss: 0.4772\n",
      "Epoch 119/300\n",
      "379/379 [==============================] - 0s - loss: 0.1455 - val_loss: 0.2284\n",
      "Epoch 120/300\n",
      "379/379 [==============================] - 0s - loss: 0.0332 - val_loss: 0.2994\n",
      "Epoch 121/300\n",
      "379/379 [==============================] - 0s - loss: 0.0511 - val_loss: 0.5928\n",
      "Epoch 122/300\n",
      "379/379 [==============================] - 0s - loss: 0.0790 - val_loss: 0.3420\n",
      "Epoch 123/300\n",
      "379/379 [==============================] - 0s - loss: 0.0646 - val_loss: 0.6993\n",
      "Epoch 124/300\n",
      "379/379 [==============================] - 0s - loss: 0.1677 - val_loss: 0.4782\n",
      "Epoch 125/300\n",
      "379/379 [==============================] - 0s - loss: 0.0385 - val_loss: 0.3275\n",
      "Epoch 126/300\n",
      "379/379 [==============================] - 0s - loss: 0.0306 - val_loss: 0.4212\n",
      "Epoch 127/300\n",
      "379/379 [==============================] - 0s - loss: 0.0302 - val_loss: 0.2744\n",
      "Epoch 128/300\n",
      "379/379 [==============================] - 0s - loss: 0.0247 - val_loss: 0.2201\n",
      "Epoch 129/300\n",
      "379/379 [==============================] - 0s - loss: 0.0861 - val_loss: 0.2340\n",
      "Epoch 130/300\n",
      "379/379 [==============================] - 0s - loss: 0.0428 - val_loss: 0.6130\n",
      "Epoch 131/300\n",
      "379/379 [==============================] - 0s - loss: 0.0565 - val_loss: 0.3096\n",
      "Epoch 132/300\n",
      "379/379 [==============================] - 0s - loss: 0.0215 - val_loss: 0.1849\n",
      "Epoch 133/300\n",
      "379/379 [==============================] - 0s - loss: 0.0377 - val_loss: 0.2150\n",
      "Epoch 134/300\n",
      "379/379 [==============================] - 0s - loss: 0.0268 - val_loss: 0.3675\n",
      "Epoch 135/300\n",
      "379/379 [==============================] - 0s - loss: 0.0343 - val_loss: 0.1993\n",
      "Epoch 136/300\n",
      "379/379 [==============================] - 0s - loss: 0.0915 - val_loss: 0.1868\n",
      "Epoch 137/300\n",
      "379/379 [==============================] - 0s - loss: 0.0345 - val_loss: 0.1964\n",
      "Epoch 138/300\n",
      "379/379 [==============================] - 0s - loss: 0.0224 - val_loss: 0.2502\n",
      "Epoch 139/300\n",
      "379/379 [==============================] - 0s - loss: 0.0205 - val_loss: 0.1834\n",
      "Epoch 140/300\n",
      "379/379 [==============================] - 0s - loss: 0.0249 - val_loss: 0.2922\n",
      "Epoch 141/300\n",
      "379/379 [==============================] - 0s - loss: 0.0447 - val_loss: 0.6485\n",
      "Epoch 142/300\n",
      "379/379 [==============================] - 0s - loss: 0.2258 - val_loss: 0.4663\n",
      "Epoch 143/300\n",
      "379/379 [==============================] - 0s - loss: 0.0881 - val_loss: 0.2985\n",
      "Epoch 144/300\n",
      "379/379 [==============================] - 0s - loss: 0.0185 - val_loss: 0.2675\n",
      "Epoch 145/300\n",
      "379/379 [==============================] - 0s - loss: 0.0177 - val_loss: 0.2622\n",
      "Epoch 146/300\n",
      "379/379 [==============================] - 0s - loss: 0.0238 - val_loss: 0.2690\n",
      "Epoch 147/300\n",
      "379/379 [==============================] - 0s - loss: 0.0351 - val_loss: 0.2848\n",
      "Epoch 148/300\n",
      "379/379 [==============================] - 0s - loss: 0.0184 - val_loss: 0.1867\n",
      "Epoch 149/300\n",
      "379/379 [==============================] - 0s - loss: 0.0315 - val_loss: 0.2935\n",
      "Epoch 150/300\n",
      "379/379 [==============================] - 0s - loss: 0.0302 - val_loss: 0.1546\n",
      "Epoch 151/300\n",
      "379/379 [==============================] - 0s - loss: 0.0817 - val_loss: 0.1695\n",
      "Epoch 152/300\n",
      "379/379 [==============================] - 0s - loss: 0.0198 - val_loss: 0.2968\n",
      "Epoch 153/300\n",
      "379/379 [==============================] - 0s - loss: 0.0369 - val_loss: 0.2564\n",
      "Epoch 154/300\n",
      "379/379 [==============================] - 0s - loss: 0.0594 - val_loss: 0.2021\n",
      "Epoch 155/300\n",
      "379/379 [==============================] - 0s - loss: 0.1429 - val_loss: 0.1358\n",
      "Epoch 156/300\n",
      "379/379 [==============================] - 0s - loss: 0.0388 - val_loss: 0.2008\n",
      "Epoch 157/300\n",
      "379/379 [==============================] - 0s - loss: 0.0277 - val_loss: 0.3507\n",
      "Epoch 158/300\n",
      "379/379 [==============================] - 0s - loss: 0.1571 - val_loss: 0.3460\n",
      "Epoch 159/300\n",
      "379/379 [==============================] - 0s - loss: 0.0312 - val_loss: 0.1996\n",
      "Epoch 160/300\n",
      "379/379 [==============================] - 0s - loss: 0.0250 - val_loss: 0.2117\n",
      "Epoch 161/300\n",
      "379/379 [==============================] - 0s - loss: 0.0188 - val_loss: 0.2389\n",
      "Epoch 162/300\n",
      "379/379 [==============================] - 0s - loss: 0.0129 - val_loss: 0.2201\n",
      "Epoch 163/300\n",
      "379/379 [==============================] - 0s - loss: 0.0164 - val_loss: 0.2581\n",
      "Epoch 164/300\n",
      "379/379 [==============================] - 0s - loss: 0.0270 - val_loss: 0.2806\n",
      "Epoch 165/300\n",
      "379/379 [==============================] - 0s - loss: 0.0285 - val_loss: 0.3599\n",
      "Epoch 166/300\n",
      "379/379 [==============================] - 0s - loss: 0.0350 - val_loss: 0.2077\n",
      "Epoch 167/300\n",
      "379/379 [==============================] - 0s - loss: 0.0186 - val_loss: 0.2786\n",
      "Epoch 168/300\n",
      "379/379 [==============================] - 0s - loss: 0.0223 - val_loss: 0.1838\n",
      "Epoch 169/300\n",
      "379/379 [==============================] - 0s - loss: 0.0317 - val_loss: 0.2048\n",
      "Epoch 170/300\n",
      "379/379 [==============================] - 0s - loss: 0.0216 - val_loss: 0.2295\n",
      "Epoch 171/300\n",
      "379/379 [==============================] - 0s - loss: 0.0122 - val_loss: 0.1922\n",
      "Epoch 172/300\n",
      "379/379 [==============================] - 0s - loss: 0.0261 - val_loss: 0.4972\n",
      "Epoch 173/300\n",
      "379/379 [==============================] - 0s - loss: 0.0895 - val_loss: 0.5281\n",
      "Epoch 174/300\n",
      "379/379 [==============================] - 0s - loss: 0.0671 - val_loss: 0.4697\n",
      "Epoch 175/300\n",
      "379/379 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2968\n",
      "Epoch 176/300\n",
      "379/379 [==============================] - 0s - loss: 0.0156 - val_loss: 0.2119\n",
      "Epoch 177/300\n",
      "379/379 [==============================] - 0s - loss: 0.0297 - val_loss: 0.2239\n",
      "Epoch 178/300\n",
      "379/379 [==============================] - 0s - loss: 0.0126 - val_loss: 0.2044\n",
      "Epoch 179/300\n",
      "379/379 [==============================] - 0s - loss: 0.0189 - val_loss: 0.2081\n",
      "Epoch 180/300\n",
      "379/379 [==============================] - 0s - loss: 0.0188 - val_loss: 0.2119\n",
      "Epoch 181/300\n",
      "379/379 [==============================] - 0s - loss: 0.0181 - val_loss: 0.2201\n",
      "Epoch 182/300\n",
      "379/379 [==============================] - 0s - loss: 0.0119 - val_loss: 0.2356\n",
      "Epoch 183/300\n",
      "379/379 [==============================] - 0s - loss: 0.0137 - val_loss: 0.3396\n",
      "Epoch 184/300\n",
      "379/379 [==============================] - 0s - loss: 0.0492 - val_loss: 0.3738\n",
      "Epoch 185/300\n",
      "379/379 [==============================] - 0s - loss: 0.0300 - val_loss: 0.2919\n",
      "Epoch 186/300\n",
      "379/379 [==============================] - 0s - loss: 0.0500 - val_loss: 0.5619\n",
      "Epoch 187/300\n",
      "379/379 [==============================] - 0s - loss: 0.1288 - val_loss: 0.4138\n",
      "Epoch 188/300\n",
      "379/379 [==============================] - 0s - loss: 0.0294 - val_loss: 0.2433\n",
      "Epoch 189/300\n",
      "379/379 [==============================] - 0s - loss: 0.0193 - val_loss: 0.2767\n",
      "Epoch 190/300\n",
      "379/379 [==============================] - 0s - loss: 0.0190 - val_loss: 0.2376\n",
      "Epoch 191/300\n",
      "379/379 [==============================] - 0s - loss: 0.0111 - val_loss: 0.2804\n",
      "Epoch 192/300\n",
      "379/379 [==============================] - 0s - loss: 0.0458 - val_loss: 0.7194\n",
      "Epoch 193/300\n",
      "379/379 [==============================] - 0s - loss: 0.1562 - val_loss: 0.9562\n",
      "Epoch 194/300\n",
      "379/379 [==============================] - 0s - loss: 0.6800 - val_loss: 0.4058\n",
      "Epoch 195/300\n",
      "379/379 [==============================] - 0s - loss: 0.0256 - val_loss: 0.3212\n",
      "Epoch 196/300\n",
      "379/379 [==============================] - 0s - loss: 0.0157 - val_loss: 0.2798\n",
      "Epoch 197/300\n",
      "379/379 [==============================] - 0s - loss: 0.0126 - val_loss: 0.2294\n",
      "Epoch 198/300\n",
      "379/379 [==============================] - 0s - loss: 0.0128 - val_loss: 0.1988\n",
      "Epoch 199/300\n",
      "379/379 [==============================] - 0s - loss: 0.0239 - val_loss: 0.3178\n",
      "Epoch 200/300\n",
      "379/379 [==============================] - 0s - loss: 0.0266 - val_loss: 0.2438\n",
      "Epoch 201/300\n",
      "379/379 [==============================] - 0s - loss: 0.0145 - val_loss: 0.2687\n",
      "Epoch 202/300\n",
      "379/379 [==============================] - 0s - loss: 0.0144 - val_loss: 0.2931\n",
      "Epoch 203/300\n",
      "379/379 [==============================] - 0s - loss: 0.0140 - val_loss: 0.2092\n",
      "Epoch 204/300\n",
      "379/379 [==============================] - 0s - loss: 0.0704 - val_loss: 0.1402\n",
      "Epoch 205/300\n",
      "379/379 [==============================] - 0s - loss: 0.0244 - val_loss: 0.2657\n",
      "Epoch 206/300\n",
      "379/379 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2538\n",
      "Epoch 207/300\n",
      "379/379 [==============================] - 0s - loss: 0.0156 - val_loss: 0.3066\n",
      "Epoch 208/300\n",
      "379/379 [==============================] - 0s - loss: 0.0129 - val_loss: 0.2122\n",
      "Epoch 209/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.1807\n",
      "Epoch 210/300\n",
      "379/379 [==============================] - 0s - loss: 0.0250 - val_loss: 0.2192\n",
      "Epoch 211/300\n",
      "379/379 [==============================] - 0s - loss: 0.0167 - val_loss: 0.1480\n",
      "Epoch 212/300\n",
      "379/379 [==============================] - 0s - loss: 0.0682 - val_loss: 0.1729\n",
      "Epoch 213/300\n",
      "379/379 [==============================] - 0s - loss: 0.2331 - val_loss: 0.1794\n",
      "Epoch 214/300\n",
      "379/379 [==============================] - 0s - loss: 0.1267 - val_loss: 0.1885\n",
      "Epoch 215/300\n",
      "379/379 [==============================] - 0s - loss: 0.0620 - val_loss: 0.2376\n",
      "Epoch 216/300\n",
      "379/379 [==============================] - 0s - loss: 0.0226 - val_loss: 0.2279\n",
      "Epoch 217/300\n",
      "379/379 [==============================] - 0s - loss: 0.0349 - val_loss: 0.4863\n",
      "Epoch 218/300\n",
      "379/379 [==============================] - 0s - loss: 0.0462 - val_loss: 0.2906\n",
      "Epoch 219/300\n",
      "379/379 [==============================] - 0s - loss: 0.0118 - val_loss: 0.1946\n",
      "Epoch 220/300\n",
      "379/379 [==============================] - 0s - loss: 0.0115 - val_loss: 0.1972\n",
      "Epoch 221/300\n",
      "379/379 [==============================] - 0s - loss: 0.1526 - val_loss: 0.2521\n",
      "Epoch 222/300\n",
      "379/379 [==============================] - 0s - loss: 0.0243 - val_loss: 0.2300\n",
      "Epoch 223/300\n",
      "379/379 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2419\n",
      "Epoch 224/300\n",
      "379/379 [==============================] - 0s - loss: 0.0217 - val_loss: 0.1655\n",
      "Epoch 225/300\n",
      "379/379 [==============================] - 0s - loss: 0.0608 - val_loss: 0.2028\n",
      "Epoch 226/300\n",
      "379/379 [==============================] - 0s - loss: 0.0100 - val_loss: 0.2508\n",
      "Epoch 227/300\n",
      "379/379 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2228\n",
      "Epoch 228/300\n",
      "379/379 [==============================] - 0s - loss: 0.0085 - val_loss: 0.1979\n",
      "Epoch 229/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2507\n",
      "Epoch 230/300\n",
      "379/379 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2647\n",
      "Epoch 231/300\n",
      "379/379 [==============================] - 0s - loss: 0.0330 - val_loss: 0.2941\n",
      "Epoch 232/300\n",
      "379/379 [==============================] - 0s - loss: 0.0191 - val_loss: 0.3213\n",
      "Epoch 233/300\n",
      "379/379 [==============================] - 0s - loss: 0.0105 - val_loss: 0.2141\n",
      "Epoch 234/300\n",
      "379/379 [==============================] - 0s - loss: 0.0112 - val_loss: 0.3151\n",
      "Epoch 235/300\n",
      "379/379 [==============================] - 0s - loss: 0.0115 - val_loss: 0.2509\n",
      "Epoch 236/300\n",
      "379/379 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2336\n",
      "Epoch 237/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2175\n",
      "Epoch 238/300\n",
      "379/379 [==============================] - 0s - loss: 0.0120 - val_loss: 0.2408\n",
      "Epoch 239/300\n",
      "379/379 [==============================] - 0s - loss: 0.0178 - val_loss: 0.2716\n",
      "Epoch 240/300\n",
      "379/379 [==============================] - 0s - loss: 0.0105 - val_loss: 0.2611\n",
      "Epoch 241/300\n",
      "379/379 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2178\n",
      "Epoch 242/300\n",
      "379/379 [==============================] - 0s - loss: 0.0085 - val_loss: 0.2526\n",
      "Epoch 243/300\n",
      "379/379 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2143\n",
      "Epoch 244/300\n",
      "379/379 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2089\n",
      "Epoch 245/300\n",
      "379/379 [==============================] - 0s - loss: 0.0124 - val_loss: 0.1920\n",
      "Epoch 246/300\n",
      "379/379 [==============================] - 0s - loss: 0.0100 - val_loss: 0.1725\n",
      "Epoch 247/300\n",
      "379/379 [==============================] - 0s - loss: 0.0505 - val_loss: 0.1405\n",
      "Epoch 248/300\n",
      "379/379 [==============================] - 0s - loss: 0.0432 - val_loss: 0.2047\n",
      "Epoch 249/300\n",
      "379/379 [==============================] - 0s - loss: 0.0102 - val_loss: 0.2787\n",
      "Epoch 250/300\n",
      "379/379 [==============================] - 0s - loss: 0.0114 - val_loss: 0.1705\n",
      "Epoch 251/300\n",
      "379/379 [==============================] - 0s - loss: 0.0201 - val_loss: 0.2373\n",
      "Epoch 252/300\n",
      "379/379 [==============================] - 0s - loss: 0.0142 - val_loss: 0.2069\n",
      "Epoch 253/300\n",
      "379/379 [==============================] - 0s - loss: 0.0110 - val_loss: 0.2731\n",
      "Epoch 254/300\n",
      "379/379 [==============================] - 0s - loss: 0.0126 - val_loss: 0.2394\n",
      "Epoch 255/300\n",
      "379/379 [==============================] - 0s - loss: 0.0230 - val_loss: 0.2841\n",
      "Epoch 256/300\n",
      "379/379 [==============================] - 0s - loss: 0.0346 - val_loss: 0.1957\n",
      "Epoch 257/300\n",
      "379/379 [==============================] - 0s - loss: 0.0222 - val_loss: 0.2037\n",
      "Epoch 258/300\n",
      "379/379 [==============================] - 0s - loss: 0.0086 - val_loss: 0.1761\n",
      "Epoch 259/300\n",
      "379/379 [==============================] - 0s - loss: 0.0325 - val_loss: 0.1652\n",
      "Epoch 260/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.1952\n",
      "Epoch 261/300\n",
      "379/379 [==============================] - 0s - loss: 0.0151 - val_loss: 0.2139\n",
      "Epoch 262/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2046\n",
      "Epoch 263/300\n",
      "379/379 [==============================] - 0s - loss: 0.0062 - val_loss: 0.2190\n",
      "Epoch 264/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2352\n",
      "Epoch 265/300\n",
      "379/379 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2287\n",
      "Epoch 266/300\n",
      "379/379 [==============================] - 0s - loss: 0.0074 - val_loss: 0.1963\n",
      "Epoch 267/300\n",
      "379/379 [==============================] - 0s - loss: 0.0158 - val_loss: 0.2051\n",
      "Epoch 268/300\n",
      "379/379 [==============================] - 0s - loss: 0.0177 - val_loss: 0.2076\n",
      "Epoch 269/300\n",
      "379/379 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2336\n",
      "Epoch 270/300\n",
      "379/379 [==============================] - 0s - loss: 0.0103 - val_loss: 0.2131\n",
      "Epoch 271/300\n",
      "379/379 [==============================] - 0s - loss: 0.0093 - val_loss: 0.1756\n",
      "Epoch 272/300\n",
      "379/379 [==============================] - 0s - loss: 0.0082 - val_loss: 0.1945\n",
      "Epoch 273/300\n",
      "379/379 [==============================] - 0s - loss: 0.0158 - val_loss: 0.1966\n",
      "Epoch 274/300\n",
      "379/379 [==============================] - 0s - loss: 0.0163 - val_loss: 0.2296\n",
      "Epoch 275/300\n",
      "379/379 [==============================] - 0s - loss: 0.0118 - val_loss: 0.2097\n",
      "Epoch 276/300\n",
      "379/379 [==============================] - 0s - loss: 0.0064 - val_loss: 0.2428\n",
      "Epoch 277/300\n",
      "379/379 [==============================] - 0s - loss: 0.0064 - val_loss: 0.1890\n",
      "Epoch 278/300\n",
      "379/379 [==============================] - 0s - loss: 0.0073 - val_loss: 0.1953\n",
      "Epoch 279/300\n",
      "379/379 [==============================] - 0s - loss: 0.0057 - val_loss: 0.2366\n",
      "Epoch 280/300\n",
      "379/379 [==============================] - 0s - loss: 0.0204 - val_loss: 0.3299\n",
      "Epoch 281/300\n",
      "379/379 [==============================] - 0s - loss: 0.0213 - val_loss: 0.3298\n",
      "Epoch 282/300\n",
      "379/379 [==============================] - 0s - loss: 0.0199 - val_loss: 0.2640\n",
      "Epoch 283/300\n",
      "379/379 [==============================] - 0s - loss: 0.0647 - val_loss: 0.2916\n",
      "Epoch 284/300\n",
      "379/379 [==============================] - 0s - loss: 0.0301 - val_loss: 0.3122\n",
      "Epoch 285/300\n",
      "379/379 [==============================] - 0s - loss: 0.0138 - val_loss: 0.2016\n",
      "Epoch 286/300\n",
      "379/379 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2167\n",
      "Epoch 287/300\n",
      "379/379 [==============================] - 0s - loss: 0.0063 - val_loss: 0.2269\n",
      "Epoch 288/300\n",
      "379/379 [==============================] - 0s - loss: 0.0137 - val_loss: 0.1814\n",
      "Epoch 289/300\n",
      "379/379 [==============================] - 0s - loss: 0.0149 - val_loss: 0.2262\n",
      "Epoch 290/300\n",
      "379/379 [==============================] - 0s - loss: 0.0219 - val_loss: 0.2377\n",
      "Epoch 291/300\n",
      "379/379 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2252\n",
      "Epoch 292/300\n",
      "379/379 [==============================] - 0s - loss: 0.0198 - val_loss: 0.2547\n",
      "Epoch 293/300\n",
      "379/379 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2769\n",
      "Epoch 294/300\n",
      "379/379 [==============================] - 0s - loss: 0.0219 - val_loss: 0.2051\n",
      "Epoch 295/300\n",
      "379/379 [==============================] - 0s - loss: 0.0064 - val_loss: 0.1662\n",
      "Epoch 296/300\n",
      "379/379 [==============================] - 0s - loss: 0.0183 - val_loss: 0.1604\n",
      "Epoch 297/300\n",
      "379/379 [==============================] - 0s - loss: 0.0196 - val_loss: 0.2113\n",
      "Epoch 298/300\n",
      "379/379 [==============================] - 0s - loss: 0.0104 - val_loss: 0.2205\n",
      "Epoch 299/300\n",
      "379/379 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2243\n",
      "Epoch 300/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.1839\n"
     ]
    }
   ],
   "source": [
    "model2_relu = Sequential()\n",
    "model2_relu.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "model2_relu.add(Activation('relu'))\n",
    "model2_relu.add(Dense(1, init='uniform'))\n",
    "model2_relu.add(Activation('linear'))\n",
    "\n",
    "sgd = SGD(lr=0.2)\n",
    "model2_relu.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "\n",
    "hist_relu = model2_relu.fit(X_train_scaled.as_matrix(), y_train_scaled.as_matrix(), nb_epoch=300, verbose=1, validation_data=(X_test_scaled.as_matrix(), y_test_scaled.as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrgTLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11565d550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_relu.history['loss'])\n",
    "plt.title(\"Square Error - Relu\")\n",
    "plt.xlabel('Ephocs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreviar la diferencia más apreciable se nota en la cantidad de *epoch* necesarias para conseguir el error mínimo, el cual ocurre mucho antes de las 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generamos los n learning rates\n",
    "n_lr = 20\n",
    "lear_rate = np.linspace(0,1,n_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/300\n",
      "379/379 [==============================] - 0s - loss: 264.2513 - val_loss: 46.4653\n",
      "Epoch 2/300\n",
      "379/379 [==============================] - 0s - loss: 15.7225 - val_loss: 11.6759\n",
      "Epoch 3/300\n",
      "379/379 [==============================] - 0s - loss: 6.0180 - val_loss: 7.6331\n",
      "Epoch 4/300\n",
      "379/379 [==============================] - 0s - loss: 3.6379 - val_loss: 4.1111\n",
      "Epoch 5/300\n",
      "379/379 [==============================] - 0s - loss: 2.3136 - val_loss: 3.7882\n",
      "Epoch 6/300\n",
      "379/379 [==============================] - 0s - loss: 1.6708 - val_loss: 2.0956\n",
      "Epoch 7/300\n",
      "379/379 [==============================] - 0s - loss: 1.3678 - val_loss: 2.4447\n",
      "Epoch 8/300\n",
      "379/379 [==============================] - 0s - loss: 1.0507 - val_loss: 1.2696\n",
      "Epoch 9/300\n",
      "379/379 [==============================] - 0s - loss: 0.9201 - val_loss: 1.3544\n",
      "Epoch 10/300\n",
      "379/379 [==============================] - 0s - loss: 0.8392 - val_loss: 1.2218\n",
      "Epoch 11/300\n",
      "379/379 [==============================] - 0s - loss: 0.7418 - val_loss: 1.4902\n",
      "Epoch 12/300\n",
      "379/379 [==============================] - 0s - loss: 0.8731 - val_loss: 0.8726\n",
      "Epoch 13/300\n",
      "379/379 [==============================] - 0s - loss: 0.6097 - val_loss: 0.8420\n",
      "Epoch 14/300\n",
      "379/379 [==============================] - 0s - loss: 0.4833 - val_loss: 1.0720\n",
      "Epoch 15/300\n",
      "379/379 [==============================] - 0s - loss: 0.4583 - val_loss: 0.7745\n",
      "Epoch 16/300\n",
      "379/379 [==============================] - 0s - loss: 0.3849 - val_loss: 0.7357\n",
      "Epoch 17/300\n",
      "379/379 [==============================] - 0s - loss: 0.4167 - val_loss: 0.8759\n",
      "Epoch 18/300\n",
      "379/379 [==============================] - 0s - loss: 0.3957 - val_loss: 0.6254\n",
      "Epoch 19/300\n",
      "379/379 [==============================] - 0s - loss: 0.5523 - val_loss: 0.4848\n",
      "Epoch 20/300\n",
      "379/379 [==============================] - 0s - loss: 0.4434 - val_loss: 0.8049\n",
      "Epoch 21/300\n",
      "379/379 [==============================] - 0s - loss: 0.2678 - val_loss: 0.5890\n",
      "Epoch 22/300\n",
      "379/379 [==============================] - 0s - loss: 0.3187 - val_loss: 0.5126\n",
      "Epoch 23/300\n",
      "379/379 [==============================] - 0s - loss: 0.3242 - val_loss: 0.7234\n",
      "Epoch 24/300\n",
      "379/379 [==============================] - 0s - loss: 0.2141 - val_loss: 0.9066\n",
      "Epoch 25/300\n",
      "379/379 [==============================] - 0s - loss: 0.2097 - val_loss: 0.4507\n",
      "Epoch 26/300\n",
      "379/379 [==============================] - 0s - loss: 0.3990 - val_loss: 0.4083\n",
      "Epoch 27/300\n",
      "379/379 [==============================] - 0s - loss: 0.1789 - val_loss: 0.3770\n",
      "Epoch 28/300\n",
      "379/379 [==============================] - 0s - loss: 0.2344 - val_loss: 0.5275\n",
      "Epoch 29/300\n",
      "379/379 [==============================] - 0s - loss: 0.4694 - val_loss: 0.6030\n",
      "Epoch 30/300\n",
      "379/379 [==============================] - 0s - loss: 0.1534 - val_loss: 0.6597\n",
      "Epoch 31/300\n",
      "379/379 [==============================] - 0s - loss: 0.1899 - val_loss: 0.3804\n",
      "Epoch 32/300\n",
      "379/379 [==============================] - 0s - loss: 0.1934 - val_loss: 0.3740\n",
      "Epoch 33/300\n",
      "379/379 [==============================] - 0s - loss: 0.1426 - val_loss: 0.3988\n",
      "Epoch 34/300\n",
      "379/379 [==============================] - 0s - loss: 0.1672 - val_loss: 0.4066\n",
      "Epoch 35/300\n",
      "379/379 [==============================] - 0s - loss: 0.1491 - val_loss: 0.3656\n",
      "Epoch 36/300\n",
      "379/379 [==============================] - 0s - loss: 0.1334 - val_loss: 0.3079\n",
      "Epoch 37/300\n",
      "379/379 [==============================] - 0s - loss: 0.4824 - val_loss: 0.3023\n",
      "Epoch 38/300\n",
      "379/379 [==============================] - 0s - loss: 0.3519 - val_loss: 0.3258\n",
      "Epoch 39/300\n",
      "379/379 [==============================] - 0s - loss: 0.1316 - val_loss: 0.4397\n",
      "Epoch 40/300\n",
      "379/379 [==============================] - 0s - loss: 0.1467 - val_loss: 0.3344\n",
      "Epoch 41/300\n",
      "379/379 [==============================] - 0s - loss: 0.2509 - val_loss: 0.1888\n",
      "Epoch 42/300\n",
      "379/379 [==============================] - 0s - loss: 0.1859 - val_loss: 0.7037\n",
      "Epoch 43/300\n",
      "379/379 [==============================] - 0s - loss: 0.1188 - val_loss: 0.2350\n",
      "Epoch 44/300\n",
      "379/379 [==============================] - 0s - loss: 0.1030 - val_loss: 0.3702\n",
      "Epoch 45/300\n",
      "379/379 [==============================] - 0s - loss: 0.1296 - val_loss: 0.6663\n",
      "Epoch 46/300\n",
      "379/379 [==============================] - 0s - loss: 1.0618 - val_loss: 0.4978\n",
      "Epoch 47/300\n",
      "379/379 [==============================] - 0s - loss: 0.1528 - val_loss: 0.7154\n",
      "Epoch 48/300\n",
      "379/379 [==============================] - 0s - loss: 0.3725 - val_loss: 1.1891\n",
      "Epoch 49/300\n",
      "379/379 [==============================] - 0s - loss: 0.5984 - val_loss: 1.1462\n",
      "Epoch 50/300\n",
      "379/379 [==============================] - 0s - loss: 0.2341 - val_loss: 0.4831\n",
      "Epoch 51/300\n",
      "379/379 [==============================] - 0s - loss: 0.0875 - val_loss: 0.2766\n",
      "Epoch 52/300\n",
      "379/379 [==============================] - 0s - loss: 0.0804 - val_loss: 0.6016\n",
      "Epoch 53/300\n",
      "379/379 [==============================] - 0s - loss: 0.0693 - val_loss: 0.3001\n",
      "Epoch 54/300\n",
      "379/379 [==============================] - 0s - loss: 0.0748 - val_loss: 0.2948\n",
      "Epoch 55/300\n",
      "379/379 [==============================] - 0s - loss: 0.0959 - val_loss: 0.4436\n",
      "Epoch 56/300\n",
      "379/379 [==============================] - 0s - loss: 0.2080 - val_loss: 0.9386\n",
      "Epoch 57/300\n",
      "379/379 [==============================] - 0s - loss: 0.3013 - val_loss: 0.8271\n",
      "Epoch 58/300\n",
      "379/379 [==============================] - 0s - loss: 0.2615 - val_loss: 0.9680\n",
      "Epoch 59/300\n",
      "379/379 [==============================] - 0s - loss: 0.1446 - val_loss: 0.5227\n",
      "Epoch 60/300\n",
      "379/379 [==============================] - 0s - loss: 0.1222 - val_loss: 0.5386\n",
      "Epoch 61/300\n",
      "379/379 [==============================] - 0s - loss: 0.1275 - val_loss: 0.1700\n",
      "Epoch 62/300\n",
      "379/379 [==============================] - 0s - loss: 0.2769 - val_loss: 0.5499\n",
      "Epoch 63/300\n",
      "379/379 [==============================] - 0s - loss: 0.1515 - val_loss: 0.4232\n",
      "Epoch 64/300\n",
      "379/379 [==============================] - 0s - loss: 0.1275 - val_loss: 0.7048\n",
      "Epoch 65/300\n",
      "379/379 [==============================] - 0s - loss: 0.0864 - val_loss: 0.9839\n",
      "Epoch 66/300\n",
      "379/379 [==============================] - 0s - loss: 0.4324 - val_loss: 0.3983\n",
      "Epoch 67/300\n",
      "379/379 [==============================] - 0s - loss: 0.1272 - val_loss: 0.9001\n",
      "Epoch 68/300\n",
      "379/379 [==============================] - 0s - loss: 0.1566 - val_loss: 0.7686\n",
      "Epoch 69/300\n",
      "379/379 [==============================] - 0s - loss: 0.1115 - val_loss: 0.1758\n",
      "Epoch 70/300\n",
      "379/379 [==============================] - 0s - loss: 0.2660 - val_loss: 0.4249\n",
      "Epoch 71/300\n",
      "379/379 [==============================] - 0s - loss: 0.1646 - val_loss: 0.3483\n",
      "Epoch 72/300\n",
      "379/379 [==============================] - 0s - loss: 0.0435 - val_loss: 0.1718\n",
      "Epoch 73/300\n",
      "379/379 [==============================] - 0s - loss: 0.0833 - val_loss: 0.2356\n",
      "Epoch 74/300\n",
      "379/379 [==============================] - 0s - loss: 0.1547 - val_loss: 0.3263\n",
      "Epoch 75/300\n",
      "379/379 [==============================] - 0s - loss: 0.0643 - val_loss: 0.2230\n",
      "Epoch 76/300\n",
      "379/379 [==============================] - 0s - loss: 0.0873 - val_loss: 0.4489\n",
      "Epoch 77/300\n",
      "379/379 [==============================] - 0s - loss: 0.1352 - val_loss: 0.5408\n",
      "Epoch 78/300\n",
      "379/379 [==============================] - 0s - loss: 0.0504 - val_loss: 0.3555\n",
      "Epoch 79/300\n",
      "379/379 [==============================] - 0s - loss: 0.0442 - val_loss: 0.2639\n",
      "Epoch 80/300\n",
      "379/379 [==============================] - 0s - loss: 0.0425 - val_loss: 0.3086\n",
      "Epoch 81/300\n",
      "379/379 [==============================] - 0s - loss: 0.0494 - val_loss: 0.2092\n",
      "Epoch 82/300\n",
      "379/379 [==============================] - 0s - loss: 0.6945 - val_loss: 0.3073\n",
      "Epoch 83/300\n",
      "379/379 [==============================] - 0s - loss: 0.0483 - val_loss: 0.2103\n",
      "Epoch 84/300\n",
      "379/379 [==============================] - 0s - loss: 0.0948 - val_loss: 0.2350\n",
      "Epoch 85/300\n",
      "379/379 [==============================] - 0s - loss: 0.2770 - val_loss: 0.1532\n",
      "Epoch 86/300\n",
      "379/379 [==============================] - 0s - loss: 0.1130 - val_loss: 0.3060\n",
      "Epoch 87/300\n",
      "379/379 [==============================] - 0s - loss: 0.0340 - val_loss: 0.1914\n",
      "Epoch 88/300\n",
      "379/379 [==============================] - 0s - loss: 0.0666 - val_loss: 0.1845\n",
      "Epoch 89/300\n",
      "379/379 [==============================] - 0s - loss: 0.1283 - val_loss: 0.2581\n",
      "Epoch 90/300\n",
      "379/379 [==============================] - 0s - loss: 0.1130 - val_loss: 0.2777\n",
      "Epoch 91/300\n",
      "379/379 [==============================] - 0s - loss: 0.0302 - val_loss: 0.2809\n",
      "Epoch 92/300\n",
      "379/379 [==============================] - 0s - loss: 0.0280 - val_loss: 0.2544\n",
      "Epoch 93/300\n",
      "379/379 [==============================] - 0s - loss: 0.0286 - val_loss: 0.3216\n",
      "Epoch 94/300\n",
      "379/379 [==============================] - 0s - loss: 0.1689 - val_loss: 2.0781\n",
      "Epoch 95/300\n",
      "379/379 [==============================] - 0s - loss: 3.2700 - val_loss: 0.6026\n",
      "Epoch 96/300\n",
      "379/379 [==============================] - 0s - loss: 0.0699 - val_loss: 0.3767\n",
      "Epoch 97/300\n",
      "379/379 [==============================] - 0s - loss: 0.0380 - val_loss: 0.2941\n",
      "Epoch 98/300\n",
      "379/379 [==============================] - 0s - loss: 0.0372 - val_loss: 0.3228\n",
      "Epoch 99/300\n",
      "379/379 [==============================] - 0s - loss: 0.0469 - val_loss: 0.2583\n",
      "Epoch 100/300\n",
      "379/379 [==============================] - 0s - loss: 0.0406 - val_loss: 0.2994\n",
      "Epoch 101/300\n",
      "379/379 [==============================] - 0s - loss: 0.0299 - val_loss: 0.2832\n",
      "Epoch 102/300\n",
      "379/379 [==============================] - 0s - loss: 0.0293 - val_loss: 0.2867\n",
      "Epoch 103/300\n",
      "379/379 [==============================] - 0s - loss: 0.0672 - val_loss: 0.4378\n",
      "Epoch 104/300\n",
      "379/379 [==============================] - 0s - loss: 0.1056 - val_loss: 0.2311\n",
      "Epoch 105/300\n",
      "379/379 [==============================] - 0s - loss: 0.0272 - val_loss: 0.2827\n",
      "Epoch 106/300\n",
      "379/379 [==============================] - 0s - loss: 0.1028 - val_loss: 0.2389\n",
      "Epoch 107/300\n",
      "379/379 [==============================] - 0s - loss: 0.0271 - val_loss: 0.1946\n",
      "Epoch 108/300\n",
      "379/379 [==============================] - 0s - loss: 0.0501 - val_loss: 0.3632\n",
      "Epoch 109/300\n",
      "379/379 [==============================] - 0s - loss: 0.0327 - val_loss: 0.4150\n",
      "Epoch 110/300\n",
      "379/379 [==============================] - 0s - loss: 0.2870 - val_loss: 0.1935\n",
      "Epoch 111/300\n",
      "379/379 [==============================] - 0s - loss: 0.0395 - val_loss: 0.1323\n",
      "Epoch 112/300\n",
      "379/379 [==============================] - 0s - loss: 0.3011 - val_loss: 0.2834\n",
      "Epoch 113/300\n",
      "379/379 [==============================] - 0s - loss: 0.0271 - val_loss: 0.3267\n",
      "Epoch 114/300\n",
      "379/379 [==============================] - 0s - loss: 0.0320 - val_loss: 0.2919\n",
      "Epoch 115/300\n",
      "379/379 [==============================] - 0s - loss: 0.0454 - val_loss: 0.2212\n",
      "Epoch 116/300\n",
      "379/379 [==============================] - 0s - loss: 0.0285 - val_loss: 0.3509\n",
      "Epoch 117/300\n",
      "379/379 [==============================] - 0s - loss: 0.0475 - val_loss: 0.2930\n",
      "Epoch 118/300\n",
      "379/379 [==============================] - 0s - loss: 0.0347 - val_loss: 0.4078\n",
      "Epoch 119/300\n",
      "379/379 [==============================] - 0s - loss: 0.0570 - val_loss: 0.3757\n",
      "Epoch 120/300\n",
      "379/379 [==============================] - 0s - loss: 0.0664 - val_loss: 0.3718\n",
      "Epoch 121/300\n",
      "379/379 [==============================] - 0s - loss: 0.0264 - val_loss: 0.2533\n",
      "Epoch 122/300\n",
      "379/379 [==============================] - 0s - loss: 0.0208 - val_loss: 0.2661\n",
      "Epoch 123/300\n",
      "379/379 [==============================] - 0s - loss: 0.0275 - val_loss: 0.2454\n",
      "Epoch 124/300\n",
      "379/379 [==============================] - 0s - loss: 0.0220 - val_loss: 0.1840\n",
      "Epoch 125/300\n",
      "379/379 [==============================] - 0s - loss: 0.0707 - val_loss: 0.3175\n",
      "Epoch 126/300\n",
      "379/379 [==============================] - 0s - loss: 0.0203 - val_loss: 0.2855\n",
      "Epoch 127/300\n",
      "379/379 [==============================] - 0s - loss: 0.0182 - val_loss: 0.2449\n",
      "Epoch 128/300\n",
      "379/379 [==============================] - 0s - loss: 0.0243 - val_loss: 0.4195\n",
      "Epoch 129/300\n",
      "379/379 [==============================] - 0s - loss: 0.0614 - val_loss: 0.2684\n",
      "Epoch 130/300\n",
      "379/379 [==============================] - 0s - loss: 0.0283 - val_loss: 0.2367\n",
      "Epoch 131/300\n",
      "379/379 [==============================] - 0s - loss: 0.0280 - val_loss: 0.1166\n",
      "Epoch 132/300\n",
      "379/379 [==============================] - 0s - loss: 0.1144 - val_loss: 0.1818\n",
      "Epoch 133/300\n",
      "379/379 [==============================] - 0s - loss: 0.0712 - val_loss: 0.2337\n",
      "Epoch 134/300\n",
      "379/379 [==============================] - 0s - loss: 0.0390 - val_loss: 0.5330\n",
      "Epoch 135/300\n",
      "379/379 [==============================] - 0s - loss: 0.0616 - val_loss: 0.2583\n",
      "Epoch 136/300\n",
      "379/379 [==============================] - 0s - loss: 0.0166 - val_loss: 0.2763\n",
      "Epoch 137/300\n",
      "379/379 [==============================] - 0s - loss: 0.0168 - val_loss: 0.2528\n",
      "Epoch 138/300\n",
      "379/379 [==============================] - 0s - loss: 0.0185 - val_loss: 0.4781\n",
      "Epoch 139/300\n",
      "379/379 [==============================] - 0s - loss: 0.0352 - val_loss: 0.2561\n",
      "Epoch 140/300\n",
      "379/379 [==============================] - 0s - loss: 0.0417 - val_loss: 0.2082\n",
      "Epoch 141/300\n",
      "379/379 [==============================] - 0s - loss: 0.0307 - val_loss: 0.3129\n",
      "Epoch 142/300\n",
      "379/379 [==============================] - 0s - loss: 0.0178 - val_loss: 0.2509\n",
      "Epoch 143/300\n",
      "379/379 [==============================] - 0s - loss: 0.0478 - val_loss: 0.1868\n",
      "Epoch 144/300\n",
      "379/379 [==============================] - 0s - loss: 0.0500 - val_loss: 0.1735\n",
      "Epoch 145/300\n",
      "379/379 [==============================] - 0s - loss: 0.0608 - val_loss: 0.1959\n",
      "Epoch 146/300\n",
      "379/379 [==============================] - 0s - loss: 0.0162 - val_loss: 0.3095\n",
      "Epoch 147/300\n",
      "379/379 [==============================] - 0s - loss: 0.0187 - val_loss: 0.3448\n",
      "Epoch 148/300\n",
      "379/379 [==============================] - 0s - loss: 0.0417 - val_loss: 0.2870\n",
      "Epoch 149/300\n",
      "379/379 [==============================] - 0s - loss: 0.0164 - val_loss: 0.2970\n",
      "Epoch 150/300\n",
      "379/379 [==============================] - 0s - loss: 0.0278 - val_loss: 0.3123\n",
      "Epoch 151/300\n",
      "379/379 [==============================] - 0s - loss: 0.0167 - val_loss: 0.2751\n",
      "Epoch 152/300\n",
      "379/379 [==============================] - 0s - loss: 0.0135 - val_loss: 0.2769\n",
      "Epoch 153/300\n",
      "379/379 [==============================] - 0s - loss: 0.0537 - val_loss: 0.1819\n",
      "Epoch 154/300\n",
      "379/379 [==============================] - 0s - loss: 0.0364 - val_loss: 0.2819\n",
      "Epoch 155/300\n",
      "379/379 [==============================] - 0s - loss: 0.0129 - val_loss: 0.2133\n",
      "Epoch 156/300\n",
      "379/379 [==============================] - 0s - loss: 0.0161 - val_loss: 0.2027\n",
      "Epoch 157/300\n",
      "379/379 [==============================] - 0s - loss: 0.0210 - val_loss: 0.2810\n",
      "Epoch 158/300\n",
      "379/379 [==============================] - 0s - loss: 0.0221 - val_loss: 0.3001\n",
      "Epoch 159/300\n",
      "379/379 [==============================] - 0s - loss: 0.0131 - val_loss: 0.2149\n",
      "Epoch 160/300\n",
      "379/379 [==============================] - 0s - loss: 0.0433 - val_loss: 0.1605\n",
      "Epoch 161/300\n",
      "379/379 [==============================] - 0s - loss: 0.0159 - val_loss: 0.2438\n",
      "Epoch 162/300\n",
      "379/379 [==============================] - 0s - loss: 0.0209 - val_loss: 0.3661\n",
      "Epoch 163/300\n",
      "379/379 [==============================] - 0s - loss: 0.0532 - val_loss: 0.3100\n",
      "Epoch 164/300\n",
      "379/379 [==============================] - 0s - loss: 0.0133 - val_loss: 0.2206\n",
      "Epoch 165/300\n",
      "379/379 [==============================] - 0s - loss: 0.0166 - val_loss: 0.1817\n",
      "Epoch 166/300\n",
      "379/379 [==============================] - 0s - loss: 0.1138 - val_loss: 0.2095\n",
      "Epoch 167/300\n",
      "379/379 [==============================] - 0s - loss: 0.0296 - val_loss: 0.2348\n",
      "Epoch 168/300\n",
      "379/379 [==============================] - 0s - loss: 0.0124 - val_loss: 0.1883\n",
      "Epoch 169/300\n",
      "379/379 [==============================] - 0s - loss: 0.0294 - val_loss: 0.2746\n",
      "Epoch 170/300\n",
      "379/379 [==============================] - 0s - loss: 0.0140 - val_loss: 0.3275\n",
      "Epoch 171/300\n",
      "379/379 [==============================] - 0s - loss: 0.0217 - val_loss: 0.2290\n",
      "Epoch 172/300\n",
      "379/379 [==============================] - 0s - loss: 0.0120 - val_loss: 0.2434\n",
      "Epoch 173/300\n",
      "379/379 [==============================] - 0s - loss: 0.0163 - val_loss: 0.1896\n",
      "Epoch 174/300\n",
      "379/379 [==============================] - 0s - loss: 0.0278 - val_loss: 0.1966\n",
      "Epoch 175/300\n",
      "379/379 [==============================] - 0s - loss: 0.0540 - val_loss: 0.1797\n",
      "Epoch 176/300\n",
      "379/379 [==============================] - 0s - loss: 0.0167 - val_loss: 0.1833\n",
      "Epoch 177/300\n",
      "379/379 [==============================] - 0s - loss: 0.0101 - val_loss: 0.2408\n",
      "Epoch 178/300\n",
      "379/379 [==============================] - 0s - loss: 0.0186 - val_loss: 0.2196\n",
      "Epoch 179/300\n",
      "379/379 [==============================] - 0s - loss: 0.0099 - val_loss: 0.2147\n",
      "Epoch 180/300\n",
      "379/379 [==============================] - 0s - loss: 0.0148 - val_loss: 0.2633\n",
      "Epoch 181/300\n",
      "379/379 [==============================] - 0s - loss: 0.0133 - val_loss: 0.1935\n",
      "Epoch 182/300\n",
      "379/379 [==============================] - 0s - loss: 0.0557 - val_loss: 0.2086\n",
      "Epoch 183/300\n",
      "379/379 [==============================] - 0s - loss: 0.0303 - val_loss: 0.2122\n",
      "Epoch 184/300\n",
      "379/379 [==============================] - 0s - loss: 0.0161 - val_loss: 0.1778\n",
      "Epoch 185/300\n",
      "379/379 [==============================] - 0s - loss: 0.0139 - val_loss: 0.1975\n",
      "Epoch 186/300\n",
      "379/379 [==============================] - 0s - loss: 0.0100 - val_loss: 0.2520\n",
      "Epoch 187/300\n",
      "379/379 [==============================] - 0s - loss: 0.0099 - val_loss: 0.3208\n",
      "Epoch 188/300\n",
      "379/379 [==============================] - 0s - loss: 0.0217 - val_loss: 0.2456\n",
      "Epoch 189/300\n",
      "379/379 [==============================] - 0s - loss: 0.0106 - val_loss: 0.2121\n",
      "Epoch 190/300\n",
      "379/379 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2448\n",
      "Epoch 191/300\n",
      "379/379 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2058\n",
      "Epoch 192/300\n",
      "379/379 [==============================] - 0s - loss: 0.0159 - val_loss: 0.1890\n",
      "Epoch 193/300\n",
      "379/379 [==============================] - 0s - loss: 0.0102 - val_loss: 0.2384\n",
      "Epoch 194/300\n",
      "379/379 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2453\n",
      "Epoch 195/300\n",
      "379/379 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2446\n",
      "Epoch 196/300\n",
      "379/379 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2229\n",
      "Epoch 197/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2471\n",
      "Epoch 198/300\n",
      "379/379 [==============================] - 0s - loss: 0.0117 - val_loss: 0.1756\n",
      "Epoch 199/300\n",
      "379/379 [==============================] - 0s - loss: 0.1567 - val_loss: 0.1842\n",
      "Epoch 200/300\n",
      "379/379 [==============================] - 0s - loss: 0.0097 - val_loss: 0.2744\n",
      "Epoch 201/300\n",
      "379/379 [==============================] - 0s - loss: 0.0189 - val_loss: 0.2345\n",
      "Epoch 202/300\n",
      "379/379 [==============================] - 0s - loss: 0.0138 - val_loss: 0.1664\n",
      "Epoch 203/300\n",
      "379/379 [==============================] - 0s - loss: 0.0415 - val_loss: 0.1781\n",
      "Epoch 204/300\n",
      "379/379 [==============================] - 0s - loss: 0.0453 - val_loss: 0.1654\n",
      "Epoch 205/300\n",
      "379/379 [==============================] - 0s - loss: 0.0166 - val_loss: 0.2821\n",
      "Epoch 206/300\n",
      "379/379 [==============================] - 0s - loss: 0.0223 - val_loss: 0.2537\n",
      "Epoch 207/300\n",
      "379/379 [==============================] - 0s - loss: 0.0479 - val_loss: 0.2699\n",
      "Epoch 208/300\n",
      "379/379 [==============================] - 0s - loss: 0.0226 - val_loss: 0.2328\n",
      "Epoch 209/300\n",
      "379/379 [==============================] - 0s - loss: 0.0079 - val_loss: 0.1911\n",
      "Epoch 210/300\n",
      "379/379 [==============================] - 0s - loss: 0.0181 - val_loss: 0.1750\n",
      "Epoch 211/300\n",
      "379/379 [==============================] - 0s - loss: 0.0097 - val_loss: 0.2285\n",
      "Epoch 212/300\n",
      "379/379 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2077\n",
      "Epoch 213/300\n",
      "379/379 [==============================] - 0s - loss: 0.0112 - val_loss: 0.1507\n",
      "Epoch 214/300\n",
      "379/379 [==============================] - 0s - loss: 0.0366 - val_loss: 0.1975\n",
      "Epoch 215/300\n",
      "379/379 [==============================] - 0s - loss: 0.0139 - val_loss: 0.2382\n",
      "Epoch 216/300\n",
      "379/379 [==============================] - 0s - loss: 0.0289 - val_loss: 0.1518\n",
      "Epoch 217/300\n",
      "379/379 [==============================] - 0s - loss: 0.0418 - val_loss: 0.1918\n",
      "Epoch 218/300\n",
      "379/379 [==============================] - 0s - loss: 0.0748 - val_loss: 0.1463\n",
      "Epoch 219/300\n",
      "379/379 [==============================] - 0s - loss: 0.0210 - val_loss: 0.2179\n",
      "Epoch 220/300\n",
      "379/379 [==============================] - 0s - loss: 0.0153 - val_loss: 0.3063\n",
      "Epoch 221/300\n",
      "379/379 [==============================] - 0s - loss: 0.0174 - val_loss: 0.3330\n",
      "Epoch 222/300\n",
      "379/379 [==============================] - 0s - loss: 0.0365 - val_loss: 0.3403\n",
      "Epoch 223/300\n",
      "379/379 [==============================] - 0s - loss: 0.0578 - val_loss: 0.2770\n",
      "Epoch 224/300\n",
      "379/379 [==============================] - 0s - loss: 0.0163 - val_loss: 0.3881\n",
      "Epoch 225/300\n",
      "379/379 [==============================] - 0s - loss: 0.0292 - val_loss: 0.1966\n",
      "Epoch 226/300\n",
      "379/379 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2382\n",
      "Epoch 227/300\n",
      "379/379 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2490\n",
      "Epoch 228/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2674\n",
      "Epoch 229/300\n",
      "379/379 [==============================] - 0s - loss: 0.0108 - val_loss: 0.1869\n",
      "Epoch 230/300\n",
      "379/379 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2512\n",
      "Epoch 231/300\n",
      "379/379 [==============================] - 0s - loss: 0.0100 - val_loss: 0.2141\n",
      "Epoch 232/300\n",
      "379/379 [==============================] - 0s - loss: 0.0069 - val_loss: 0.2431\n",
      "Epoch 233/300\n",
      "379/379 [==============================] - 0s - loss: 0.0069 - val_loss: 0.1769\n",
      "Epoch 234/300\n",
      "379/379 [==============================] - 0s - loss: 0.0140 - val_loss: 0.1842\n",
      "Epoch 235/300\n",
      "379/379 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2520\n",
      "Epoch 236/300\n",
      "379/379 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2471\n",
      "Epoch 237/300\n",
      "379/379 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2778\n",
      "Epoch 238/300\n",
      "379/379 [==============================] - 0s - loss: 0.0200 - val_loss: 0.2910\n",
      "Epoch 239/300\n",
      "379/379 [==============================] - 0s - loss: 0.0158 - val_loss: 0.2269\n",
      "Epoch 240/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2692\n",
      "Epoch 241/300\n",
      "379/379 [==============================] - 0s - loss: 0.0776 - val_loss: 0.1867\n",
      "Epoch 242/300\n",
      "379/379 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2214\n",
      "Epoch 243/300\n",
      "379/379 [==============================] - 0s - loss: 0.0107 - val_loss: 0.2065\n",
      "Epoch 244/300\n",
      "379/379 [==============================] - 0s - loss: 0.0175 - val_loss: 0.1687\n",
      "Epoch 245/300\n",
      "379/379 [==============================] - 0s - loss: 0.0091 - val_loss: 0.1567\n",
      "Epoch 246/300\n",
      "379/379 [==============================] - 0s - loss: 0.0127 - val_loss: 0.2050\n",
      "Epoch 247/300\n",
      "379/379 [==============================] - 0s - loss: 0.0064 - val_loss: 0.2382\n",
      "Epoch 248/300\n",
      "379/379 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2413\n",
      "Epoch 249/300\n",
      "379/379 [==============================] - 0s - loss: 0.0081 - val_loss: 0.1840\n",
      "Epoch 250/300\n",
      "379/379 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2084\n",
      "Epoch 251/300\n",
      "379/379 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2408\n",
      "Epoch 252/300\n",
      "379/379 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2224\n",
      "Epoch 253/300\n",
      "379/379 [==============================] - 0s - loss: 0.0095 - val_loss: 0.1818\n",
      "Epoch 254/300\n",
      "379/379 [==============================] - 0s - loss: 0.0087 - val_loss: 0.1909\n",
      "Epoch 255/300\n",
      "379/379 [==============================] - 0s - loss: 0.0140 - val_loss: 0.2056\n",
      "Epoch 256/300\n",
      "379/379 [==============================] - 0s - loss: 0.0057 - val_loss: 0.2058\n",
      "Epoch 257/300\n",
      "379/379 [==============================] - 0s - loss: 0.0100 - val_loss: 0.1830\n",
      "Epoch 258/300\n",
      "379/379 [==============================] - 0s - loss: 0.0109 - val_loss: 0.2028\n",
      "Epoch 259/300\n",
      "379/379 [==============================] - 0s - loss: 0.0102 - val_loss: 0.1814\n",
      "Epoch 260/300\n",
      "379/379 [==============================] - 0s - loss: 0.0059 - val_loss: 0.1896\n",
      "Epoch 261/300\n",
      "379/379 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2151\n",
      "Epoch 262/300\n",
      "379/379 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2168\n",
      "Epoch 263/300\n",
      "379/379 [==============================] - 0s - loss: 0.0062 - val_loss: 0.1781\n",
      "Epoch 264/300\n",
      "379/379 [==============================] - 0s - loss: 0.0331 - val_loss: 0.1705\n",
      "Epoch 265/300\n",
      "379/379 [==============================] - 0s - loss: 0.0148 - val_loss: 0.2302\n",
      "Epoch 266/300\n",
      "379/379 [==============================] - 0s - loss: 0.0059 - val_loss: 0.2192\n",
      "Epoch 267/300\n",
      "379/379 [==============================] - 0s - loss: 0.0082 - val_loss: 0.1854\n",
      "Epoch 268/300\n",
      "379/379 [==============================] - 0s - loss: 0.0218 - val_loss: 0.2094\n",
      "Epoch 269/300\n",
      "379/379 [==============================] - 0s - loss: 0.0148 - val_loss: 0.1615\n",
      "Epoch 270/300\n",
      "379/379 [==============================] - 0s - loss: 0.0120 - val_loss: 0.2151\n",
      "Epoch 271/300\n",
      "379/379 [==============================] - 0s - loss: 0.0059 - val_loss: 0.2060\n",
      "Epoch 272/300\n",
      "379/379 [==============================] - 0s - loss: 0.0052 - val_loss: 0.2114\n",
      "Epoch 273/300\n",
      "379/379 [==============================] - 0s - loss: 0.0059 - val_loss: 0.2012\n",
      "Epoch 274/300\n",
      "379/379 [==============================] - 0s - loss: 0.0061 - val_loss: 0.2233\n",
      "Epoch 275/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.1986\n",
      "Epoch 276/300\n",
      "379/379 [==============================] - 0s - loss: 0.0059 - val_loss: 0.2131\n",
      "Epoch 277/300\n",
      "379/379 [==============================] - 0s - loss: 0.0056 - val_loss: 0.2135\n",
      "Epoch 278/300\n",
      "379/379 [==============================] - 0s - loss: 0.0053 - val_loss: 0.2072\n",
      "Epoch 279/300\n",
      "379/379 [==============================] - 0s - loss: 0.0058 - val_loss: 0.1869\n",
      "Epoch 280/300\n",
      "379/379 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2682\n",
      "Epoch 281/300\n",
      "379/379 [==============================] - 0s - loss: 0.0176 - val_loss: 0.2641\n",
      "Epoch 282/300\n",
      "379/379 [==============================] - 0s - loss: 0.0161 - val_loss: 0.2345\n",
      "Epoch 283/300\n",
      "379/379 [==============================] - 0s - loss: 0.0152 - val_loss: 0.2422\n",
      "Epoch 284/300\n",
      "379/379 [==============================] - 0s - loss: 0.0245 - val_loss: 0.1811\n",
      "Epoch 285/300\n",
      "379/379 [==============================] - 0s - loss: 0.0568 - val_loss: 0.1610\n",
      "Epoch 286/300\n",
      "379/379 [==============================] - 0s - loss: 0.0273 - val_loss: 0.1785\n",
      "Epoch 287/300\n",
      "379/379 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2674\n",
      "Epoch 288/300\n",
      "379/379 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2828\n",
      "Epoch 289/300\n",
      "379/379 [==============================] - 0s - loss: 0.0212 - val_loss: 0.2123\n",
      "Epoch 290/300\n",
      "379/379 [==============================] - 0s - loss: 0.0181 - val_loss: 0.2034\n",
      "Epoch 291/300\n",
      "379/379 [==============================] - 0s - loss: 0.0375 - val_loss: 0.1629\n",
      "Epoch 292/300\n",
      "379/379 [==============================] - 0s - loss: 0.0231 - val_loss: 0.2087\n",
      "Epoch 293/300\n",
      "379/379 [==============================] - 0s - loss: 0.0054 - val_loss: 0.1978\n",
      "Epoch 294/300\n",
      "379/379 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2462\n",
      "Epoch 295/300\n",
      "379/379 [==============================] - 0s - loss: 0.0147 - val_loss: 0.2233\n",
      "Epoch 296/300\n",
      "379/379 [==============================] - 0s - loss: 0.0047 - val_loss: 0.2028\n",
      "Epoch 297/300\n",
      "379/379 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2167\n",
      "Epoch 298/300\n",
      "379/379 [==============================] - 0s - loss: 0.0045 - val_loss: 0.1953\n",
      "Epoch 299/300\n",
      "379/379 [==============================] - 0s - loss: 0.0097 - val_loss: 0.2191\n",
      "Epoch 300/300\n",
      "379/379 [==============================] - 0s - loss: 0.0143 - val_loss: 0.2550\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMDOzQhwYZmZWyP8HIwS8KB7qjuAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115cb1d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/300\n",
      "379/379 [==============================] - 0s - loss: 269.1262 - val_loss: 31.8957\n",
      "Epoch 2/300\n",
      "379/379 [==============================] - 0s - loss: 13.8449 - val_loss: 9.5667\n",
      "Epoch 3/300\n",
      "379/379 [==============================] - 0s - loss: 5.5062 - val_loss: 4.9394\n",
      "Epoch 4/300\n",
      "379/379 [==============================] - 0s - loss: 3.3789 - val_loss: 4.0013\n",
      "Epoch 5/300\n",
      "379/379 [==============================] - 0s - loss: 2.4478 - val_loss: 2.3800\n",
      "Epoch 6/300\n",
      "379/379 [==============================] - 0s - loss: 1.7087 - val_loss: 1.8040\n",
      "Epoch 7/300\n",
      "379/379 [==============================] - 0s - loss: 1.4103 - val_loss: 2.6229\n",
      "Epoch 8/300\n",
      "379/379 [==============================] - 0s - loss: 1.1712 - val_loss: 2.1090\n",
      "Epoch 9/300\n",
      "379/379 [==============================] - 0s - loss: 0.9212 - val_loss: 1.1086\n",
      "Epoch 10/300\n",
      "379/379 [==============================] - 0s - loss: 0.8974 - val_loss: 1.2584\n",
      "Epoch 11/300\n",
      "379/379 [==============================] - 0s - loss: 0.8886 - val_loss: 1.3689\n",
      "Epoch 12/300\n",
      "379/379 [==============================] - 0s - loss: 0.8540 - val_loss: 1.0756\n",
      "Epoch 13/300\n",
      "379/379 [==============================] - 0s - loss: 0.6516 - val_loss: 0.8314\n",
      "Epoch 14/300\n",
      "379/379 [==============================] - 0s - loss: 0.5874 - val_loss: 1.8708\n",
      "Epoch 15/300\n",
      "379/379 [==============================] - 0s - loss: 0.6075 - val_loss: 0.8522\n",
      "Epoch 16/300\n",
      "379/379 [==============================] - 0s - loss: 0.4620 - val_loss: 0.7408\n",
      "Epoch 17/300\n",
      "379/379 [==============================] - 0s - loss: 0.3866 - val_loss: 0.5326\n",
      "Epoch 18/300\n",
      "379/379 [==============================] - 0s - loss: 0.3547 - val_loss: 0.7490\n",
      "Epoch 19/300\n",
      "379/379 [==============================] - 0s - loss: 0.3542 - val_loss: 0.5621\n",
      "Epoch 20/300\n",
      "379/379 [==============================] - 0s - loss: 0.6301 - val_loss: 0.6396\n",
      "Epoch 21/300\n",
      "379/379 [==============================] - 0s - loss: 0.3495 - val_loss: 0.6763\n",
      "Epoch 22/300\n",
      "379/379 [==============================] - 0s - loss: 0.3187 - val_loss: 0.8735\n",
      "Epoch 23/300\n",
      "379/379 [==============================] - 0s - loss: 0.3258 - val_loss: 1.0997\n",
      "Epoch 24/300\n",
      "379/379 [==============================] - 0s - loss: 0.6331 - val_loss: 0.5237\n",
      "Epoch 25/300\n",
      "379/379 [==============================] - 0s - loss: 0.2304 - val_loss: 0.8525\n",
      "Epoch 26/300\n",
      "379/379 [==============================] - 0s - loss: 0.2188 - val_loss: 0.5368\n",
      "Epoch 27/300\n",
      "379/379 [==============================] - 0s - loss: 0.2194 - val_loss: 0.7722\n",
      "Epoch 28/300\n",
      "379/379 [==============================] - 0s - loss: 0.3497 - val_loss: 1.9331\n",
      "Epoch 29/300\n",
      "379/379 [==============================] - 0s - loss: 0.3291 - val_loss: 0.3829\n",
      "Epoch 30/300\n",
      "379/379 [==============================] - 0s - loss: 1.0591 - val_loss: 0.3944\n",
      "Epoch 31/300\n",
      "379/379 [==============================] - 0s - loss: 0.4184 - val_loss: 0.5451\n",
      "Epoch 32/300\n",
      "379/379 [==============================] - 0s - loss: 0.2334 - val_loss: 0.5942\n",
      "Epoch 33/300\n",
      "379/379 [==============================] - 0s - loss: 0.3699 - val_loss: 0.7946\n",
      "Epoch 34/300\n",
      "379/379 [==============================] - 0s - loss: 0.2679 - val_loss: 0.2649\n",
      "Epoch 35/300\n",
      "379/379 [==============================] - 0s - loss: 0.3274 - val_loss: 0.3070\n",
      "Epoch 36/300\n",
      "379/379 [==============================] - 0s - loss: 0.1375 - val_loss: 0.6200\n",
      "Epoch 37/300\n",
      "379/379 [==============================] - 0s - loss: 0.3596 - val_loss: 0.5625\n",
      "Epoch 38/300\n",
      "379/379 [==============================] - 0s - loss: 0.2330 - val_loss: 0.7564\n",
      "Epoch 39/300\n",
      "379/379 [==============================] - 0s - loss: 0.5435 - val_loss: 0.5613\n",
      "Epoch 40/300\n",
      "379/379 [==============================] - 0s - loss: 0.1481 - val_loss: 0.6876\n",
      "Epoch 41/300\n",
      "379/379 [==============================] - 0s - loss: 0.2541 - val_loss: 1.3717\n",
      "Epoch 42/300\n",
      "379/379 [==============================] - 0s - loss: 0.5153 - val_loss: 1.0799\n",
      "Epoch 43/300\n",
      "379/379 [==============================] - 0s - loss: 0.2783 - val_loss: 1.6485\n",
      "Epoch 44/300\n",
      "379/379 [==============================] - 0s - loss: 1.8281 - val_loss: 0.6087\n",
      "Epoch 45/300\n",
      "379/379 [==============================] - 0s - loss: 0.2289 - val_loss: 0.5118\n",
      "Epoch 46/300\n",
      "379/379 [==============================] - 0s - loss: 0.1604 - val_loss: 0.5916\n",
      "Epoch 47/300\n",
      "379/379 [==============================] - 0s - loss: 0.1168 - val_loss: 0.4020\n",
      "Epoch 48/300\n",
      "379/379 [==============================] - 0s - loss: 0.2847 - val_loss: 1.9082\n",
      "Epoch 49/300\n",
      "379/379 [==============================] - 0s - loss: 1.6402 - val_loss: 0.8474\n",
      "Epoch 50/300\n",
      "379/379 [==============================] - 0s - loss: 0.1265 - val_loss: 0.5068\n",
      "Epoch 51/300\n",
      "379/379 [==============================] - 0s - loss: 0.1054 - val_loss: 0.3034\n",
      "Epoch 52/300\n",
      "379/379 [==============================] - 0s - loss: 0.1217 - val_loss: 0.7635\n",
      "Epoch 53/300\n",
      "379/379 [==============================] - 0s - loss: 0.1461 - val_loss: 0.3128\n",
      "Epoch 54/300\n",
      "379/379 [==============================] - 0s - loss: 0.0890 - val_loss: 0.2242\n",
      "Epoch 55/300\n",
      "379/379 [==============================] - 0s - loss: 0.0783 - val_loss: 0.3198\n",
      "Epoch 56/300\n",
      "379/379 [==============================] - 0s - loss: 0.1153 - val_loss: 0.8470\n",
      "Epoch 57/300\n",
      "379/379 [==============================] - 0s - loss: 0.6069 - val_loss: 0.3715\n",
      "Epoch 58/300\n",
      "379/379 [==============================] - 0s - loss: 0.0944 - val_loss: 0.2726\n",
      "Epoch 59/300\n",
      "379/379 [==============================] - 0s - loss: 0.0823 - val_loss: 0.2937\n",
      "Epoch 60/300\n",
      "379/379 [==============================] - 0s - loss: 0.0607 - val_loss: 0.4489\n",
      "Epoch 61/300\n",
      "379/379 [==============================] - 0s - loss: 0.0757 - val_loss: 0.2300\n",
      "Epoch 62/300\n",
      "379/379 [==============================] - 0s - loss: 0.0992 - val_loss: 0.2002\n",
      "Epoch 63/300\n",
      "379/379 [==============================] - 0s - loss: 0.0699 - val_loss: 0.1997\n",
      "Epoch 64/300\n",
      "379/379 [==============================] - 0s - loss: 0.1105 - val_loss: 0.2043\n",
      "Epoch 65/300\n",
      "379/379 [==============================] - 0s - loss: 0.0567 - val_loss: 0.3008\n",
      "Epoch 66/300\n",
      "379/379 [==============================] - 0s - loss: 0.0562 - val_loss: 0.3966\n",
      "Epoch 67/300\n",
      "379/379 [==============================] - 0s - loss: 0.0819 - val_loss: 0.4953\n",
      "Epoch 68/300\n",
      "379/379 [==============================] - 0s - loss: 0.2309 - val_loss: 0.5850\n",
      "Epoch 69/300\n",
      "379/379 [==============================] - 0s - loss: 0.1042 - val_loss: 0.2059\n",
      "Epoch 70/300\n",
      "379/379 [==============================] - 0s - loss: 0.1472 - val_loss: 0.4651\n",
      "Epoch 71/300\n",
      "379/379 [==============================] - 0s - loss: 0.2313 - val_loss: 0.1554\n",
      "Epoch 72/300\n",
      "379/379 [==============================] - 0s - loss: 0.1055 - val_loss: 0.3371\n",
      "Epoch 73/300\n",
      "379/379 [==============================] - 0s - loss: 0.0560 - val_loss: 0.4567\n",
      "Epoch 74/300\n",
      "379/379 [==============================] - 0s - loss: 0.0453 - val_loss: 0.2215\n",
      "Epoch 75/300\n",
      "379/379 [==============================] - 0s - loss: 0.2357 - val_loss: 0.2732\n",
      "Epoch 76/300\n",
      "379/379 [==============================] - 0s - loss: 0.0892 - val_loss: 0.3071\n",
      "Epoch 77/300\n",
      "379/379 [==============================] - 0s - loss: 0.2633 - val_loss: 1.7849\n",
      "Epoch 78/300\n",
      "379/379 [==============================] - 0s - loss: 0.1438 - val_loss: 0.8938\n",
      "Epoch 79/300\n",
      "379/379 [==============================] - 0s - loss: 0.0834 - val_loss: 0.1638\n",
      "Epoch 80/300\n",
      "379/379 [==============================] - 0s - loss: 0.0690 - val_loss: 0.2966\n",
      "Epoch 81/300\n",
      "379/379 [==============================] - 0s - loss: 0.1248 - val_loss: 0.4792\n",
      "Epoch 82/300\n",
      "379/379 [==============================] - 0s - loss: 0.0524 - val_loss: 0.4136\n",
      "Epoch 83/300\n",
      "379/379 [==============================] - 0s - loss: 0.0421 - val_loss: 0.1980\n",
      "Epoch 84/300\n",
      "379/379 [==============================] - 0s - loss: 0.1223 - val_loss: 0.1870\n",
      "Epoch 85/300\n",
      "379/379 [==============================] - 0s - loss: 0.1497 - val_loss: 0.3386\n",
      "Epoch 86/300\n",
      "379/379 [==============================] - 0s - loss: 0.0391 - val_loss: 0.2759\n",
      "Epoch 87/300\n",
      "379/379 [==============================] - 0s - loss: 0.0360 - val_loss: 0.3017\n",
      "Epoch 88/300\n",
      "379/379 [==============================] - 0s - loss: 0.0375 - val_loss: 0.2721\n",
      "Epoch 89/300\n",
      "379/379 [==============================] - 0s - loss: 0.0348 - val_loss: 0.2047\n",
      "Epoch 90/300\n",
      "379/379 [==============================] - 0s - loss: 0.0454 - val_loss: 0.1856\n",
      "Epoch 91/300\n",
      "379/379 [==============================] - 0s - loss: 0.0449 - val_loss: 0.2582\n",
      "Epoch 92/300\n",
      "379/379 [==============================] - 0s - loss: 0.0489 - val_loss: 0.2703\n",
      "Epoch 93/300\n",
      "379/379 [==============================] - 0s - loss: 0.0325 - val_loss: 0.2618\n",
      "Epoch 94/300\n",
      "379/379 [==============================] - 0s - loss: 0.0279 - val_loss: 0.2692\n",
      "Epoch 95/300\n",
      "379/379 [==============================] - 0s - loss: 0.0666 - val_loss: 0.2241\n",
      "Epoch 96/300\n",
      "379/379 [==============================] - 0s - loss: 0.0506 - val_loss: 0.7860\n",
      "Epoch 97/300\n",
      "379/379 [==============================] - 0s - loss: 0.4776 - val_loss: 0.9152\n",
      "Epoch 98/300\n",
      "379/379 [==============================] - 0s - loss: 0.2086 - val_loss: 0.2750\n",
      "Epoch 99/300\n",
      "379/379 [==============================] - 0s - loss: 0.0323 - val_loss: 0.3190\n",
      "Epoch 100/300\n",
      "379/379 [==============================] - 0s - loss: 0.0340 - val_loss: 0.3035\n",
      "Epoch 101/300\n",
      "379/379 [==============================] - 0s - loss: 0.0273 - val_loss: 0.2600\n",
      "Epoch 102/300\n",
      "379/379 [==============================] - 0s - loss: 0.0246 - val_loss: 0.2891\n",
      "Epoch 103/300\n",
      "379/379 [==============================] - 0s - loss: 0.0576 - val_loss: 0.1659\n",
      "Epoch 104/300\n",
      "379/379 [==============================] - 0s - loss: 0.0382 - val_loss: 0.2453\n",
      "Epoch 105/300\n",
      "379/379 [==============================] - 0s - loss: 0.0292 - val_loss: 0.3768\n",
      "Epoch 106/300\n",
      "379/379 [==============================] - 0s - loss: 0.0937 - val_loss: 0.2654\n",
      "Epoch 107/300\n",
      "379/379 [==============================] - 0s - loss: 0.0581 - val_loss: 0.2892\n",
      "Epoch 108/300\n",
      "379/379 [==============================] - 0s - loss: 0.0405 - val_loss: 0.1942\n",
      "Epoch 109/300\n",
      "379/379 [==============================] - 0s - loss: 0.1665 - val_loss: 0.1937\n",
      "Epoch 110/300\n",
      "379/379 [==============================] - 0s - loss: 0.0306 - val_loss: 0.3410\n",
      "Epoch 111/300\n",
      "379/379 [==============================] - 0s - loss: 0.0588 - val_loss: 0.4930\n",
      "Epoch 112/300\n",
      "379/379 [==============================] - 0s - loss: 0.0347 - val_loss: 0.1896\n",
      "Epoch 113/300\n",
      "379/379 [==============================] - 0s - loss: 0.0352 - val_loss: 0.2115\n",
      "Epoch 114/300\n",
      "379/379 [==============================] - 0s - loss: 0.0222 - val_loss: 0.3731\n",
      "Epoch 115/300\n",
      "379/379 [==============================] - 0s - loss: 0.0265 - val_loss: 0.1915\n",
      "Epoch 116/300\n",
      "379/379 [==============================] - 0s - loss: 0.0414 - val_loss: 0.2360\n",
      "Epoch 117/300\n",
      "379/379 [==============================] - 0s - loss: 0.0373 - val_loss: 0.6105\n",
      "Epoch 118/300\n",
      "379/379 [==============================] - 0s - loss: 0.3231 - val_loss: 0.8180\n",
      "Epoch 119/300\n",
      "379/379 [==============================] - 0s - loss: 0.1509 - val_loss: 0.9003\n",
      "Epoch 120/300\n",
      "379/379 [==============================] - 0s - loss: 0.2338 - val_loss: 0.3858\n",
      "Epoch 121/300\n",
      "379/379 [==============================] - 0s - loss: 0.0454 - val_loss: 0.2500\n",
      "Epoch 122/300\n",
      "379/379 [==============================] - 0s - loss: 0.0227 - val_loss: 0.1753\n",
      "Epoch 123/300\n",
      "379/379 [==============================] - 0s - loss: 0.1098 - val_loss: 0.1743\n",
      "Epoch 124/300\n",
      "379/379 [==============================] - 0s - loss: 0.0450 - val_loss: 0.2430\n",
      "Epoch 125/300\n",
      "379/379 [==============================] - 0s - loss: 0.0185 - val_loss: 0.2367\n",
      "Epoch 126/300\n",
      "379/379 [==============================] - 0s - loss: 0.0544 - val_loss: 0.2654\n",
      "Epoch 127/300\n",
      "379/379 [==============================] - 0s - loss: 0.0589 - val_loss: 0.1918\n",
      "Epoch 128/300\n",
      "379/379 [==============================] - 0s - loss: 0.0237 - val_loss: 0.2343\n",
      "Epoch 129/300\n",
      "379/379 [==============================] - 0s - loss: 0.0539 - val_loss: 0.1772\n",
      "Epoch 130/300\n",
      "379/379 [==============================] - 0s - loss: 0.0538 - val_loss: 0.2267\n",
      "Epoch 131/300\n",
      "379/379 [==============================] - 0s - loss: 0.0475 - val_loss: 0.1216\n",
      "Epoch 132/300\n",
      "379/379 [==============================] - 0s - loss: 0.0365 - val_loss: 0.2462\n",
      "Epoch 133/300\n",
      "379/379 [==============================] - 0s - loss: 0.0190 - val_loss: 0.2253\n",
      "Epoch 134/300\n",
      "379/379 [==============================] - 0s - loss: 0.0163 - val_loss: 0.2727\n",
      "Epoch 135/300\n",
      "379/379 [==============================] - 0s - loss: 0.0221 - val_loss: 0.1942\n",
      "Epoch 136/300\n",
      "379/379 [==============================] - 0s - loss: 0.0290 - val_loss: 0.2157\n",
      "Epoch 137/300\n",
      "379/379 [==============================] - 0s - loss: 0.0606 - val_loss: 0.1968\n",
      "Epoch 138/300\n",
      "379/379 [==============================] - 0s - loss: 0.0205 - val_loss: 0.2145\n",
      "Epoch 139/300\n",
      "379/379 [==============================] - 0s - loss: 0.0169 - val_loss: 0.2058\n",
      "Epoch 140/300\n",
      "379/379 [==============================] - 0s - loss: 0.0147 - val_loss: 0.2255\n",
      "Epoch 141/300\n",
      "379/379 [==============================] - 0s - loss: 0.0151 - val_loss: 0.1976\n",
      "Epoch 142/300\n",
      "379/379 [==============================] - 0s - loss: 0.0204 - val_loss: 0.3212\n",
      "Epoch 143/300\n",
      "379/379 [==============================] - 0s - loss: 0.0184 - val_loss: 0.2296\n",
      "Epoch 144/300\n",
      "379/379 [==============================] - 0s - loss: 0.0235 - val_loss: 0.1327\n",
      "Epoch 145/300\n",
      "379/379 [==============================] - 0s - loss: 0.0949 - val_loss: 0.2198\n",
      "Epoch 146/300\n",
      "379/379 [==============================] - 0s - loss: 0.0182 - val_loss: 0.2842\n",
      "Epoch 147/300\n",
      "379/379 [==============================] - 0s - loss: 0.0168 - val_loss: 0.2354\n",
      "Epoch 148/300\n",
      "379/379 [==============================] - 0s - loss: 0.0242 - val_loss: 0.2370\n",
      "Epoch 149/300\n",
      "379/379 [==============================] - 0s - loss: 0.0469 - val_loss: 0.4664\n",
      "Epoch 150/300\n",
      "379/379 [==============================] - 0s - loss: 0.0918 - val_loss: 0.2461\n",
      "Epoch 151/300\n",
      "379/379 [==============================] - 0s - loss: 0.0358 - val_loss: 0.1310\n",
      "Epoch 152/300\n",
      "379/379 [==============================] - 0s - loss: 0.0582 - val_loss: 0.2107\n",
      "Epoch 153/300\n",
      "379/379 [==============================] - 0s - loss: 0.0124 - val_loss: 0.2852\n",
      "Epoch 154/300\n",
      "379/379 [==============================] - 0s - loss: 0.0163 - val_loss: 0.2124\n",
      "Epoch 155/300\n",
      "379/379 [==============================] - 0s - loss: 0.0121 - val_loss: 0.2245\n",
      "Epoch 156/300\n",
      "379/379 [==============================] - 0s - loss: 0.0411 - val_loss: 0.3655\n",
      "Epoch 157/300\n",
      "379/379 [==============================] - 0s - loss: 0.0880 - val_loss: 0.2480\n",
      "Epoch 158/300\n",
      "379/379 [==============================] - 0s - loss: 0.0139 - val_loss: 0.2302\n",
      "Epoch 159/300\n",
      "379/379 [==============================] - 0s - loss: 0.0126 - val_loss: 0.2279\n",
      "Epoch 160/300\n",
      "379/379 [==============================] - 0s - loss: 0.0126 - val_loss: 0.2700\n",
      "Epoch 161/300\n",
      "379/379 [==============================] - 0s - loss: 0.0352 - val_loss: 0.3215\n",
      "Epoch 162/300\n",
      "379/379 [==============================] - 0s - loss: 0.0345 - val_loss: 0.4155\n",
      "Epoch 163/300\n",
      "379/379 [==============================] - 0s - loss: 0.0169 - val_loss: 0.2618\n",
      "Epoch 164/300\n",
      "379/379 [==============================] - 0s - loss: 0.0239 - val_loss: 0.2042\n",
      "Epoch 165/300\n",
      "379/379 [==============================] - 0s - loss: 0.0135 - val_loss: 0.1912\n",
      "Epoch 166/300\n",
      "379/379 [==============================] - 0s - loss: 0.0390 - val_loss: 0.2044\n",
      "Epoch 167/300\n",
      "379/379 [==============================] - 0s - loss: 0.0379 - val_loss: 0.3004\n",
      "Epoch 168/300\n",
      "379/379 [==============================] - 0s - loss: 0.1047 - val_loss: 0.4424\n",
      "Epoch 169/300\n",
      "379/379 [==============================] - 0s - loss: 0.0368 - val_loss: 0.1755\n",
      "Epoch 170/300\n",
      "379/379 [==============================] - 0s - loss: 0.0176 - val_loss: 0.1303\n",
      "Epoch 171/300\n",
      "379/379 [==============================] - 0s - loss: 0.0880 - val_loss: 0.1600\n",
      "Epoch 172/300\n",
      "379/379 [==============================] - 0s - loss: 0.0188 - val_loss: 0.1513\n",
      "Epoch 173/300\n",
      "379/379 [==============================] - 0s - loss: 0.0319 - val_loss: 0.2207\n",
      "Epoch 174/300\n",
      "379/379 [==============================] - 0s - loss: 0.0214 - val_loss: 0.3497\n",
      "Epoch 175/300\n",
      "379/379 [==============================] - 0s - loss: 0.0140 - val_loss: 0.2502\n",
      "Epoch 176/300\n",
      "379/379 [==============================] - 0s - loss: 0.0123 - val_loss: 0.2123\n",
      "Epoch 177/300\n",
      "379/379 [==============================] - 0s - loss: 0.0485 - val_loss: 0.1976\n",
      "Epoch 178/300\n",
      "379/379 [==============================] - 0s - loss: 0.0908 - val_loss: 0.1951\n",
      "Epoch 179/300\n",
      "379/379 [==============================] - 0s - loss: 0.0220 - val_loss: 0.2073\n",
      "Epoch 180/300\n",
      "379/379 [==============================] - 0s - loss: 0.0119 - val_loss: 0.2584\n",
      "Epoch 181/300\n",
      "379/379 [==============================] - 0s - loss: 0.0130 - val_loss: 0.2672\n",
      "Epoch 182/300\n",
      "379/379 [==============================] - 0s - loss: 0.0200 - val_loss: 0.2250\n",
      "Epoch 183/300\n",
      "379/379 [==============================] - 0s - loss: 0.0125 - val_loss: 0.2155\n",
      "Epoch 184/300\n",
      "379/379 [==============================] - 0s - loss: 0.0095 - val_loss: 0.2639\n",
      "Epoch 185/300\n",
      "379/379 [==============================] - 0s - loss: 0.0162 - val_loss: 0.3541\n",
      "Epoch 186/300\n",
      "379/379 [==============================] - 0s - loss: 0.0260 - val_loss: 0.1492\n",
      "Epoch 187/300\n",
      "379/379 [==============================] - 0s - loss: 0.0879 - val_loss: 0.1545\n",
      "Epoch 188/300\n",
      "379/379 [==============================] - 0s - loss: 0.0259 - val_loss: 0.1944\n",
      "Epoch 189/300\n",
      "379/379 [==============================] - 0s - loss: 0.0219 - val_loss: 0.2061\n",
      "Epoch 190/300\n",
      "379/379 [==============================] - 0s - loss: 0.0115 - val_loss: 0.1680\n",
      "Epoch 191/300\n",
      "379/379 [==============================] - 0s - loss: 0.0147 - val_loss: 0.2254\n",
      "Epoch 192/300\n",
      "379/379 [==============================] - 0s - loss: 0.0125 - val_loss: 0.1772\n",
      "Epoch 193/300\n",
      "379/379 [==============================] - 0s - loss: 0.0237 - val_loss: 0.1616\n",
      "Epoch 194/300\n",
      "379/379 [==============================] - 0s - loss: 0.0255 - val_loss: 0.1435\n",
      "Epoch 195/300\n",
      "379/379 [==============================] - 0s - loss: 0.0421 - val_loss: 0.1991\n",
      "Epoch 196/300\n",
      "379/379 [==============================] - 0s - loss: 0.0128 - val_loss: 0.1308\n",
      "Epoch 197/300\n",
      "379/379 [==============================] - 0s - loss: 0.0540 - val_loss: 0.1820\n",
      "Epoch 198/300\n",
      "379/379 [==============================] - 0s - loss: 0.0146 - val_loss: 0.2169\n",
      "Epoch 199/300\n",
      "379/379 [==============================] - 0s - loss: 0.0243 - val_loss: 0.5313\n",
      "Epoch 200/300\n",
      "379/379 [==============================] - 0s - loss: 0.0710 - val_loss: 0.1446\n",
      "Epoch 201/300\n",
      "379/379 [==============================] - 0s - loss: 0.0217 - val_loss: 0.2464\n",
      "Epoch 202/300\n",
      "379/379 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2022\n",
      "Epoch 203/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2221\n",
      "Epoch 204/300\n",
      "379/379 [==============================] - 0s - loss: 0.0217 - val_loss: 0.2224\n",
      "Epoch 205/300\n",
      "379/379 [==============================] - 0s - loss: 0.0090 - val_loss: 0.2186\n",
      "Epoch 206/300\n",
      "379/379 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2410\n",
      "Epoch 207/300\n",
      "379/379 [==============================] - 0s - loss: 0.0092 - val_loss: 0.1851\n",
      "Epoch 208/300\n",
      "379/379 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2683\n",
      "Epoch 209/300\n",
      "379/379 [==============================] - 0s - loss: 0.0173 - val_loss: 0.2402\n",
      "Epoch 210/300\n",
      "379/379 [==============================] - 0s - loss: 0.0300 - val_loss: 0.4648\n",
      "Epoch 211/300\n",
      "379/379 [==============================] - 0s - loss: 0.3269 - val_loss: 0.5901\n",
      "Epoch 212/300\n",
      "379/379 [==============================] - 0s - loss: 0.2972 - val_loss: 0.2808\n",
      "Epoch 213/300\n",
      "379/379 [==============================] - 0s - loss: 0.0593 - val_loss: 0.2342\n",
      "Epoch 214/300\n",
      "379/379 [==============================] - 0s - loss: 0.0244 - val_loss: 0.1460\n",
      "Epoch 215/300\n",
      "379/379 [==============================] - 0s - loss: 0.0331 - val_loss: 0.2162\n",
      "Epoch 216/300\n",
      "379/379 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2348\n",
      "Epoch 217/300\n",
      "379/379 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2065\n",
      "Epoch 218/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2302\n",
      "Epoch 219/300\n",
      "379/379 [==============================] - 0s - loss: 0.0238 - val_loss: 0.2963\n",
      "Epoch 220/300\n",
      "379/379 [==============================] - 0s - loss: 0.0153 - val_loss: 0.2588\n",
      "Epoch 221/300\n",
      "379/379 [==============================] - 0s - loss: 0.0143 - val_loss: 0.2759\n",
      "Epoch 222/300\n",
      "379/379 [==============================] - 0s - loss: 0.0299 - val_loss: 0.2825\n",
      "Epoch 223/300\n",
      "379/379 [==============================] - 0s - loss: 0.0559 - val_loss: 0.1786\n",
      "Epoch 224/300\n",
      "379/379 [==============================] - 0s - loss: 0.0148 - val_loss: 0.2238\n",
      "Epoch 225/300\n",
      "379/379 [==============================] - 0s - loss: 0.0079 - val_loss: 0.1918\n",
      "Epoch 226/300\n",
      "379/379 [==============================] - 0s - loss: 0.0135 - val_loss: 0.2100\n",
      "Epoch 227/300\n",
      "379/379 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2048\n",
      "Epoch 228/300\n",
      "379/379 [==============================] - 0s - loss: 0.0110 - val_loss: 0.2788\n",
      "Epoch 229/300\n",
      "379/379 [==============================] - 0s - loss: 0.0266 - val_loss: 0.1938\n",
      "Epoch 230/300\n",
      "379/379 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2325\n",
      "Epoch 231/300\n",
      "379/379 [==============================] - 0s - loss: 0.0071 - val_loss: 0.1937\n",
      "Epoch 232/300\n",
      "379/379 [==============================] - 0s - loss: 0.0103 - val_loss: 0.2575\n",
      "Epoch 233/300\n",
      "379/379 [==============================] - 0s - loss: 0.0109 - val_loss: 0.2183\n",
      "Epoch 234/300\n",
      "379/379 [==============================] - 0s - loss: 0.0063 - val_loss: 0.2177\n",
      "Epoch 235/300\n",
      "379/379 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2243\n",
      "Epoch 236/300\n",
      "379/379 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2140\n",
      "Epoch 237/300\n",
      "379/379 [==============================] - 0s - loss: 0.0085 - val_loss: 0.2419\n",
      "Epoch 238/300\n",
      "379/379 [==============================] - 0s - loss: 0.0149 - val_loss: 0.2302\n",
      "Epoch 239/300\n",
      "379/379 [==============================] - 0s - loss: 0.0135 - val_loss: 0.1613\n",
      "Epoch 240/300\n",
      "379/379 [==============================] - 0s - loss: 0.0327 - val_loss: 0.1741\n",
      "Epoch 241/300\n",
      "379/379 [==============================] - 0s - loss: 0.0120 - val_loss: 0.2284\n",
      "Epoch 242/300\n",
      "379/379 [==============================] - 0s - loss: 0.0062 - val_loss: 0.2279\n",
      "Epoch 243/300\n",
      "379/379 [==============================] - 0s - loss: 0.0341 - val_loss: 0.1673\n",
      "Epoch 244/300\n",
      "379/379 [==============================] - 0s - loss: 0.1130 - val_loss: 0.1617\n",
      "Epoch 245/300\n",
      "379/379 [==============================] - 0s - loss: 0.0607 - val_loss: 0.1579\n",
      "Epoch 246/300\n",
      "379/379 [==============================] - 0s - loss: 0.0177 - val_loss: 0.3021\n",
      "Epoch 247/300\n",
      "379/379 [==============================] - 0s - loss: 0.0168 - val_loss: 0.2322\n",
      "Epoch 248/300\n",
      "379/379 [==============================] - 0s - loss: 0.0094 - val_loss: 0.2300\n",
      "Epoch 249/300\n",
      "379/379 [==============================] - 0s - loss: 0.0141 - val_loss: 0.2894\n",
      "Epoch 250/300\n",
      "379/379 [==============================] - 0s - loss: 0.0143 - val_loss: 0.2456\n",
      "Epoch 251/300\n",
      "379/379 [==============================] - 0s - loss: 0.0168 - val_loss: 0.3163\n",
      "Epoch 252/300\n",
      "379/379 [==============================] - 0s - loss: 0.0199 - val_loss: 0.2228\n",
      "Epoch 253/300\n",
      "379/379 [==============================] - 0s - loss: 0.0082 - val_loss: 0.2822\n",
      "Epoch 254/300\n",
      "379/379 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2162\n",
      "Epoch 255/300\n",
      "379/379 [==============================] - 0s - loss: 0.0119 - val_loss: 0.2139\n",
      "Epoch 256/300\n",
      "379/379 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2053\n",
      "Epoch 257/300\n",
      "379/379 [==============================] - 0s - loss: 0.0060 - val_loss: 0.1912\n",
      "Epoch 258/300\n",
      "379/379 [==============================] - 0s - loss: 0.0438 - val_loss: 0.1671\n",
      "Epoch 259/300\n",
      "379/379 [==============================] - 0s - loss: 0.0124 - val_loss: 0.1723\n",
      "Epoch 260/300\n",
      "379/379 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2447\n",
      "Epoch 261/300\n",
      "379/379 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2406\n",
      "Epoch 262/300\n",
      "379/379 [==============================] - 0s - loss: 0.0057 - val_loss: 0.2114\n",
      "Epoch 263/300\n",
      "379/379 [==============================] - 0s - loss: 0.0090 - val_loss: 0.2387\n",
      "Epoch 264/300\n",
      "379/379 [==============================] - 0s - loss: 0.0087 - val_loss: 0.1996\n",
      "Epoch 265/300\n",
      "379/379 [==============================] - 0s - loss: 0.0061 - val_loss: 0.2283\n",
      "Epoch 266/300\n",
      "379/379 [==============================] - 0s - loss: 0.0054 - val_loss: 0.2037\n",
      "Epoch 267/300\n",
      "379/379 [==============================] - 0s - loss: 0.0062 - val_loss: 0.1894\n",
      "Epoch 268/300\n",
      "379/379 [==============================] - 0s - loss: 0.0305 - val_loss: 0.2168\n",
      "Epoch 269/300\n",
      "379/379 [==============================] - 0s - loss: 0.2172 - val_loss: 0.1565\n",
      "Epoch 270/300\n",
      "379/379 [==============================] - 0s - loss: 0.0493 - val_loss: 0.1976\n",
      "Epoch 271/300\n",
      "379/379 [==============================] - 0s - loss: 0.0071 - val_loss: 0.1778\n",
      "Epoch 272/300\n",
      "379/379 [==============================] - 0s - loss: 0.0056 - val_loss: 0.2132\n",
      "Epoch 273/300\n",
      "379/379 [==============================] - 0s - loss: 0.0058 - val_loss: 0.1973\n",
      "Epoch 274/300\n",
      "379/379 [==============================] - 0s - loss: 0.0056 - val_loss: 0.2149\n",
      "Epoch 275/300\n",
      "379/379 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2548\n",
      "Epoch 276/300\n",
      "379/379 [==============================] - 0s - loss: 0.0117 - val_loss: 0.2281\n",
      "Epoch 277/300\n",
      "379/379 [==============================] - 0s - loss: 0.0068 - val_loss: 0.2277\n",
      "Epoch 278/300\n",
      "379/379 [==============================] - 0s - loss: 0.0076 - val_loss: 0.1974\n",
      "Epoch 279/300\n",
      "379/379 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2184\n",
      "Epoch 280/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2059\n",
      "Epoch 281/300\n",
      "379/379 [==============================] - 0s - loss: 0.0050 - val_loss: 0.2013\n",
      "Epoch 282/300\n",
      "379/379 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2149\n",
      "Epoch 283/300\n",
      "379/379 [==============================] - 0s - loss: 0.0055 - val_loss: 0.1567\n",
      "Epoch 284/300\n",
      "379/379 [==============================] - 0s - loss: 0.0171 - val_loss: 0.1991\n",
      "Epoch 285/300\n",
      "379/379 [==============================] - 0s - loss: 0.0113 - val_loss: 0.1777\n",
      "Epoch 286/300\n",
      "379/379 [==============================] - 0s - loss: 0.0224 - val_loss: 0.1855\n",
      "Epoch 287/300\n",
      "379/379 [==============================] - 0s - loss: 0.0064 - val_loss: 0.1733\n",
      "Epoch 288/300\n",
      "379/379 [==============================] - 0s - loss: 0.0081 - val_loss: 0.1683\n",
      "Epoch 289/300\n",
      "379/379 [==============================] - 0s - loss: 0.0344 - val_loss: 0.2293\n",
      "Epoch 290/300\n",
      "379/379 [==============================] - 0s - loss: 0.0052 - val_loss: 0.1675\n",
      "Epoch 291/300\n",
      "379/379 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2287\n",
      "Epoch 292/300\n",
      "379/379 [==============================] - 0s - loss: 0.0063 - val_loss: 0.2019\n",
      "Epoch 293/300\n",
      "379/379 [==============================] - 0s - loss: 0.0061 - val_loss: 0.1864\n",
      "Epoch 294/300\n",
      "379/379 [==============================] - 0s - loss: 0.0231 - val_loss: 0.2251\n",
      "Epoch 295/300\n",
      "379/379 [==============================] - 0s - loss: 0.0102 - val_loss: 0.2434\n",
      "Epoch 296/300\n",
      "379/379 [==============================] - 0s - loss: 0.0102 - val_loss: 0.1937\n",
      "Epoch 297/300\n",
      "379/379 [==============================] - 0s - loss: 0.0057 - val_loss: 0.2129\n",
      "Epoch 298/300\n",
      "379/379 [==============================] - 0s - loss: 0.0052 - val_loss: 0.2172\n",
      "Epoch 299/300\n",
      "379/379 [==============================] - 0s - loss: 0.0045 - val_loss: 0.1938\n",
      "Epoch 300/300\n",
      "379/379 [==============================] - 0s - loss: 0.0073 - val_loss: 0.1725\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116059190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/300\n",
      "379/379 [==============================] - 0s - loss: 271.9430 - val_loss: 33.2757\n",
      "Epoch 2/300\n",
      "379/379 [==============================] - 0s - loss: 14.5835 - val_loss: 10.2473\n",
      "Epoch 3/300\n",
      "379/379 [==============================] - 0s - loss: 5.9477 - val_loss: 7.4569\n",
      "Epoch 4/300\n",
      "379/379 [==============================] - 0s - loss: 3.6278 - val_loss: 4.1853\n",
      "Epoch 5/300\n",
      "379/379 [==============================] - 0s - loss: 2.3231 - val_loss: 3.7971\n",
      "Epoch 6/300\n",
      "379/379 [==============================] - 0s - loss: 2.2200 - val_loss: 2.1209\n",
      "Epoch 7/300\n",
      "379/379 [==============================] - 0s - loss: 1.5043 - val_loss: 2.3724\n",
      "Epoch 8/300\n",
      "379/379 [==============================] - 0s - loss: 1.5622 - val_loss: 1.7071\n",
      "Epoch 9/300\n",
      "379/379 [==============================] - 0s - loss: 0.9235 - val_loss: 1.2687\n",
      "Epoch 10/300\n",
      "379/379 [==============================] - 0s - loss: 0.8240 - val_loss: 1.0101\n",
      "Epoch 11/300\n",
      "379/379 [==============================] - 0s - loss: 0.7206 - val_loss: 1.6422\n",
      "Epoch 12/300\n",
      "379/379 [==============================] - 0s - loss: 0.7330 - val_loss: 0.9739\n",
      "Epoch 13/300\n",
      "379/379 [==============================] - 0s - loss: 0.5700 - val_loss: 1.2220\n",
      "Epoch 14/300\n",
      "379/379 [==============================] - 0s - loss: 0.4849 - val_loss: 0.7202\n",
      "Epoch 15/300\n",
      "379/379 [==============================] - 0s - loss: 0.5003 - val_loss: 1.0585\n",
      "Epoch 16/300\n",
      "379/379 [==============================] - 0s - loss: 0.6918 - val_loss: 1.2753\n",
      "Epoch 17/300\n",
      "379/379 [==============================] - 0s - loss: 0.6130 - val_loss: 0.8507\n",
      "Epoch 18/300\n",
      "379/379 [==============================] - 0s - loss: 0.3968 - val_loss: 0.4734\n",
      "Epoch 19/300\n",
      "379/379 [==============================] - 0s - loss: 0.3757 - val_loss: 1.1900\n",
      "Epoch 20/300\n",
      "379/379 [==============================] - 0s - loss: 0.3696 - val_loss: 0.5742\n",
      "Epoch 21/300\n",
      "379/379 [==============================] - 0s - loss: 0.4115 - val_loss: 0.4358\n",
      "Epoch 22/300\n",
      "379/379 [==============================] - 0s - loss: 0.3025 - val_loss: 1.3727\n",
      "Epoch 23/300\n",
      "379/379 [==============================] - 0s - loss: 0.2878 - val_loss: 1.1445\n",
      "Epoch 24/300\n",
      "379/379 [==============================] - 0s - loss: 0.2527 - val_loss: 0.4038\n",
      "Epoch 25/300\n",
      "379/379 [==============================] - 0s - loss: 0.2189 - val_loss: 0.4701\n",
      "Epoch 26/300\n",
      "379/379 [==============================] - 0s - loss: 0.3358 - val_loss: 1.2899\n",
      "Epoch 27/300\n",
      "379/379 [==============================] - 0s - loss: 0.5121 - val_loss: 0.7164\n",
      "Epoch 28/300\n",
      "379/379 [==============================] - 0s - loss: 0.2754 - val_loss: 0.6575\n",
      "Epoch 29/300\n",
      "379/379 [==============================] - 0s - loss: 0.2207 - val_loss: 0.4053\n",
      "Epoch 30/300\n",
      "379/379 [==============================] - 0s - loss: 0.1703 - val_loss: 0.3204\n",
      "Epoch 31/300\n",
      "379/379 [==============================] - 0s - loss: 0.1591 - val_loss: 0.4070\n",
      "Epoch 32/300\n",
      "379/379 [==============================] - 0s - loss: 0.1806 - val_loss: 0.6647\n",
      "Epoch 33/300\n",
      "379/379 [==============================] - 0s - loss: 0.1500 - val_loss: 0.4589\n",
      "Epoch 34/300\n",
      "379/379 [==============================] - 0s - loss: 0.1490 - val_loss: 0.4633\n",
      "Epoch 35/300\n",
      "379/379 [==============================] - 0s - loss: 0.1419 - val_loss: 0.4858\n",
      "Epoch 36/300\n",
      "379/379 [==============================] - 0s - loss: 0.1365 - val_loss: 0.3798\n",
      "Epoch 37/300\n",
      "379/379 [==============================] - 0s - loss: 0.1440 - val_loss: 0.2518\n",
      "Epoch 38/300\n",
      "379/379 [==============================] - 0s - loss: 0.2724 - val_loss: 0.5999\n",
      "Epoch 39/300\n",
      "379/379 [==============================] - 0s - loss: 0.1273 - val_loss: 0.3113\n",
      "Epoch 40/300\n",
      "379/379 [==============================] - 0s - loss: 0.1077 - val_loss: 0.4353\n",
      "Epoch 41/300\n",
      "379/379 [==============================] - 0s - loss: 0.0940 - val_loss: 0.3439\n",
      "Epoch 42/300\n",
      "379/379 [==============================] - 0s - loss: 0.0945 - val_loss: 0.6016\n",
      "Epoch 43/300\n",
      "379/379 [==============================] - 0s - loss: 0.1242 - val_loss: 0.6023\n",
      "Epoch 44/300\n",
      "379/379 [==============================] - 0s - loss: 0.2775 - val_loss: 2.3044\n",
      "Epoch 45/300\n",
      "379/379 [==============================] - 0s - loss: 0.9032 - val_loss: 0.3373\n",
      "Epoch 46/300\n",
      "379/379 [==============================] - 0s - loss: 0.1351 - val_loss: 0.9119\n",
      "Epoch 47/300\n",
      "379/379 [==============================] - 0s - loss: 0.1855 - val_loss: 0.4899\n",
      "Epoch 48/300\n",
      "379/379 [==============================] - 0s - loss: 0.2879 - val_loss: 0.2040\n",
      "Epoch 49/300\n",
      "379/379 [==============================] - 0s - loss: 0.7156 - val_loss: 0.2821\n",
      "Epoch 50/300\n",
      "379/379 [==============================] - 0s - loss: 0.3166 - val_loss: 0.3307\n",
      "Epoch 51/300\n",
      "379/379 [==============================] - 0s - loss: 0.3166 - val_loss: 0.3871\n",
      "Epoch 52/300\n",
      "379/379 [==============================] - 0s - loss: 0.0960 - val_loss: 0.2936\n",
      "Epoch 53/300\n",
      "379/379 [==============================] - 0s - loss: 0.0868 - val_loss: 0.5967\n",
      "Epoch 54/300\n",
      "379/379 [==============================] - 0s - loss: 0.0711 - val_loss: 0.3205\n",
      "Epoch 55/300\n",
      "379/379 [==============================] - 0s - loss: 0.1313 - val_loss: 0.6959\n",
      "Epoch 56/300\n",
      "379/379 [==============================] - 0s - loss: 0.1917 - val_loss: 1.4768\n",
      "Epoch 57/300\n",
      "379/379 [==============================] - 0s - loss: 0.8509 - val_loss: 0.6336\n",
      "Epoch 58/300\n",
      "379/379 [==============================] - 0s - loss: 0.1545 - val_loss: 0.2605\n",
      "Epoch 59/300\n",
      "379/379 [==============================] - 0s - loss: 0.2288 - val_loss: 0.1938\n",
      "Epoch 60/300\n",
      "379/379 [==============================] - 0s - loss: 0.2119 - val_loss: 0.7012\n",
      "Epoch 61/300\n",
      "379/379 [==============================] - 0s - loss: 0.9841 - val_loss: 0.4598\n",
      "Epoch 62/300\n",
      "379/379 [==============================] - 0s - loss: 0.6416 - val_loss: 0.4669\n",
      "Epoch 63/300\n",
      "379/379 [==============================] - 0s - loss: 0.0948 - val_loss: 0.6246\n",
      "Epoch 64/300\n",
      "379/379 [==============================] - 0s - loss: 0.1396 - val_loss: 0.3261\n",
      "Epoch 65/300\n",
      "379/379 [==============================] - 0s - loss: 0.1395 - val_loss: 0.6931\n",
      "Epoch 66/300\n",
      "379/379 [==============================] - 0s - loss: 0.2884 - val_loss: 0.3165\n",
      "Epoch 67/300\n",
      "379/379 [==============================] - 0s - loss: 0.0688 - val_loss: 0.8580\n",
      "Epoch 68/300\n",
      "379/379 [==============================] - 0s - loss: 0.1223 - val_loss: 0.2118\n",
      "Epoch 69/300\n",
      "379/379 [==============================] - 0s - loss: 0.0612 - val_loss: 0.3282\n",
      "Epoch 70/300\n",
      "379/379 [==============================] - 0s - loss: 0.0800 - val_loss: 0.3422\n",
      "Epoch 71/300\n",
      "379/379 [==============================] - 0s - loss: 0.0508 - val_loss: 0.2207\n",
      "Epoch 72/300\n",
      "379/379 [==============================] - 0s - loss: 0.1135 - val_loss: 0.3891\n",
      "Epoch 73/300\n",
      "379/379 [==============================] - 0s - loss: 0.1386 - val_loss: 0.2338\n",
      "Epoch 74/300\n",
      "379/379 [==============================] - 0s - loss: 0.1394 - val_loss: 0.1690\n",
      "Epoch 75/300\n",
      "379/379 [==============================] - 0s - loss: 0.2049 - val_loss: 0.2501\n",
      "Epoch 76/300\n",
      "379/379 [==============================] - 0s - loss: 0.0489 - val_loss: 0.4197\n",
      "Epoch 77/300\n",
      "379/379 [==============================] - 0s - loss: 0.0511 - val_loss: 0.3897\n",
      "Epoch 78/300\n",
      "379/379 [==============================] - 0s - loss: 0.0437 - val_loss: 0.3416\n",
      "Epoch 79/300\n",
      "379/379 [==============================] - 0s - loss: 0.1300 - val_loss: 1.2822\n",
      "Epoch 80/300\n",
      "379/379 [==============================] - 0s - loss: 0.5691 - val_loss: 1.6346\n",
      "Epoch 81/300\n",
      "379/379 [==============================] - 0s - loss: 0.8344 - val_loss: 1.7793\n",
      "Epoch 82/300\n",
      "379/379 [==============================] - 0s - loss: 1.2365 - val_loss: 0.3672\n",
      "Epoch 83/300\n",
      "379/379 [==============================] - 0s - loss: 0.1004 - val_loss: 0.3382\n",
      "Epoch 84/300\n",
      "379/379 [==============================] - 0s - loss: 0.0446 - val_loss: 0.2499\n",
      "Epoch 85/300\n",
      "379/379 [==============================] - 0s - loss: 0.0672 - val_loss: 0.1924\n",
      "Epoch 86/300\n",
      "379/379 [==============================] - 0s - loss: 0.0852 - val_loss: 0.2024\n",
      "Epoch 87/300\n",
      "379/379 [==============================] - 0s - loss: 0.0437 - val_loss: 0.2720\n",
      "Epoch 88/300\n",
      "379/379 [==============================] - 0s - loss: 0.1043 - val_loss: 0.2383\n",
      "Epoch 89/300\n",
      "379/379 [==============================] - 0s - loss: 0.0635 - val_loss: 0.2470\n",
      "Epoch 90/300\n",
      "379/379 [==============================] - 0s - loss: 0.0374 - val_loss: 0.2106\n",
      "Epoch 91/300\n",
      "379/379 [==============================] - 0s - loss: 0.0557 - val_loss: 0.2320\n",
      "Epoch 92/300\n",
      "379/379 [==============================] - 0s - loss: 0.0333 - val_loss: 0.3290\n",
      "Epoch 93/300\n",
      "379/379 [==============================] - 0s - loss: 0.0362 - val_loss: 0.2445\n",
      "Epoch 94/300\n",
      "379/379 [==============================] - 0s - loss: 0.0317 - val_loss: 0.2333\n",
      "Epoch 95/300\n",
      "379/379 [==============================] - 0s - loss: 0.0312 - val_loss: 0.2658\n",
      "Epoch 96/300\n",
      "379/379 [==============================] - 0s - loss: 0.0423 - val_loss: 0.1726\n",
      "Epoch 97/300\n",
      "379/379 [==============================] - 0s - loss: 0.1056 - val_loss: 0.3858\n",
      "Epoch 98/300\n",
      "379/379 [==============================] - 0s - loss: 0.0351 - val_loss: 0.2039\n",
      "Epoch 99/300\n",
      "379/379 [==============================] - 0s - loss: 0.0508 - val_loss: 0.3031\n",
      "Epoch 100/300\n",
      "379/379 [==============================] - 0s - loss: 0.0434 - val_loss: 0.1773\n",
      "Epoch 101/300\n",
      "379/379 [==============================] - 0s - loss: 0.0402 - val_loss: 0.2243\n",
      "Epoch 102/300\n",
      "379/379 [==============================] - 0s - loss: 0.0437 - val_loss: 0.4127\n",
      "Epoch 103/300\n",
      "379/379 [==============================] - 0s - loss: 0.0430 - val_loss: 0.2943\n",
      "Epoch 104/300\n",
      "379/379 [==============================] - 0s - loss: 0.0298 - val_loss: 0.2638\n",
      "Epoch 105/300\n",
      "379/379 [==============================] - 0s - loss: 0.0281 - val_loss: 0.2620\n",
      "Epoch 106/300\n",
      "379/379 [==============================] - 0s - loss: 0.0484 - val_loss: 0.3708\n",
      "Epoch 107/300\n",
      "379/379 [==============================] - 0s - loss: 0.0315 - val_loss: 0.4361\n",
      "Epoch 108/300\n",
      "379/379 [==============================] - 0s - loss: 0.0321 - val_loss: 0.3237\n",
      "Epoch 109/300\n",
      "379/379 [==============================] - 0s - loss: 0.0446 - val_loss: 0.3547\n",
      "Epoch 110/300\n",
      "379/379 [==============================] - 0s - loss: 0.0250 - val_loss: 0.2793\n",
      "Epoch 111/300\n",
      "379/379 [==============================] - 0s - loss: 0.0266 - val_loss: 0.2419\n",
      "Epoch 112/300\n",
      "379/379 [==============================] - 0s - loss: 0.0347 - val_loss: 0.3188\n",
      "Epoch 113/300\n",
      "379/379 [==============================] - 0s - loss: 0.0983 - val_loss: 0.2673\n",
      "Epoch 114/300\n",
      "379/379 [==============================] - 0s - loss: 0.0261 - val_loss: 0.2297\n",
      "Epoch 115/300\n",
      "379/379 [==============================] - 0s - loss: 0.0307 - val_loss: 0.3424\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e797f2f4816a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mhist_relu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Graficamos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    840\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, force)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mbar\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprog_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mbar\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m']'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asanhuez/.miniconda2/lib/python2.7/site-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file"
     ]
    }
   ],
   "source": [
    "for lr in lear_rate:\n",
    "    model_lr = Sequential()\n",
    "    model_lr.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "    model_lr.add(Activation('relu'))\n",
    "    model_lr.add(Dense(1, init='uniform'))\n",
    "    model_lr.add(Activation('linear'))\n",
    "\n",
    "    sgd = SGD(lr=lr)\n",
    "    model_lr.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "\n",
    "    hist_relu = model_lr.fit(X_train_scaled.as_matrix(), y_train_scaled.as_matrix(), nb_epoch=300, verbose=1, validation_data=(X_test_scaled.as_matrix(), y_test_scaled.as_matrix()))\n",
    "    \n",
    "    # Graficamos\n",
    "    plt.plot(hist_relu.history['loss'])\n",
    "    plt.title(\"Square Error - Relu. Learning Rate = {0}\".format(lr))\n",
    "    plt.xlabel('Ephocs')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
